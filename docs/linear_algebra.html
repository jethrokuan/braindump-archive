<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-11-19 Mon 16:53 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Linear Algebra</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Linear Algebra</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org6176daf">1. Definitions</a></li>
<li><a href="#org05e1604">2. Linear Independence</a></li>
<li><a href="#orgf7095fb">3. Span</a></li>
<li><a href="#org82e2b61">4. Basis</a></li>
<li><a href="#org352addf">5. Special Matrices</a></li>
<li><a href="#orgcbd2be1">6. Matrix properties</a></li>
<li><a href="#orge24c731">7. Solutions to linear systems</a></li>
<li><a href="#orge1136e8">8. Matrices as Linear Transformations</a></li>
<li><a href="#org0ec2f71">9. Matrix Multiplication as Composition</a></li>
<li><a href="#org5e82856">10. Determinant</a></li>
<li><a href="#orgbd29e76">11. Matrices for solving linear equations</a></li>
<li><a href="#org11077f4">12. Rank</a></li>
<li><a href="#org06c6585">13. Column Space</a></li>
<li><a href="#orgded891b">14. Dot Product</a></li>
<li><a href="#orgc2a6654">15. Cross Product</a></li>
<li><a href="#org85f5c6f">16. How to translate a matrix</a></li>
<li><a href="#orgf22e5aa">17. Eigenvectors and eigenvalues</a></li>
<li><a href="#org9ec102c">18. Null Space</a></li>
<li><a href="#orgfedd36f">19. Singular Value Decomposition</a></li>
<li><a href="#orgdc23293">20. References</a></li>
</ul>
</div>
</div>
<div id="outline-container-org6176daf" class="outline-2">
<h2 id="org6176daf"><span class="section-number-2">1</span> Definitions</h2>
<div class="outline-text-2" id="text-1">
<p>
A matrix of dimensions M-by-N looks like:
</p>

\begin{equation}
  A = \begin{bmatrix}
    a_{11} & a_{12} & \dots & a_{1N} \\
    a_{21} & a_{22} & \dots & a_{2N} \\
    \vdots & \vdots & \vdots & \vdots \\
    a_{M1} & a_{M2} & \dots & a_{MN}
    \end{bmatrix}
\end{equation}

<p>
Its transpose flips the dimensions. In the below example transposing a
N-by-1 matrix makes it a 1-by-N matrix.
</p>

\begin{equation}
  a^T= \left[ a_1, a_2, \dots, a_N \right]
\end{equation}

<p>
The magnitude of a vector is given by \(\left| a \right| = \sqrt{a_1^2 +
a_2^2 + \dots + a_n^2}\).
</p>
</div>
</div>
<div id="outline-container-org05e1604" class="outline-2">
<h2 id="org05e1604"><span class="section-number-2">2</span> Linear Independence</h2>
<div class="outline-text-2" id="text-2">
<p>
A set of vectors \(\mathbf{x}\) is linearly dependent if there exist a set of
scalars \(\mathbf{\alpha}\), not all zero, such that \(\sum_{i=1}^m \alpha_i x_i = 0\).
If the only way to satisfy the equation is to have \(\alpha_i = 0 \forall i\), then
the set of vectors \(\mathbf{x}\) is linearly independent.
</p>
</div>
</div>

<div id="outline-container-orgf7095fb" class="outline-2">
<h2 id="orgf7095fb"><span class="section-number-2">3</span> Span</h2>
<div class="outline-text-2" id="text-3">
<p>
The "span" of \(\hat{v}\) and \(\hat{w}\) is the set of all their linear
combinations:
</p>

\begin{equation}
 a \hat{v} + b \hat{w}
\end{equation}
</div>
</div>

<div id="outline-container-org82e2b61" class="outline-2">
<h2 id="org82e2b61"><span class="section-number-2">4</span> Basis</h2>
<div class="outline-text-2" id="text-4">
<p>
The basis of a vector space is a set of linearly independent vectors
that span the full space.
</p>

<p>
If \(\mathbf{v}\) is the basis set, then any N-by-1 vector \(x\) can be
written as \(x = \sum^n_{i=1}c_i v_i\) for some scalar \(c_i\).
</p>
</div>
</div>

<div id="outline-container-org352addf" class="outline-2">
<h2 id="org352addf"><span class="section-number-2">5</span> Special Matrices</h2>
<div class="outline-text-2" id="text-5">
<dl class="org-dl">
<dt>diagonal matrix</dt><dd>a matrix with all off-diagonal entries equal to 0.</dd>
<dt>identity matrix</dt><dd>A diagonal matrix with diagonal entries equal to 1.</dd>
<dt>symmetric matrix</dt><dd>\(A = A^T\)</dd>
<dt>skew symmetric matrix</dt><dd>\(A = -A^T\)</dd>
</dl>
</div>
</div>

<div id="outline-container-orgcbd2be1" class="outline-2">
<h2 id="orgcbd2be1"><span class="section-number-2">6</span> Matrix properties</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>\((AB)^-1 = B^-1 A^{-1}\)</li>
<li>\((AB)^T = B^T A^T\)</li>
<li>\((A^{-1})^T = (A^T)^{-1}\)</li>
<li>\(A^{-1}^{}A = I\)</li>
<li>\((AB)C = A(BC)\)</li>
<li>\(A(B+C) = AB + AC\)</li>
<li>\((B+C)D = BD + CD\)</li>
</ul>
</div>
</div>

<div id="outline-container-orge24c731" class="outline-2">
<h2 id="orge24c731"><span class="section-number-2">7</span> Solutions to linear systems</h2>
<div class="outline-text-2" id="text-7">
<p>
Given the matrix \(A\) and vector \(b\), the problem \(Ax=b\) has a solution
iff the vector \(b\) can be expressed as a linear combination of the
columns of \(A\). If \(A\) is a square matrix and is invertible, then the
solution is \(x = A^{-1}b\).
</p>

<p>
If \(A\) is a rectangular matrix of size M-by-N, and if \(A^T A\) is
invertible (square and full rank), then the linear least squares
solution to \(x\) is given by \(x = (A^T A)^{-1} A^T b\).
</p>
</div>
</div>

<div id="outline-container-orge1136e8" class="outline-2">
<h2 id="orge1136e8"><span class="section-number-2">8</span> Matrices as Linear Transformations</h2>
<div class="outline-text-2" id="text-8">
<p>
A transformation is linear if it fulfills two properties:
</p>

<ol class="org-ol">
<li>All lines remain lines (they don't get curved)</li>
<li>The origin is fixed.</li>
</ol>

<p>
Under a linear transformation, grid lines remain parallel and evenly
spaced. This property allows us to compute the transformed vector,
only by recording how the basis vectors are transformed.
</p>


<div class="figure">
<p><img src="images/linear_algebra/Matrices%20as%20Linear%20Transformations/screenshot_2018-08-25_14-32-57.png" alt="screenshot_2018-08-25_14-32-57.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org0ec2f71" class="outline-2">
<h2 id="org0ec2f71"><span class="section-number-2">9</span> Matrix Multiplication as Composition</h2>
<div class="outline-text-2" id="text-9">
<p>
Often, we want to describe the effects of multiple linear
transformations composed together, for example, a rotation, followed
by a shear. The composition of linear transformations is also a linear
transformation, and can be described with a single matrix (see above).
</p>


<div class="figure">
<p><img src="images/linear_algebra/Matrix%20Multiplication%20as%20Composition/screenshot_2018-08-25_14-36-56.png" alt="screenshot_2018-08-25_14-36-56.png" />
</p>
</div>


<div class="figure">
<p><img src="images/linear_algebra/Matrix Multiplication as Composition/screenshot_2018-08-25_14-41-53.png" alt="screenshot_2018-08-25_14-41-53.png" />
</p>
</div>

<p>
Hence, we can think about matrix multiplication as computing where the
final basis vectors land.
</p>
</div>
</div>

<div id="outline-container-org5e82856" class="outline-2">
<h2 id="org5e82856"><span class="section-number-2">10</span> Determinant</h2>
<div class="outline-text-2" id="text-10">
<p>
The fact that linear transformations leave grid lines parallel and
evenly spaced, means that the area of each unit square is scaled by the
same amount.
</p>

<p>
The determinant of a transformation is the amount of scaling of area
of a unit square. If the determinant is negative, then the orientation
of the resulting grid space is reversed.
</p>



<div class="figure">
<p><img src="images/linear_algebra/Determinant/screenshot_2018-08-25_15-28-49.png" alt="screenshot_2018-08-25_15-28-49.png" />
</p>
</div>

<p>
In 3D space, the determinant is the volume of the parallelpiped. 
</p>

\begin{equation}
det \left( \begin{bmatrix}
  a & b & c \\
  d & e & f \\
  g & h & i \\
\end{bmatrix} \right)  = a \cdot det \left( \begin{bmatrix}
  e & f \\
  h & i
\end{bmatrix} \right)
- b \cdot det \left( \begin{bmatrix}
  d & f \\
  g & i
\end{bmatrix}  \right)
+ c \cdot det \left( \begin{bmatrix}
  d & e \\
  g & h
\end{bmatrix} \right)
\end{equation}
</div>
</div>

<div id="outline-container-orgbd29e76" class="outline-2">
<h2 id="orgbd29e76"><span class="section-number-2">11</span> Matrices for solving linear equations</h2>
<div class="outline-text-2" id="text-11">

<div class="figure">
<p><img src="images/linear_algebra/Matrices for solving linear equations/screenshot_2018-08-25_15-40-10.png" alt="screenshot_2018-08-25_15-40-10.png" />
</p>
</div>

<p>
Suppose we want to compute \(\vec{x}\) such that \(A\vec{x} = \vec{v}\).
Then we can compute the inverse of the matrix \(A\), which corresponds
to the inverse transformation. For example if \(A\) were to rotate the
grid space clockwise 90 degrees, then the inverse of \(A\) would be to
rotate the grid space anti-clockwise 90 degrees: \(\vec{x} = A^{^{-1}}
\vec{v}\).
</p>

<p>
Suppose the determinant of the transformation is 0. Then we know that
it does not have an inverse. However, solutions can still exist. 
</p>
</div>
</div>

<div id="outline-container-org11077f4" class="outline-2">
<h2 id="org11077f4"><span class="section-number-2">12</span> Rank</h2>
<div class="outline-text-2" id="text-12">
<p>
The rank is the number of dimensions of the output of the
transformation. It is easy to see that the maximum rank of the
transformation is the original dimensions of the matrix. Rank
corresponds to the maximal number of linearly independent columns of
\(A\).
</p>

<ul class="org-ul">
<li>\(r(AB) \le r(A)\)</li>
<li>\(r(AB) \le r(B)\)</li>
<li>\(r(AB) \le \text{min}(r(A), r(B))\)</li>
</ul>
</div>
</div>

<div id="outline-container-org06c6585" class="outline-2">
<h2 id="org06c6585"><span class="section-number-2">13</span> Column Space</h2>
<div class="outline-text-2" id="text-13">
<p>
The column space of the matrix \(A\) is the set of all possible outputs of
\(A \vec{v}\). It is also the span of all the columns.
</p>
</div>
</div>

<div id="outline-container-orgded891b" class="outline-2">
<h2 id="orgded891b"><span class="section-number-2">14</span> Dot Product</h2>
<div class="outline-text-2" id="text-14">
<p>
The dot product \(\vec{w} \cdot \vec{v}\) can be viewed as the
\((\text{length of projected vector }\vec{x}) \cdot (\text{length of
}\vec{x})\).
</p>

<p>
We can think of \(1 \times 2\) matrices as projection matrices, where the
first column indicates where \(\hat{i}\) lands, and the second column
indicates where \(\hat{j}\) lands. Suppose we have a vector \(\hat{i}\),
and we want to project it onto \(\hat{\mu}\). By symmetry, it's the same
value as when \(\hat{\mu}\) is projected onto \(\hat{i}\). However, this is
just the \(x\) coordinate value of \(\hat{\mu}\).
</p>


<div class="figure">
<p><img src="images/linear_algebra/Dot Product/screenshot_2018-08-25_16-03-15.png" alt="screenshot_2018-08-25_16-03-15.png" />
</p>
</div>

<p>
Hence, \(\hat{i}\) and \(\hat{j}\) land at \(\mu_x\) and \(\mu_y\) respectively. We
can easily see the duality between matrix-vector product and dot
product here.
</p>



<div class="figure">
<p><img src="images/linear_algebra/Dot Product/screenshot_2018-08-25_16-05-08.png" alt="screenshot_2018-08-25_16-05-08.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgc2a6654" class="outline-2">
<h2 id="orgc2a6654"><span class="section-number-2">15</span> Cross Product</h2>
<div class="outline-text-2" id="text-15">
<p>
The cross product of \(\vec{v}\) and \(\vec{w}\), denoted \(\vec{v} \times
\vec{w}\) is a vector. The vector has length equal to the area of a
parallelogram obtained by duplicating and shifting the two vectors.
The sign of the cross product is determined using the right-hand rule.
This vector is perpendicular to the parallelogram.
</p>


<div class="figure">
<p><img src="images/linear_algebra/Cross Product/screenshot_2018-08-25_16-29-46.png" alt="screenshot_2018-08-25_16-29-46.png" />
</p>
</div>



<div class="figure">
<p><img src="images/linear_algebra/Cross Product/screenshot_2018-08-25_16-33-08.png" alt="screenshot_2018-08-25_16-33-08.png" />
</p>
</div>

<p>
We want to find the dual vector \(\hat{p}\) that corresponds to the
cross product.
</p>


<div class="figure">
<p><img src="images/linear_algebra/Cross Product/screenshot_2018-08-25_16-39-00.png" alt="screenshot_2018-08-25_16-39-00.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org85f5c6f" class="outline-2">
<h2 id="org85f5c6f"><span class="section-number-2">16</span> How to translate a matrix</h2>
<div class="outline-text-2" id="text-16">
<p>
Suppose someone uses a different coordinate system (i.e. different
basis vectors), which we can represent with a matrix:
</p>

\begin{equation}
  \begin{bmatrix}
    2 & -1 \\
    1 & 1
  \end{bmatrix}
\end{equation}

<p>
Suppose then that we want to apply a linear transformation to a vector
in her coordinate system. In the case of a rotation 90 degrees
anti-clockwise, it would be represented in a matrix as:
</p>

\begin{equation}
  \begin{bmatrix}
    0 & -1 \\
    1 & 0
  \end{bmatrix}
\end{equation}

<p>
In the "default" basis vector coordinate system. What does this
transformation look like in the new coordinate system? Given some
vector \(\hat{v}\) in the other language. First, we translate the
vector into one in the default language:
</p>

\begin{equation}
  \begin{bmatrix}
    2 & -1 \\
    1 & 1
  \end{bmatrix}
  \hat{v}
\end{equation}

<p>
Then, we apply the transformation to the vector in the default
language:
</p>

\begin{equation}
  \begin{bmatrix}
    0 & -1 \\
    1 & 0
  \end{bmatrix}
  \begin{bmatrix}
    2 & -1 \\
    1 & 1
  \end{bmatrix}
  \hat{v}
\end{equation}

<p>
Then, we apply to the inverse of the change in basis matrix, to return
the vector to the other language:
</p>

\begin{equation}
  \begin{bmatrix}
    2 & -1 \\
    1 & 1
  \end{bmatrix}^{-1}
  \begin{bmatrix}
    0 & -1 \\
    1 & 0
  \end{bmatrix}
  \begin{bmatrix}
    2 & -1 \\
    1 & 1
  \end{bmatrix}
  \hat{v}
\end{equation}

<p>
This form \(A^{{-1}}MA\) is frequently encountered when dealing with
eigenvectors and eigenvalues.
</p>
</div>
</div>

<div id="outline-container-orgf22e5aa" class="outline-2">
<h2 id="orgf22e5aa"><span class="section-number-2">17</span> Eigenvectors and eigenvalues</h2>
<div class="outline-text-2" id="text-17">
<p>
Consider the span of a particular vector, that is, the set of vectors
obtainable by applying a scaling constant to it. Some vectors remain
on their own span, even with linear transformations.
</p>

<p>
These vectors are called <i>eigenvectors</i>, and the value of the scaling
constant is called the <i>eigenvalue</i>. Mathematically, this is expressed
as:
</p>

\begin{equation}
  A \hat{v} = \lambda \hat{v}
\end{equation}

<p>
Consider a 3D rotation. If we can find an eigenvector for this 3D
transformation, then we have found the axis of rotation.
</p>

\begin{equation}
  \sum_{i=1}^{n} \lambda_i = trace(A)
\end{equation}

\begin{equation}
  \prod_{i=1}^{n}\lambda_i = \lvert A \rvert
\end{equation}

<p>
Eigenvectors corresponding to different eigenvalues are linearly
independent.
</p>

<p>
Eigenvectors of a real symmetric matrix are orthogonal and real.
</p>
</div>
</div>

<div id="outline-container-org9ec102c" class="outline-2">
<h2 id="org9ec102c"><span class="section-number-2">18</span> Null Space</h2>
<div class="outline-text-2" id="text-18">
<p>
The null space of a matrix \(A\) consists of all vectors \(x\) such that
\(Ax = 0\). if \(A\) is rank-deficient, then there is a non-zero solution
for \(x\).
</p>
</div>
</div>

<div id="outline-container-orgfedd36f" class="outline-2">
<h2 id="orgfedd36f"><span class="section-number-2">19</span> Singular Value Decomposition</h2>
<div class="outline-text-2" id="text-19">
<p>
Given an input data matrix \(A\) consisting of \(m\) documents and \(n\)
terms, we can decompose it into 3 matrices.
</p>

\begin{equation}
  A_{[m \times n]} = U_{[m \times r]} \Sigma_{[r \times r]} V_{[n
    \times r]}^T
\end{equation}

<p>
\(U\) are left singular vectors of size \(m \times r\), which we can think of
as \(m\) documents and \(r\) concepts. \(\Sigma\) is a \(r \times r\) diagonal matrix,
representing the strength of each concept, where \(r\) is the rank of
the matrix \(A\). \(V\) stores the right singular vectors, consisting of
\(n\) terms and \(r\) concepts.
</p>


<div class="figure">
<p><img src="images/linear_algebra/Singular Value Decomposition/screenshot_2018-08-25_17-26-42.png" alt="screenshot_2018-08-25_17-26-42.png" />
</p>
</div>

<p>
It is always possible to decompose a real matrix \(A\) into \(A = U \Sigma
V^T\), where:
</p>

<ol class="org-ol">
<li>\(U\), \(\Sigma\), and \(V\) are unique</li>
<li>\(U\) and \(V\) are column orthonormal: \(U^T U = I\), \(V^T V = I\)</li>
<li>\(\Sigma\) is diagonal, and entries are positive, sorted in decreasing
order.</li>
</ol>

<p>
The entries on the diagonal of \(\Sigma\) are known as the singular values,
and they are the squareroots of the eigenvalues of both \(AA^T\) and \(A^T
A\). The number of non-zero singular values equals to the rank of \(A\).
</p>

<p>
SVD can be used to solve linear equations of the form \(Ax = b\), where
\(A\) is known, and \(b\) is the zero-vector. We can do a SVD of \(A\) and
write the solution for \(x\):
</p>

\begin{equation}
  U \Sigma V^T x = 0
\end{equation}

<p>
If A is full-rank, then \(x\) must be the zero-vector. Else, \(x\) has a
non-zero solution equal to the linear combination of the last few rows
of \(V^T\) corresponding to zero singular values.
</p>



<div class="figure">
<p><img src="images/linear_algebra/Singular%20Value%20Decomposition/screenshot_2018-11-18_17-32-26.png" alt="screenshot_2018-11-18_17-32-26.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgdc23293" class="outline-2">
<h2 id="orgdc23293"><span class="section-number-2">20</span> References</h2>
<div class="outline-text-2" id="text-20">
<ol class="org-ol">
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra</a></li>
<li><a href="https://www.youtube.com/watch?v=P5mlg91as1c">Lecture 47 - Singular Value Decomposition | Stanford University</a></li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-11-19 Mon 16:53</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
