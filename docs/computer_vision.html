<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-11-24 Sat 20:31 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Computer Vision</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Computer Vision</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org6a01959">1. Mathematics Fundamentals</a>
<ul>
<li><a href="#org6cd7835">1.1. Taylor Approximation</a></li>
<li><a href="#org9d9660b">1.2. Linear Algebra</a></li>
</ul>
</li>
<li><a href="#org2e9dc64">2. Camera Basics</a>
<ul>
<li><a href="#org5e5ff53">2.1. Single Lens Reflex (SLR)</a></li>
<li><a href="#org43b91da">2.2. DSLR (Digital SLR)</a></li>
</ul>
</li>
<li><a href="#orga7e0ed0">3. Introduction</a></li>
<li><a href="#org1e4cc08">4. Image Formation</a>
<ul>
<li><a href="#org1754238">4.1. Geometric Primitives</a></li>
<li><a href="#org9c74fb8">4.2. 2D Transformations</a></li>
<li><a href="#org61357e7">4.3. 3D Rotations</a>
<ul>
<li><a href="#org230a180">4.3.1. Euler Angles</a></li>
<li><a href="#orge235f67">4.3.2. Axis/angle (exponential twist)</a></li>
<li><a href="#org56c7059">4.3.3. Unit Quarternions</a></li>
</ul>
</li>
<li><a href="#org2ed3feb">4.4. 3D to 2D projections</a>
<ul>
<li><a href="#orga085c11">4.4.1. Orthography</a></li>
<li><a href="#org8256be7">4.4.2. Perspective</a></li>
<li><a href="#orgf14df1f">4.4.3. Camera Instrinsics</a></li>
<li><a href="#orgebdeb0b">4.4.4. Lens distortion</a></li>
<li><a href="#orga1c24cc">4.4.5. Camera Calibration</a>
<ul>
<li><a href="#orgb7c056e">4.4.5.1. Extrinsic Parameters</a></li>
<li><a href="#orgea88b61">4.4.5.2. Intrinsic Parameters</a></li>
<li><a href="#org082cbe3">4.4.5.3. Combining Extrinsic and Intrinsic Calibration Parameters</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgb9a89cf">4.5. Photometric image formation</a>
<ul>
<li><a href="#orgcd07261">4.5.1. Lighting</a></li>
<li><a href="#org61a7b1a">4.5.2. Reflectance and shading</a>
<ul>
<li><a href="#orgb1c1607">4.5.2.1. The Bidirectional Reflectance Distribution Function (BRDF)</a></li>
<li><a href="#org8551a0c">4.5.2.2. Diffuse Reflection</a></li>
<li><a href="#org2942664">4.5.2.3. Specular Reflection</a></li>
<li><a href="#orgad88c5d">4.5.2.4. Phong Shading</a></li>
<li><a href="#org981ec01">4.5.2.5. Optics</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org259ca6c">5. Image Processing</a>
<ul>
<li><a href="#org3a94655">5.1. Point Operators</a></li>
<li><a href="#org944f247">5.2. Image Enhancement</a>
<ul>
<li><a href="#org5883772">5.2.1. Histogram Equalization</a></li>
<li><a href="#org1fad2d4">5.2.2. Convolutions</a></li>
</ul>
</li>
<li><a href="#orge109e63">5.3. Colour</a>
<ul>
<li><a href="#org9fc08e0">5.3.1. Measuring Colour Differences</a></li>
<li><a href="#org3e0bc21">5.3.2. Computing Means</a></li>
<li><a href="#org6ace06e">5.3.3. Digital Cameras sensing colour</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgb46cca5">6. Change Detection</a></li>
<li><a href="#org3944902">7. Motion Tracking</a>
<ul>
<li><a href="#org0b4a308">7.1. Feature-based</a>
<ul>
<li><a href="#orgbfc8e9c">7.1.1. Harris corner detector</a>
<ul>
<li><a href="#orga087e16">7.1.1.1. Properties</a></li>
</ul>
</li>
<li><a href="#org656134f">7.1.2. Tomasi corner detector</a></li>
</ul>
</li>
<li><a href="#org79a1b6a">7.2. Gradient-based</a>
<ul>
<li><a href="#orgcb12734">7.2.1. Lucas-Kanade method</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgf0baa5b">8. Homography</a></li>
<li><a href="#org97c3615">9. Structure For Motion</a></li>
</ul>
</div>
</div>

<div id="outline-container-org6a01959" class="outline-2">
<h2 id="org6a01959"><span class="section-number-2">1</span> Mathematics Fundamentals</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org6cd7835" class="outline-3">
<h3 id="org6cd7835"><span class="section-number-3">1.1</span> Taylor Approximation</h3>
<div class="outline-text-3" id="text-1-1">
<p>
The Taylor series of a function \(f(x)\) at \(x=a\) is given by:
</p>

\begin{equation}
  f(x) = \sum_{n=0}^{\infty} \frac{f^n(a)}{n!} (x-a)^n
\end{equation}
</div>
</div>

<div id="outline-container-org9d9660b" class="outline-3">
<h3 id="org9d9660b"><span class="section-number-3">1.2</span> <a href="linear_algebra.html">Linear Algebra</a></h3>
</div>
</div>

<div id="outline-container-org2e9dc64" class="outline-2">
<h2 id="org2e9dc64"><span class="section-number-2">2</span> Camera Basics</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org5e5ff53" class="outline-3">
<h3 id="org5e5ff53"><span class="section-number-3">2.1</span> Single Lens Reflex (SLR)</h3>
<div class="outline-text-3" id="text-2-1">
<p>
A camera that typically uses a mirror and prism system that permits
the photographer to view through the lens and see exactly what will be
captured.
</p>


<div class="figure">
<p><img src="images/computer_vision/Camera%20Basics/slr2_2018-11-18_23-35-22.gif" alt="slr2_2018-11-18_23-35-22.gif" />
</p>
</div>
</div>
</div>


<div id="outline-container-org43b91da" class="outline-3">
<h3 id="org43b91da"><span class="section-number-3">2.2</span> DSLR (Digital SLR)</h3>
<div class="outline-text-3" id="text-2-2">

<div class="figure">
<p><img src="images/computer_vision/Camera%20Basics/DSLR_stru_2018-11-18_23-37-03.jpeg" alt="DSLR_stru_2018-11-18_23-37-03.jpeg" />
</p>
</div>

<ol class="org-ol">
<li>Matte focusing screen: screen on which the light passing through
the lens will project</li>
<li>Condensing lens: A lens that is used to concentrate the incoming light</li>
<li>Pentaprism: To produce a correctly oriented, right side up image
and project it into the viewfinder eyepiece</li>
<li>AF sensor: accomplishes autofocus</li>
<li>Viewfinder eyepiece: Allows us to see what will be recorded by the
image sensor</li>
<li>LCD Screen: display stored photos or what will be recorded by image sensor</li>
<li>Image sensor: contains a large number of pixels for converting an
optical image into electrical signals. Charge-coupled device (CCD)
and Complementary Metal-oxide-semiconductor (CMOS) are the more
common ones.</li>
<li>AE sensor: accomplishes auto exposure</li>
<li>Sub mirror: To reflect light passing through semi transparent main
mirror onto AF sensor</li>
<li>Main mirror: reflect light into viewfinder compartment. Small
semi-transparent area to facilitate AF.</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-orga7e0ed0" class="outline-2">
<h2 id="orga7e0ed0"><span class="section-number-2">3</span> Introduction</h2>
<div class="outline-text-2" id="text-3">
<p>
Despite the advances in research in computer vision, the dream of
having a computer interpret an image at the same level of a human is
still far away. Computer vision is inherently a difficult problem, for
many reasons. First, it is an <i>inverse problem</i>, in which we seek to
recover some unknowns given insufficient information to specify the
solution. Hence, we resort to physics-based and probabilistic models
to disambiguate between potential solutions.
</p>

<p>
Forward models that we use in computer vision are usually grounded in
physics and computer graphics. Both these fields model how objects
move and animate, how light reflects off their surfaces, is scattered
by the atmosphere, refracted through camera lenses and finally
projected onto a flat image plane.
</p>

<p>
In computer vision, we want to describe the world that we see in one
or more images and to reconstruct its properties, such as shape,
illumination and color distributions. Some examples of computer vision
being used in real-world applications include Optical Character
Recognition (OCR) , machine inspection, retail, medical imaging, and
automotive safety.
</p>

<p>
In many applications, it is better to think back from the problem at
hand to suitable techniques, typical of an engineering approach. A
heavy emphasis will be placed on algorithms that are robust to noise,
and are reasonably efficient.
</p>



<div class="figure">
<p><img src="images/computer_vision/screenshot_2018-08-18_21-17-41.png" alt="screenshot_2018-08-18_21-17-41.png" />
</p>
</div>

<p>
The above figure shows a rough layout of the content of computer
vision, and we see that from top to bottom, there are increasing
levels of modeling and abstraction.
</p>
</div>
</div>

<div id="outline-container-org1e4cc08" class="outline-2">
<h2 id="org1e4cc08"><span class="section-number-2">4</span> Image Formation</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org1754238" class="outline-3">
<h3 id="org1754238"><span class="section-number-3">4.1</span> Geometric Primitives</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Geometric primitives form the basic building blocks used to describe
three-dimensional shapes.
</p>

<p>
2D points can be denoted using a pair of values, \(x = (x, y) \in
\mathbb{R}^2\):
</p>

\begin{equation}
  x = \begin{bmatrix}
    x \\
    y
  \end{bmatrix}
\end{equation}

<p>
2D points can also be represented using homogeneous coordinates,
 \(\tilde{\mathbf{x}} = (\tilde{x}, \tilde{y}, \tilde{w}) \in \mathbb{P}^2\), where vectors
 that differ only by scale are considered to be equivalent.
 \(\mathbb{P}^2 = \mathbb{R}^3 - (0, 0, 0)\) is called the <i>2D projective
 space</i>.
</p>

<p>
A homogeneous vector \(\tilde{\mathbf{x}}\) can be converted back into an
inhomogeneous vector \(\mathbf{x}\) by diving through the last element
\(\tilde{w}\).
</p>

<p>
2D lines can be represented using homogeneous coordinates
\(\tilde{\mathbf{l}} = (a, b, c)\). The corresponding line equation is:
</p>

\begin{equation}
  \bar{\mathbf{x}} \cdot \tilde{\mathbf{l}} = ax + by + c = 0
\end{equation}

<p>
The line equation vector can be normalized so that \(\mathbf{l} =
(\hat{n}_x, \hat{n}_y, d) = (\hat{\mathbf{n}}, d)\)
 with \(\lvert
\hat{\mathbf{n}} \rvert = 1\). When using homogeneous coordinates, we
can compute the intersection of two lines as 
</p>

\begin{equation}
  \tilde{\mathbf{x}} = \tilde{\mathbf{l}}_1 \times \tilde{\mathbf{l}}_2
\end{equation}

<p>
Similarly, the line joining two points can be written as:
</p>

\begin{equation}
  \tilde{\mathbf{l}} = \tilde{\mathbf{x}}_1 \times \tilde{\mathbf{x}}_2
\end{equation}

<p>
Conic sections (which arise from the intersection of a plane and a 3d
cone) can be written using a quadric equation
</p>

\begin{equation}
  \tilde{\mathbf{x}}^T\mathbf{Q}\tilde{\mathbf{x}} = 0
\end{equation}

<p>
3D points can be written using inhomogeneous coordinates \(\mathbf{x} =
(x,y,z) \in \mathbb{R}^3\), or homogeneous coordinates \(\tilde{\mathbf{x}} =
(\tilde{x}, \tilde{y}, \tilde{z}, \tilde{w}) \in \mathbb{P}^3\).
</p>

<p>
3D planes can be represented as homogeneous coordinates \(\tilde{\mathbf{m}}
= (a, b, c, d)\) with the equation:
</p>

\begin{equation}
\bar{\mathbf{x}} \cdot \tilde{\mathbf{m}} = ax + by + cz + d = 0
\end{equation}

<p>
3D lines can be represented using 2 points on the line \((\mathbf{p},
\mathbf{q})\). Any other point on the line can be expressed as a linear
combination of these 2 points.
</p>

\begin{equation}
  \mathbf{r} = (1 - \lambda)\mathbf{p} + \lambda \mathbf{q}
\end{equation}
</div>
</div>

<div id="outline-container-org9c74fb8" class="outline-3">
<h3 id="org9c74fb8"><span class="section-number-3">4.2</span> 2D Transformations</h3>
<div class="outline-text-3" id="text-4-2">
<p>
The basic primitives introduced above can be transformed, the simplest
of which occur in the 2D plane.
</p>



<div class="figure">
<p><img src="images/computer_vision/Image Formation/screenshot_2018-08-19_13-49-15.png" alt="screenshot_2018-08-19_13-49-15.png" />
</p>
</div>

<p>
2D translations can be written as \(\mathbf{x}' = \mathbf{x} +
\mathbf{t}\), or:
</p>

\begin{align}
  \mathbf{x}' &= \begin{bmatrix}
              \mathbf{I} & \mathbf{t}
              \end{bmatrix}\bar{\mathbf{x}} \\
              &= \begin{bmatrix}
                 \mathbf{I} & \mathbf{t} \\
                 \mathbf{0}^T & 1
              \end{bmatrix}\bar{\mathbf{x}}
\end{align}

<p>
where \(\mathbf{0}\) is the zero vector.
</p>

<p>
The combination of rotation and translation is known as 2D <i>rigid body
motion</i>, or the 2D Euclidean transformation, since Euclidean distances
are preserved. It can be written as \(\mathbf{x}' =
\mathbf{R}\mathbf{x} + \mathbf{t}\) or:
</p>

\begin{equation}
  \mathbf{x}' = \begin{bmatrix}
              \mathbf{R} & \mathbf{t}
              \end{bmatrix}\bar{\mathbf{x}}
\end{equation}

<p>
where
</p>

\begin{equation}
  \mathbf{R} = \begin{bmatrix}
    \cos \theta & - \sin \theta \\
    \sin \theta & \cos \theta
  \end{bmatrix}
\end{equation}

<p>
is an orthonormal rotation matrix with
\(\mathbf{R}\mathbf{R}^T=\mathbf{I}\) and \(\lVert R \rVert = 1\).
</p>

<p>
The <b>similarity transform</b>, or scaled rotation, can be expressed as
\(\mathbf{x}' = s\mathbf{R}\mathbf{x} + \mathbf{t}\). This preserves
angles between lines.
</p>

<p>
The <b>affine transformation</b> is written as \(\mathbf{x}' =
\mathbf{A}\hat{\mathbf{x}}\), where \(\mathbf{A}\) is an arbitrary \(2 \times
3\) matrix.
</p>

<p>
Parallel lines remain parallel under affine transformations.
</p>

<p>
Affine transformations with 6 unknowns can be solved via SVD by
forming a matrix equation of the form \(Mx = b\). Local transformations
apply different transformations to different regions, and give finer control.
</p>

<p>
The <b>projective transformation</b>, also known as the perspective transform
or homography, operates on homogeneous coordinates:
</p>

\begin{equation}
  \hat{\mathbf{x}}' = \tilde{\mathbf{H}}\tilde{\mathbf{x}}
\end{equation}

<p>
where \(\tilde{\mathbf{H}}\) is an arbitrary \(3 \times 3\) matrix. Note that
\(\tilde{\mathbf{H}}\) is homogeneous.
</p>

<p>
Each of these transformation preserves some properties, and can be
presented in a hierarchy.
</p>


<div class="figure">
<p><img src="images/computer_vision/Image Formation/screenshot_2018-08-19_14-02-51.png" alt="screenshot_2018-08-19_14-02-51.png" />
</p>
</div>

<p>
Some transformations that cannot be classified so easily include:
</p>

<ol class="org-ol">
<li>Stretching and Squashing</li>
<li>Planar surface flow</li>
<li>Bilinear interpolant</li>
</ol>

<p>
The set of 3D transformations are very similar to the 2D
transformations.
</p>


<div class="figure">
<p><img src="images/computer_vision/Image Formation/screenshot_2018-08-19_14-05-15.png" alt="screenshot_2018-08-19_14-05-15.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org61357e7" class="outline-3">
<h3 id="org61357e7"><span class="section-number-3">4.3</span> 3D Rotations</h3>
<div class="outline-text-3" id="text-4-3">
<p>
The biggest difference between 2D and 3D coordinate transformations is
that the parameterization of the 3D rotation matrix \(\mathbf{R}\) is
not as straightforward.
</p>
</div>

<div id="outline-container-org230a180" class="outline-4">
<h4 id="org230a180"><span class="section-number-4">4.3.1</span> Euler Angles</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
A rotation matrix can be formed as the product of three rotations
around three cardinal axes, e.g. \(x\), \(y\), and \(z\). This is generally
a bad idea, because the result depends on the order of
transformations, and it is not always possible to move smoothly in a
parameter space.
</p>
</div>
</div>

<div id="outline-container-orge235f67" class="outline-4">
<h4 id="orge235f67"><span class="section-number-4">4.3.2</span> Axis/angle (exponential twist)</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
A rotation can be represented by a rotation axis \(\hat{\mathbf{n}}\)
and an angle \(\theta\), or equivalently by a 3D vector \(\mathbf{\omega} =
\theta\hat{\mathbf{n}}\). We can write the rotation matrix corresponding to
a rotation by \(\theta\) around an axis \(\hat{\mathbf{n}}\) as:
</p>

\begin{equation}
  \mathbf{R}(\hat{\mathbf{n}}, \theta) = \mathbf{I} + \sin \theta
  [\hat{\mathbf{n}}]_\times + \left(1-\cos\theta\right)[\hat{\mathbf{n}}]^2_\times
\end{equation}

<p>
Also known as <i>Rodriguez's formula</i>.
</p>

<p>
For small rotations, this is an excellent choice, as it simplifies to:
</p>

\begin{equation}
  \mathbf{R}(\mathbf{\omega}) \approx \mathbf{I} + \sin\theta[\hat{\mathbf{n}}]_\times = \begin{bmatrix}
    1 & -\omega_x & -\omega_y \\
    \omega_z & 1 & -\omega_x \\
    -\omega_y & \omega_x & 1
  \end{bmatrix}
\end{equation}

<p>
This gives a nice linearized relationship between the rotation
parameters \(\omega\) and \(\mathbf{R}\).  We can also compute the derivative
of \(\mathbf{R}v\) with respect to \(\omega\),
</p>

\begin{equation}
\frac{\partial \mathbf{R}v}{\partial \omega^T} = -[\mathbf{v}]_\times = \begin{bmatrix}
  0 & z & -y \\
  -z & 0 & x \\
  y & -x & 0
\end{bmatrix}
\end{equation}
</div>
</div>

<div id="outline-container-org56c7059" class="outline-4">
<h4 id="org56c7059"><span class="section-number-4">4.3.3</span> Unit Quarternions</h4>
<div class="outline-text-4" id="text-4-3-3">
<p>
<a href="https://eater.net/quaternions">https://eater.net/quaternions</a>
<a href="https://www.youtube.com/watch?v=d4EgbgTm0Bg">https://www.youtube.com/watch?v=d4EgbgTm0Bg</a>
</p>

<p>
A unit quarternion is a unit length 4-vector whose components can be
written as \(\mathbf{q} = (x, y, z, w)\). Unit quarternions live on the
unit sphere \(\lVert q \rVert = 1\) and antipodal quartenions, \(q\) and
\(-q\) represent the same rotation. This representation is continuous
and are very popular representations for pose and for pose
interpolation.
</p>

<p>
Quarternions can be derived from the axis/angle representation through
the formula:
</p>

\begin{equation}
  \mathbf{q} = (\mathbf{v}, w) = \left(\sin\frac{\theta}{2}\hat{\mathbf{n}}, \cos\frac{\theta}{2}\right)
\end{equation}

<p>
where \(\hat{\mathbf{n}}\) and \(\theta\) are the rotation axis and angle.
Rodriguez's formula can be converted to:
</p>

\begin{equation}
  \mathbf{R}(\hat{\mathbf{n}}, \theta) = \mathbf{I} + 2w[\mathbf{v}]_\times + 2[\mathbf{v}]^2_\times
\end{equation}

<p>
The nicest aspect of unit quarternions is that there is a simple
algebra for composing rotations expressed as unit quartenions:
</p>

\begin{equation}
  \mathbf{q}_2 = \mathbf{q}_0 \mathbf{q}_1 = (\mathbf{v}_0 \times \mathbf{v}_1 + w_0 \mathbf{v}_1 + w_1 \mathbf{v}_0, w_0 w_1 - \mathbf{v}_0 \cdot \mathbf{v}_1)
\end{equation}

<p>
The inverse of a quarternion is just flipping the sign of \(\mathbf{v}\)
or \(w\), but not both. Then quarternion division can be defined as:
</p>

\begin{equation}
  \mathbf{q}_2 = \mathbf{q}_0 / \mathbf{q}_1 = (\mathbf{v}_0 \times \mathbf{v}_1 + w_0 \mathbf{v}_1 - w_1 \mathbf{v}_0, - w_0 w_1 - \mathbf{v}_0 \cdot \mathbf{v}_1)
\end{equation}
</div>
</div>
</div>


<div id="outline-container-org2ed3feb" class="outline-3">
<h3 id="org2ed3feb"><span class="section-number-3">4.4</span> 3D to 2D projections</h3>
<div class="outline-text-3" id="text-4-4">

<div class="figure">
<p><img src="images/computer_vision/Image Formation/screenshot_2018-08-20_17-35-43.png" alt="screenshot_2018-08-20_17-35-43.png" />
</p>
</div>

<p>
We need to specify how 3D primitives are projected onto the image
plane. The simplest model is orthography, which requires no division
to get the final (inhomogeneous) result. The more commonly used model
is perspective, since this more accurately models the behaviour of
real cameras.
</p>
</div>

<div id="outline-container-orga085c11" class="outline-4">
<h4 id="orga085c11"><span class="section-number-4">4.4.1</span> Orthography</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
An orthographic projection simply drops the \(z\) component of the
three-dimensional coordinate \(\mathbf{p}\) to obtain the 2D point
\(\mathbf{x}\).
</p>

\begin{equation}
  \mathbf{x} = \left[\mathbf{I}_{2\times 2} | \mathbf{0} \right] \mathbf{p}
\end{equation}

<p>
In practice, world coordinates need to be scaled to fit onto an image
sensor, for this reason, <i>scaled orthography</i> is actually more commonly
used:
</p>

\begin{equation}
\mathbf{x} = \left[s\mathbf{I}_{2 \times 2}\right | \mathbf{0}]\mathbf{p}
\end{equation}

<p>
This model is equivalent to first projecting the world points onto a
local fronto-parallel image plane, and then scaling this image using
regular perspective projection.
</p>

<p>
A closely related model is called <i>para-perspective</i>, which projects the
object points onto a local reference plane parallel to the image
plane. However, rather than being projected orthogonally to this
plane, they are projected parallel to the line of sight to the object
center. This is followed by the usual projection onto the final image
plane, and the combination of these two projections is affine.
</p>

\begin{equation}
\tilde{\mathbf{x}} = \begin{bmatrix}
  a_{00} & a_{01} & a_{02} & a_{03} \\
  a_{10} & a_{11} & a_{12} & a_{13} \\
  0 & 0 & 0 & 1
\end{bmatrix}
\tilde{\mathbf{p}}
\end{equation}
</div>
</div>

<div id="outline-container-org8256be7" class="outline-4">
<h4 id="org8256be7"><span class="section-number-4">4.4.2</span> Perspective</h4>
<div class="outline-text-4" id="text-4-4-2">
<p>
Points are projected onto the image plane by dividing them by their
\(z\) component. Using homogeneous coordinates, this can be written as:
</p>

\begin{equation}
\tilde{\mathbf{x}} = \mathcal{P}_z(\mathbf{p}) = \begin{bmatrix}
x / z \\
y / z \\
1
\end{bmatrix}
\end{equation}

<p>
In homogeneous coordinates, the projection has a simple linear form,
</p>

\begin{equation}
\tilde{\mathbf{x}} = \begin{bmatrix}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
\end{bmatrix}\tilde{\mathbf{p}}
\end{equation}

<p>
we drop the \(w\) component of \(\mathbf{p}\). Thus after projection, we
are unable to recover the distance of the 3D point from the image.
</p>
</div>
</div>

<div id="outline-container-orgf14df1f" class="outline-4">
<h4 id="orgf14df1f"><span class="section-number-4">4.4.3</span> Camera Instrinsics</h4>
<div class="outline-text-4" id="text-4-4-3">
<p>
Once we have projected a 3D point through an ideal pinhole using a
projection matrix, we must still transform the resulting coordinates
according to the pixel sensor spacing and the relative position of the
sensor plane to the origin.
</p>

<p>
Image sensors return <i>pixel values</i> indexed by integer pixel coordinates
\((x_s, y_s)\). To map pixel centers to 3D coordinates, we first scalet he
\((x_s, y_s)\) values by the pixel spacings \((s_x, s_y)\), and then describe
the orientation of the sensor array relative to the camera projection
center \(\mathbf{O}_c\) with an origin \(\mathbf{c}_s\) and a 3D rotation
\(\mathbf{R}_s\).
</p>

\begin{equation}
\mathbf{p} = \left[\mathbf{R}_s | \mathbf{c}_s \right] \begin{bmatrix}
s_x & 0 & 0 \\
0 & s_y & 0 \\
0 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix} \begin{bmatrix}
x_s \\
y_s \\
1
\end{bmatrix} = \mathbf{M}_s \hat{\mathbf{x}}_s
\end{equation}

<p>
The first 2 columns of the \(3 \times 3\) matrix \(\mathbf{M}_s\) are the 3D vectors
corresponding to the unit steps in the image pixel array along the
\(x_s\) and \(y_s\) directions, while the third column is the 3D image array
origin \(\mathbf{c}_s\).
</p>

<p>
The matrix \(\mathbf{M}_s\) is parameterized by 8 unknowns, and that
makes estimating the camera model impractical, even though there are
really only 7 degrees of freedom. Most practitioners assume a general
\(3 \times 3\) homogeneous matrix form.
</p>

<p>
<a href="http://ksimek.github.io/2013/08/13/intrinsic/">http://ksimek.github.io/2013/08/13/intrinsic/</a>
</p>

\begin{align}
    P &= \overbrace{K}^\text{Intrinsic Matrix} \times \overbrace{[R \mid  \mathbf{t}]}^\text{Extrinsic Matrix} \\[0.5em]
     &= 
        \overbrace{

            \underbrace{
                \left (
                \begin{array}{ c c c}
                 1  &  0  & x_0 \\
                 0  &  1  & y_0 \\
                 0  &  0  & 1
                \end{array}
                \right )
            }_\text{2D Translation}

            \times

            \underbrace{
                \left (
                \begin{array}{ c c c}
                f_x &  0  & 0 \\
                 0  & f_y & 0 \\
                 0  &  0  & 1
                \end{array}
                \right )
            }_\text{2D Scaling}

            \times

            \underbrace{
                \left (
                \begin{array}{ c c c}
                 1  &  s/f_x  & 0 \\
                 0  &    1    & 0 \\
                 0  &    0    & 1
                \end{array}
                \right )
            }_\text{2D Shear}

        }^\text{Intrinsic Matrix}

        \times

        \overbrace{
        \underbrace{
             \left( \begin{array}{c | c} 
            I & \mathbf{t}
             \end{array}\right)
        }_\text{3D Translation}
        \times
        \underbrace{
             \left( \begin{array}{c | c} 
            R & 0 \\ \hline
            0 & 1
             \end{array}\right)
        }_\text{3D Rotation}
        }^\text{Extrinsic Matrix}
    \end{align}
</div>
</div>

<div id="outline-container-orgebdeb0b" class="outline-4">
<h4 id="orgebdeb0b"><span class="section-number-4">4.4.4</span> Lens distortion</h4>
<div class="outline-text-4" id="text-4-4-4">
<p>
Thus far, it has been assumed that the cameras obey a linear
projection model. In reality, many wide-angled lens suffer heavily
from radial distortion, which manifests itself as a visible curvature
in the projection of straight lines. Fortunately, compensating for
radial distortion  is not that difficult in practice. The radial
distortion model says that the coordinates in the observed images are
displaced away (barrel distortion) or towards (pincushion distortion)
the image center by an amount proportional to their radial distance.
</p>



<div class="figure">
<p><img src="images/computer_vision/Image Formation/screenshot_2018-08-20_18-17-31.png" alt="screenshot_2018-08-20_18-17-31.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orga1c24cc" class="outline-4">
<h4 id="orga1c24cc"><span class="section-number-4">4.4.5</span> Camera Calibration</h4>
<div class="outline-text-4" id="text-4-4-5">
<p>
We want to use the camera to tell us things about the world, so we
need the relationship between coordinates in the world, and
coordinates in the image.
</p>

<p>
Geometric camera calibration is composed of:
</p>

<dl class="org-dl">
<dt>extrinsic parameters (camera pose)</dt><dd>from some arbitrary world
coordinate system to the camera's 3D coordinate system</dd>
<dt>intrinsic parameters</dt><dd>From the 3D coordinates in the camera frame
to the 2D image plane via projection</dd>
</dl>
</div>

<div id="outline-container-orgb7c056e" class="outline-5">
<h5 id="orgb7c056e"><span class="section-number-5">4.4.5.1</span> Extrinsic Parameters</h5>
<div class="outline-text-5" id="text-4-4-5-1">
<p>
The transform \(T\) is a transform that goes from the world to the
camera system.
</p>
</div>
<ol class="org-ol">
<li><a id="org7ad7a48"></a>Translation<br />
<div class="outline-text-6" id="text-4-4-5-1-1">
<p>
The coordinate \(P\) in \(B\)'s frame is the coordinate \(P\) in frame \(A\),
and the location of the camera in frame \(B\).
</p>

\begin{equation}
  ^B P = ^A P + ^B O_A
\end{equation}

\begin{equation}
  \begin{bmatrix}
    ^B P \\
    1
    \end{bmatrix} = \begin{bmatrix}
      I_{3\times3} & ^B O_A \\
      0^T & 1
    \end{bmatrix} \begin{bmatrix}
      ^A P \\
      1
    \end{bmatrix}
\end{equation}
</div>
</li>

<li><a id="org3daf536"></a>Rotation<br />
<div class="outline-text-6" id="text-4-4-5-1-2">
<p>
We can similarly describe a rotation matrix:
</p>

\begin{equation}
^B P = ^B _A R ^AP
\end{equation}

\begin{equation}
  ^B_A R = \begin{bmatrix}
    ^B i_A & ^B j_A & ^B k_A
  \end{bmatrix} =
  \begin{bmatrix}
    ^Ai_B^T \\
    ^Aj_B^T \\
    ^Ak_B^T
  \end{bmatrix}
\end{equation}

<p>
Under homogeneous coordinates, rotation can also be expressed as a
matrix multiplication.
</p>

\begin{equation}
  \begin{bmatrix}
    ^B P \\
    1
  \end{bmatrix} = \begin{bmatrix}
    ^B_AR & 0 \\
    0^T & 1
  \end{bmatrix} \begin{bmatrix}
    ^A P \\
    1
  \end{bmatrix}
\end{equation}

<p>
Then, we can express rigid transformations as:
</p>

\begin{equation}
  \begin{bmatrix}
    ^B P \\
    1
  \end{bmatrix} = \begin{bmatrix}
    1 & ^BO_A \\
    0^T & 1 \\
  \end{bmatrix} \begin{bmatrix}
    ^B_AR & 0 \\
    0^T & 1 \\
  \end{bmatrix} \begin{bmatrix}
    ^A P \\
    1
  \end{bmatrix} = \begin{bmatrix}
    ^B_AR & ^BO_A \\
    0^T & 1
  \end{bmatrix} \begin{bmatrix}
    ^A P \\
    1
\end{equation}

<p>
And we write:
</p>

\begin{equation}
^B_A T = \begin{bmatrix}
    ^B_AR & ^BO_A \\
    0^T & 1
  \end{bmatrix} 
\end{equation}



<div class="figure">
<p><img src="images/computer_vision/Image%20Formation/screenshot_2018-11-24_13-05-41.png" alt="screenshot_2018-11-24_13-05-41.png" />
</p>
</div>

<p>
The world to camera transformation matrix is the extrinsic parameter
matrix (4x4).
</p>



<div class="figure">
<p><img src="images/computer_vision/Image%20Formation/screenshot_2018-11-24_13-10-13.png" alt="screenshot_2018-11-24_13-10-13.png" />
</p>
</div>


<p>
The rotation matrix \(R\) has two important properties:
</p>

<ol class="org-ol">
<li>\(R\) is orthonormal: \(R^T R = I\)</li>
<li>\(|R| = 1\)</li>
</ol>

<p>
One can represent rotation using euler angles:
</p>

<dl class="org-dl">
<dt>pitch (\(\omega\))</dt><dd>rotation about x-axis</dd>
<dt>yaw (\(\phi\))</dt><dd>rotation about y-axis</dd>
<dt>roll (\(\kappa\))</dt><dd>rotation about z-axis</dd>
</dl>

<p>
Euler angles can be converted to rotation matrix:
</p>

\begin{align}
  R &= R_x R_y R_z
\end{align}

<p>
Rotations can also be specified as a right-handed rotation by an angle
\(\theta\) about the axis specified by the unit vector \(\left(\omega_x,
\omega_y, \omega_z \right)\).
</p>

<p>
This has the same disadvantage as the Euler angle representation,
where algorithms are not numerically well-conditioned. Hence, the
preferred way is to use <a href="#org56c7059">quarternions</a>. Rotations are represented with
unit quarternions.
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgea88b61" class="outline-5">
<h5 id="orgea88b61"><span class="section-number-5">4.4.5.2</span> Intrinsic Parameters</h5>
<div class="outline-text-5" id="text-4-4-5-2">
<p>
We have looked at perspective projection, and we obtain the ideal
coordinates:
</p>

\begin{align}
  u &= f \frac{X}{Z} \\
  v &= f \frac{Y}{Z}
\end{align}

<p>
However, pixels are arbitrary spatial units, so we introduce an alpha
to scale the value.
</p>

\begin{align}
  u &= \alpha \frac{X}{Z} \\
  v &= \alpha \frac{Y}{Z}
\end{align}

<p>
However, pixels may not necessarily be square, so we have to introduce
a different parameter for \(u\) and \(v\).
</p>

\begin{align}
  u &= \alpha \frac{X}{Z} \\
  v &= \beta \frac{Y}{Z}
\end{align}

<p>
We don't know the origin of our camera pixel coordinates, so we have
to add offsets:
</p>

\begin{align}
  u &= \alpha \frac{X}{Z} + u_0 \\
  v &= \beta \frac{Y}{Z} + v_0
\end{align}

<p>
We also assume here that \(u\) and \(v\) are perpendicular. To correct for
this, we need to introduce skew coefficients:
</p>


\begin{align}
  u &= \alpha \frac{X}{Z} - \alpha \cot \theta \frac{Y}{Z} + u_0 \\
  v &= \frac{\beta}{\sin \theta} \frac{Y}{Z} + v_0
\end{align}

<p>
We can simplify this by expressing it in homogeneous coordinates:
</p>



<div class="figure">
<p><img src="images/computer_vision/Image%20Formation/screenshot_2018-11-24_13-18-11.png" alt="screenshot_2018-11-24_13-18-11.png" />
</p>
</div>

<p>
The 3x4 matrix is the intrinsic matrix.
</p>

<p>
This can be represented in an easier way:
</p>


<div class="figure">
<p><img src="images/computer_vision/Image%20Formation/screenshot_2018-11-24_13-20-19.png" alt="screenshot_2018-11-24_13-20-19.png" />
</p>
</div>

<p>
And if we assume:
</p>

<ul class="org-ul">
<li>pixels are square</li>
<li>there is no skew</li>
<li>and the optical center is in the center, then \(K\) reduces to</li>
</ul>

\begin{equation}
K = \begin{bmatrix}
  f & 0 & 0 \\
  0 & f & 0 \\
  0 & 0 & 1
\end{bmatrix}
\end{equation}
</div>
</div>

<div id="outline-container-org082cbe3" class="outline-5">
<h5 id="org082cbe3"><span class="section-number-5">4.4.5.3</span> Combining Extrinsic and Intrinsic Calibration Parameters</h5>
<div class="outline-text-5" id="text-4-4-5-3">
<p>
We can write:
</p>

\begin{equation}
  p' = K \begin{bmatrix}
    ^C_WR & ^C_Wt 
  \end{bmatrix} ^Wp
\end{equation}
</div>
</div>
</div>
</div>
<div id="outline-container-orgb9a89cf" class="outline-3">
<h3 id="orgb9a89cf"><span class="section-number-3">4.5</span> Photometric image formation</h3>
<div class="outline-text-3" id="text-4-5">
<p>
Images are not composed of 2D features, but of discrete color or
intensity values. Where do these values come from, and how do they
relate to the lighting in the environment, surface properties and
geometry, camera optics and sensor properties?
</p>
</div>

<div id="outline-container-orgcd07261" class="outline-4">
<h4 id="orgcd07261"><span class="section-number-4">4.5.1</span> Lighting</h4>
<div class="outline-text-4" id="text-4-5-1">
<p>
To produce an image, a scene must be illuminated with one or more
light sources.
</p>

<p>
A point light source originates at a single location in space. In
addition to its location, a point light source has an intensity and a
color spectrum (a distribution over wavelengths).
</p>

<p>
An area light source with a diffuser can be modeled as a finite
rectangular area emitting light equally in all directions. When the
distribution is strongly directional, a four-dimensional lightfield
can be used instead.
</p>
</div>
</div>

<div id="outline-container-org61a7b1a" class="outline-4">
<h4 id="org61a7b1a"><span class="section-number-4">4.5.2</span> Reflectance and shading</h4>
<div class="outline-text-4" id="text-4-5-2">
<p>
When light hits an object surface, it is scattered and reflected. We
look at some more specialised models, including the diffuse, specular
and Phong shading models.
</p>
</div>

<div id="outline-container-orgb1c1607" class="outline-5">
<h5 id="orgb1c1607"><span class="section-number-5">4.5.2.1</span> The Bidirectional Reflectance Distribution Function (BRDF)</h5>
<div class="outline-text-5" id="text-4-5-2-1">
<p>
Relative to some local coordinate frame on the surface, the BRDF is a
four-dimensional function that describes how much of each wavelength
arriving at an incident direction \(\hat{\mathbf{v}}_i\) is emitted in a
reflected direction \(\hat{\mathbf{v}}_r\). The function can be written
in terms of the angles of the incident and reflected directions
relative to the surface frame as \(f_r(\theta_i, \phi_i, \theta_r,
\phi_r;\lambda)\).
</p>

<p>
BRDFs for a given surface can be obtained through physical modeling,
heuristic modeling or empirical observation. Typical BRDFs can be
split into their diffuse and specular components.
</p>
</div>
</div>

<div id="outline-container-org8551a0c" class="outline-5">
<h5 id="org8551a0c"><span class="section-number-5">4.5.2.2</span> Diffuse Reflection</h5>
<div class="outline-text-5" id="text-4-5-2-2">
<p>
The diffuse component scatters light uniformly in all directions and
is the phenomenon we most normally associate with shading. Diffuse
reflection also often imparts a strong body color to the light.
</p>

<p>
When light is scattered uniformly in all directions, the BRDF is
constant:
</p>

\begin{equation}
f_d(\hat{\mathbf{v}}_i, \mathbf{v}}_r, \mathbf{n}};\lambda) = f_d(\lambda)
\end{equation}

<p>
and the amount of light depends on the angle between the incident
light direction and the surface normal \(\theta_i\).
</p>
</div>
</div>

<div id="outline-container-org2942664" class="outline-5">
<h5 id="org2942664"><span class="section-number-5">4.5.2.3</span> Specular Reflection</h5>
<div class="outline-text-5" id="text-4-5-2-3">
<p>
The specular reflection component heavily depends on the direction of
the outgoing light. Incident light rays are reflected in a direction
that is rotated by 180^&deg; around the surface normal
\(\hat{\mathbf{n}}\).
</p>



<div class="figure">
<p><img src="images/computer_vision/Image Formation/screenshot_2018-08-21_11-16-05.png" alt="screenshot_2018-08-21_11-16-05.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgad88c5d" class="outline-5">
<h5 id="orgad88c5d"><span class="section-number-5">4.5.2.4</span> Phong Shading</h5>
<div class="outline-text-5" id="text-4-5-2-4">
<p>
Phong combined the diffuse and specular components of reflection with
another term, which he called the ambient illumination. This term
accounts for the fact that objects are generally illuminated not only
by point light sources but also by a general diffuse illumination
corresponding to inter-reflection or distance sources. In the Phong
model, the ambient term does not depend on surface orientation, but
depends on the color of both the ambient illumination \(L_a(\lambda)\)
and the object \(k_a(\lambda)\),
</p>

\begin{equation}
f_a(\lambda) = k_a(\lambda) L_a(\lambda)
\end{equation}

<p>
The Phong shading model can then be fully specified as:
</p>

\begin{equation}
L_r(\hat{\mathbf{v}}_r ; \lambda) = k_a(\lambda) L_a(\lambda)
+ k_d(\lambda) \sum_i L_i(\lambda) [\hat{\mathbf{v}}_i \cdot \hat{\mathbf{n}}]^+
+ k_s(\lambda) \sum_i L_i(\lambda) (\hat{\mathbf{v}}_r \cdot \hat{\mathbf{s}}_i)^{k_e}
\end{equation}

<p>
The Phong model has been superseded by other models in terms of
physical accuracy. These models include the di-chromatic reflection
model.
</p>
</div>
</div>

<div id="outline-container-org981ec01" class="outline-5">
<h5 id="org981ec01"><span class="section-number-5">4.5.2.5</span> Optics</h5>
<div class="outline-text-5" id="text-4-5-2-5">
<p>
Once the light from a scene reaches a camera, it must still pass
through the lens before reaching the sensor.
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org259ca6c" class="outline-2">
<h2 id="org259ca6c"><span class="section-number-2">5</span> Image Processing</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org3a94655" class="outline-3">
<h3 id="org3a94655"><span class="section-number-3">5.1</span> Point Operators</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Point operators are image processing transforms where each output
pixel's value depends only on the corresponding input pixel value.
Examples of such operators include:
</p>

<ul class="org-ul">
<li>brightness and contrast adjustments</li>
<li>color correction and transformations</li>
</ul>
</div>
</div>
<div id="outline-container-org944f247" class="outline-3">
<h3 id="org944f247"><span class="section-number-3">5.2</span> Image Enhancement</h3>
<div class="outline-text-3" id="text-5-2">
</div>
<div id="outline-container-org5883772" class="outline-4">
<h4 id="org5883772"><span class="section-number-4">5.2.1</span> Histogram Equalization</h4>
<div class="outline-text-4" id="text-5-2-1">
<p>
<a href="https://www.math.uci.edu/icamp/courses/math77c/demos/hist_eq.pdf">https://www.math.uci.edu/icamp/courses/math77c/demos/hist_eq.pdf</a>
</p>

<p>
The underlying math behind histogram equalization involves mapping one
distribution (the given histogram of intensity values) to another
distribution (a wider and, ideally, uniform distribution of intensity
values).
</p>


<div class="figure">
<p><img src="images/computer_vision/Image Processing/screenshot_2018-08-30_16-43-39.png" alt="screenshot_2018-08-30_16-43-39.png" />
</p>
</div>

<p>
We may use the cumulative distribution function to remap the original
distribution as an equally spread distribution simply by looking up
each y-value in the original distribution and seeing where it should
go in the equalized distribution.
</p>

\begin{equation}
  g_{i,j} = \left\lfloor \left( L - 1 \right) \sum_{n = 0}^{f_{i,j}}
  p_n  \right\rfloor
\end{equation}
</div>
</div>

<div id="outline-container-org1fad2d4" class="outline-4">
<h4 id="org1fad2d4"><span class="section-number-4">5.2.2</span> Convolutions</h4>
<div class="outline-text-4" id="text-5-2-2">
<p>
Convolution is the process of adding each element of the image to its
local neighbors, weighted by the kernel.
</p>

<p>
Convolutions can be used to denoise, descratch, blur, unblur and even
feature extraction. 
</p>

<p>
Median filtering is good for removing salt-and-pepper noise, or
scratches in image
</p>



<div class="figure">
<p><img src="images/computer_vision/Image%20Processing/screenshot_2018-11-23_10-05-43.png" alt="screenshot_2018-11-23_10-05-43.png" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orge109e63" class="outline-3">
<h3 id="orge109e63"><span class="section-number-3">5.3</span> Colour</h3>
<div class="outline-text-3" id="text-5-3">
<p>
A human retina has 2 kinds of light receptors: rods are sensitive to
amount of light, while cones are sensitive to wavelengths of light
</p>

<p>
There are 3 kinds of cones:
</p>

<dl class="org-dl">
<dt>short</dt><dd>most sensitive to blue</dd>
<dt>medium</dt><dd>most sensitive to green</dd>
<dt>long</dt><dd>most sensitive to red</dd>
</dl>

<p>
Cones send signals to the brain, and the brain interprets this mixture
of signals as colours. This gives rise to the RGB colour coding
scheme. Different coding schemes have different colour spaces.
</p>

<p>
Cones are sensitive to vaious colours, ranging from wavelengths of
400nm (violet) to 700nm (red).
</p>

<p>
TODO: insert picture of visible spectrum
</p>

<p>
There are some regions that extend beyond the visible region, but are
still relevant to image processing:
</p>

<ul class="org-ul">
<li>0.7-1.0\(\mu m\): Near infrared (NIR)</li>
<li>1.0-3.0\(\mu m\): Short-wave infrared (SWIR)</li>
<li>3.0-5.0\(\mu m\): Mid-wave infrared (MWIR)</li>
<li>8.0-12.0\(\mu m\): Long-wave infrared (LWIR)</li>
<li>12.0-1000.0\(\mu m\): Far infrared or very long-wave infraerd (VLWIR)</li>
</ul>

<p>
The range 5-8\(\mu m\) corresponds to a wavelength spectrum that is
largely absorbed by the water in the atmosphere.
</p>

<p>
<b>Color constancy</b> is the ability of the human visual system to be immune
to changing illumination in perception of colour. The human colour
receptors perceive the overall effect of the mixture of colours, and
cannot tell its composition.
</p>

<p>
<b>Gamut</b> is the range of colours that can be reproduced with a given
colour reproduction system.
</p>

<p>
TODO: image of colour systems
</p>

<p>
In the RGB colour space, each value is an unsigned 8-bit value from
0-255.
</p>

<p>
In the HSV (Hue Saturation Value) colour space, hue corresponds to
colour type from 0 (red) to 360. Saturation corresponds to the
colourfulness (0 - 1 full colour), while value refers to the
brightness (0 black - 1 white).
</p>

<p>
The YCbCRr Colour space is used for TV and video. Y stands for
luminance, Cb blue difference, and Cr red difference.
</p>

<p>
There are several colour conversion algorithms to convert values in
one colour space to another.
</p>

<p>
TODO: Insert equations
</p>

<p>
Primary colours are the set of colours combined to make a range of
colours. Since human vision is trichromatic, we only need to use 3
primary colours. The combination of primary colours can be additive or
subtractive.
</p>

<p>
Examples of additive combinations include overlapping projected lights
and CRT displays. RGB is commonly used in additive combinatinos.
Examples of subtracting combinations include mixing of color pigments
or dyes. The primary colours used in these cases are normally cyan,
magenta and yellow.
</p>

<p>
TODO: Images of additive/subtractive colour mixing.
</p>
</div>

<div id="outline-container-org9fc08e0" class="outline-4">
<h4 id="org9fc08e0"><span class="section-number-4">5.3.1</span> Measuring Colour Differences</h4>
<div class="outline-text-4" id="text-5-3-1">
<p>
The simplest metric is the euclidean distance between colours in the
RGB space:
</p>

\begin{equation}
  d(C_1, C_2) = \sqrt{\left( R_1 - R_2 \right)^2 + \left( G_1 - G_2
    \right)^2 + \left( B_1 - B_2 \right)^2}
\end{equation}

<p>
However, the RGB space is not perceptually uniform, and this is
inappropriate if one needs to match human perception. HSV, YCbCr are
also not perceptually uniform. Some colour spaces that are more
perceptually uniform are the Munsell, CIELAB and CIELUB colour spaces.
</p>
</div>
</div>

<div id="outline-container-org3e0bc21" class="outline-4">
<h4 id="org3e0bc21"><span class="section-number-4">5.3.2</span> Computing Means</h4>
<div class="outline-text-4" id="text-5-3-2">
<p>
The usual formula of computing means \(M = \frac{1}{n}S =
\frac{1}{n}\sum_{i=1}^n R_i\) can lead to overflow even for small \(n\).
One way to get around it is to use a floating point representation for
\(S\). The second method is to do incremental averaging:
</p>

\begin{equation}
  M_k = \frac{k-1}{k}M_{k-1} + \frac{1}{k}R_k
\end{equation}
</div>
</div>

<div id="outline-container-org6ace06e" class="outline-4">
<h4 id="org6ace06e"><span class="section-number-4">5.3.3</span> Digital Cameras sensing colour</h4>
<div class="outline-text-4" id="text-5-3-3">
<p>
TODO: Bayer filter
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb46cca5" class="outline-2">
<h2 id="orgb46cca5"><span class="section-number-2">6</span> Change Detection</h2>
<div class="outline-text-2" id="text-6">
<p>
To detect change between 2 video frames, it is straightforward to
compute differences in pixel intensities across the two frames:
</p>

\begin{equation}
  D_t(x, y) = | I(x,y,t+1)  - I(x,y,t)|
\end{equation}

<p>
It is common to use a threshold for \(D_t(x,y)\) to declare if a pixel
has changed.
</p>

<p>
To detect positional changes, the method used must be immune to
illumination change. This requires motion tracking.
</p>

<p>
At the same time, to detect illumination change, the method must be
immune to positional change. In the case of a stationary scene and
camera, the straightforward method can be used. However, in the non
trivial case, motion tracking will be required.
</p>
</div>
</div>

<div id="outline-container-org3944902" class="outline-2">
<h2 id="org3944902"><span class="section-number-2">7</span> Motion Tracking</h2>
<div class="outline-text-2" id="text-7">
<p>
There are two approaches to motion tracking: feature-based and
intensity-gradient based.
</p>
</div>

<div id="outline-container-org0b4a308" class="outline-3">
<h3 id="org0b4a308"><span class="section-number-3">7.1</span> Feature-based</h3>
<div class="outline-text-3" id="text-7-1">
<p>
Feature-based motion tracking utilises distinct features that changes
positions. For each feature, we search for the matching feature in the
next frame, to check if there is a displacement.
</p>

<p>
Good features are called "corners". The two popular corner detectors
are the Harris corner detector and the Tomasi corner detector.
</p>

<p>
Although corners are only a small percentage of the image, they
contain the most important features in restoring image information,
and they can be used to minimize the amount of processed data for
motion tracking, image stitching, building 2D mosaics, stereo vision,
image representation and other related computer vision areas.
</p>
</div>

<div id="outline-container-orgbfc8e9c" class="outline-4">
<h4 id="orgbfc8e9c"><span class="section-number-4">7.1.1</span> <a href="https://en.wikipedia.org/wiki/Harris_Corner_Detector">Harris corner detector</a></h4>
<div class="outline-text-4" id="text-7-1-1">
<p>
Compared to the Kanade-Lucas-Tomasi corner detector, the Harris corner
detector provides good repeatability under changing illumination and
rotation, and therefore, it is more often used in stereo matching and
image database retrieval.
</p>

<p>
Interpreting the eigenvalues:
</p>


<div class="figure">
<p><img src="images/computer_vision/Motion%20Tracking/screenshot_2018-11-24_14-54-48.png" alt="screenshot_2018-11-24_14-54-48.png" />
</p>
</div>

<p>
In flat regions, the eigenvalues are both small, in edges, only one of
the eigenvalues are large. On the other hand, in corners, both
eigenvalues are large but the 2 eigenvalues of the same magnitude, the error
\(E\) increases in all directions.
</p>



<div class="figure">
<p><img src="images/computer_vision/Motion%20Tracking/screenshot_2018-11-24_14-56-43.png" alt="screenshot_2018-11-24_14-56-43.png" />
</p>
</div>

<p>
The Harris corner response function essentially filters out the corners.
</p>
</div>

<div id="outline-container-orga087e16" class="outline-5">
<h5 id="orga087e16"><span class="section-number-5">7.1.1.1</span> Properties</h5>
<div class="outline-text-5" id="text-7-1-1-1">
<ol class="org-ol">
<li>Harris corner detector is invariant to rotation: Ellipse has the
same eigenvalues regardless of rotation.</li>
<li>Mostly invariant to additive and multiplicative intensity changes
(threshold issue for multiplicative)</li>
<li>Not invariant to image scale!</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org656134f" class="outline-4">
<h4 id="org656134f"><span class="section-number-4">7.1.2</span> Tomasi corner detector</h4>
<div class="outline-text-4" id="text-7-1-2">
\begin{equation}
  \frac{1}{N} \sum_{u} \sum_v \begin{bmatrix}
    I_x^2 & I_x I_y \\
    I_x I_y & I_y^2 \\
  \end{bmatrix}
\end{equation}

<p>
where \(I_x = \frac{\partial I}{\partial x}\), \(N\) is the total number
of pixels in window of interest, \(u\) and \(v\) are the horizontal and
vertical index of the pixel in the window of interest.
</p>

<p>
Let the eigenvalues of the above matrix be \(\lambda_{max}\) and
\(\lambda_{min}\). Then the greater \(\lambda_{min}\), the more
"cornerness" the feature.
</p>

<p>
Examples of feature descriptors include SIFT and SURF. The typical
workflow involves:
</p>

<ol class="org-ol">
<li>Detecting good features</li>
<li>Building feature descriptors on each of these features</li>
<li>Matching these descriptors on the second image to establish the
corresponding points</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org79a1b6a" class="outline-3">
<h3 id="org79a1b6a"><span class="section-number-3">7.2</span> Gradient-based</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Gradient-based motion tracking makes 2 basic assumptions:
</p>

<ol class="org-ol">
<li>Intensity changes smoothly within image</li>
<li>Pixel intensities of a given object does not change over time</li>
</ol>

<p>
Suppose that an object is in motion. Then the position of the object
is given by \((dx, dy)\) over time \(dt\). From the brightness constancy
assumption,
</p>

\begin{equation}
I(x + dx, y + yd, t + dt) = I(x,y,t)
\end{equation}

<p>
If we apply the Taylor series expansion on the left hand side, we get:
</p>

\begin{equation}
  I(x + dx, y + dy, t + dt) = I(x,y,t) + \frac{\partial I}{\partial
    x}dx + \frac{\partial I}{\partial y} dy + \frac{\partial
    I}{\partial t}dt + \dots
\end{equation}

<p>
Omitting higher-order terms, we get
</p>

\begin{equation}
  \frac{\partial I}{\partial
    x}dx + \frac{\partial I}{\partial y} dy + \frac{\partial
    I}{\partial t}dt = 0
\end{equation}

<p>
We denote this as \(I_x u + I_y v + I_t = 0\), but this has 2 unknowns, and
is unsolvable.
</p>
</div>

<div id="outline-container-orgcb12734" class="outline-4">
<h4 id="orgcb12734"><span class="section-number-4">7.2.1</span> Lucas-Kanade method</h4>
<div class="outline-text-4" id="text-7-2-1">
<p>
Suppose an object moves by displacement \(\mathbb{d} = (dx, dy)^T\). Then
\(J(x+d) = I(x)\), or \(J(x) = I(x-d)\).
</p>

<p>
Due to noise, there is some error at position \(x\):
</p>

\begin{equation}
e(x) = I(x - d) - J(x)
\end{equation}

<p>
We sum the errors over some window \(W\) at position \(x\):
</p>

\begin{equation}
E(x) = \sum_{x \in W} w(x) \left[ I(x-d) - J(x) \right]^2
\end{equation}

<p>
If \(E\) is small, then the patterns in \(I\) and \(J\) match well. We find
the \(d\) that minimises \(E\). If we expand \(I(x-d)\) with Taylor
expansion:
</p>

\begin{equation}
I(x-dx, y-dy) = I(x,y) - dx I_x(x,y) - dy I_y (x,y) + \dots
\end{equation}

<p>
Then,
</p>

\begin{equation}
  J(x) = I(x - d) = I(x) - d^T g(x), g(x) = \begin{bmatrix}
    I_x(x) \\
    I_y(x)
  \end{bmatrix}
\end{equation}

<p>
Where g(x) is the intensity gradient. Substituting the above equation,
and setting \(\frac{\partial E}{\partial d} = 0\):
</p>

\begin{equation}
\frac{\partial E}{\partial d} = -2 \sum_{x \in W} w(x) \left[ I(x) -
  J(x) - d^T g(x) \right] g(x)
\end{equation}

\begin{equation}
\sum_{x \in W} w(x)\left[ I(x) - J(x) \right] g(x) = \sum_{x \in W}
w(x) g(x) g^T(x) d
\end{equation}

<p>
We denote this as:
</p>

\begin{equation}
  Z d = b
\end{equation}

<p>
where
</p>

\begin{equation}
Z = \begin{bmatrix}
  \sum_{x \in W} w I_x^2 & \sum_{x \in W} w I_x I_y \\
  \sum_{x \in W} wI_x I_y & \sum_{x \in W} w I_y^2
\end{bmatrix}, b = \begin{bmatrix}
  \sum_{x \in W} w(I-J)I_x \\
  \sum_{x \in W} w(I-J)I_y
\end{bmatrix}
\end{equation}

<p>
With 2 unknowns and 2 equations, we can solve for \(d\).
</p>

<p>
Lucas-Kanade algorithm is often used with Harris/Tomasi's corner
detectors. First, corner detectors are applied to detect good
features, then LK method is applied to compute \(d\) for each pixel. \(d\)
is then accepted only for good features.
</p>

<p>
The math of LK tracker assumes \(d\) is small, and would only work for
small displacements. To handle large displacements, the image is
downsampled. Usually , the Gaussian filter is used to smoothen the
image before scaling down.
</p>



<div class="figure">
<p><img src="images/computer_vision/Motion%20Tracking/screenshot_2018-11-23_12-36-44.png" alt="screenshot_2018-11-23_12-36-44.png" />
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-orgf0baa5b" class="outline-2">
<h2 id="orgf0baa5b"><span class="section-number-2">8</span> Homography</h2>
<div class="outline-text-2" id="text-8">
<p>
<a href="https://docs.opencv.org/3.4.1/d9/dab/tutorial_homography.html">https://docs.opencv.org/3.4.1/d9/dab/tutorial_homography.html</a>
</p>

<p>
The planar homography relates the transformation between 2 planes, up
to a scale factor:
</p>

\begin{equation}
s \begin{bmatrix}
  x' \\
  y' \\
  1
\end{bmatrix} =
H \begin{bmatrix}
  x \\
  y \\
  1
\end{bmatrix} =
\begin{bmatrix}
  h_{11} & h_{12} & h_{13} \\
  h_{21} & h_{22} & h_{23} \\
  h_{31} & h_{32} & h_{33} \\
\end{bmatrix}
\begin{bmatrix}
  x \\
  y \\
  1
\end{bmatrix}
\end{equation}

<p>
The homography is a \(3 \times 3\) matrix with 8 degrees of freedom, as it is
estimated up to a scale.
</p>


<div class="figure">
<p><img src="Homography/homography_perspective_correction_chessboard_matches_2018-10-19_11-08-50.jpg" alt="homography_perspective_correction_chessboard_matches_2018-10-19_11-08-50.jpg" />
</p>
</div>

<p>
Homographies are used in:
</p>

<ul class="org-ul">
<li>camera pose estimation</li>
<li>panorama stitching</li>
<li>perspective removal/correction</li>
</ul>

<p>
<a href="https://cseweb.ucsd.edu/classes/wi07/cse252a/homography_estimation/homography_estimation.pdf">https://cseweb.ucsd.edu/classes/wi07/cse252a/homography_estimation/homography_estimation.pdf</a>
</p>
</div>
</div>

<div id="outline-container-org97c3615" class="outline-2">
<h2 id="org97c3615"><span class="section-number-2">9</span> Structure For Motion</h2>
<div class="outline-text-2" id="text-9">
<p>
In general, a single image cannot provide 3D information. From a set
of images taken with varying camera positions, we can extract 3D
information of the scene. This requires us to match (associate)
features in one image with the same feature in another image.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-11-24 Sat 20:31</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
