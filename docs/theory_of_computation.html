<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-11-26 Mon 09:55 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Theory Of Computation</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Theory Of Computation</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgeb20bd4">1. Introduction</a>
<ul>
<li><a href="#orgee8bc4f">1.1. Formal Proofs</a></li>
<li><a href="#orgbcb8e13">1.2. Inductive Proofs</a></li>
<li><a href="#orge95f867">1.3. Structural Inductions</a></li>
</ul>
</li>
<li><a href="#orgc8bf09b">2. Automata Theory</a>
<ul>
<li><a href="#orgda3bcf1">2.1. Definitions</a></li>
</ul>
</li>
<li><a href="#orgebe0d9d">3. Finite Automata</a>
<ul>
<li><a href="#org9fbd40f">3.1. Deterministic Finite Automata</a>
<ul>
<li><a href="#orgb146678">3.1.1. Simpler Notations</a></li>
</ul>
</li>
<li><a href="#orge3bb3b1">3.2. Language of a DFA</a></li>
<li><a href="#org8af0f99">3.3. Extending Transition Function to Strings</a></li>
</ul>
</li>
<li><a href="#orgdac5906">4. Nondeterministic Finite Automata</a>
<ul>
<li><a href="#org5124294">4.1. Definition</a></li>
<li><a href="#orge5b0317">4.2. The Language of an NFA</a></li>
<li><a href="#orgc8bfa6d">4.3. The Equivalence of DFA and NFA</a></li>
<li><a href="#org10c4f9f">4.4. Finite Automata with Epsilon-Transitions</a></li>
<li><a href="#orgd8de73c">4.5. Epsilon-Closures</a></li>
<li><a href="#orgf138646">4.6. Eliminating &epsilon;-Transitions</a></li>
</ul>
</li>
<li><a href="#orgbb6b2d7">5. Regular Expressions</a>
<ul>
<li><a href="#orgae01541">5.1. Precedence of regular expression operators</a></li>
<li><a href="#org1046182">5.2. Equivalence of DFA and Regular Expressions</a>
<ul>
<li><a href="#orgeac88ba">5.2.1. From DFA to Regular Expression</a></li>
<li><a href="#org7485bba">5.2.2. Converting DFAs to regular expressions by eliminating states</a></li>
<li><a href="#org751bfe1">5.2.3. Converting regular expressions to automata</a></li>
<li><a href="#org7f8772c">5.2.4. Algebraic law for regular expressions</a></li>
<li><a href="#org61edc88">5.2.5. Discovering laws for regular expressions</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgfae69a5">6. Properties of Regular Languages</a>
<ul>
<li><a href="#orge34ea24">6.1. Pumping Lemma</a></li>
<li><a href="#org4374c7e">6.2. Closure of Regular Languages</a></li>
</ul>
</li>
<li><a href="#org9931f22">7. Context-free Grammars and Languages</a>
<ul>
<li><a href="#orga7bbf52">7.1. Derivations using a Grammar</a></li>
<li><a href="#org43e154e">7.2. Leftmost and Rightmost Derivations</a></li>
<li><a href="#orgc1ded42">7.3. The language of a Grammar</a></li>
<li><a href="#org75d54f8">7.4. Sentential Forms</a></li>
<li><a href="#org42077d7">7.5. Parse Trees</a>
<ul>
<li><a href="#org4a4d585">7.5.1. Construction</a></li>
<li><a href="#orgb2d6031">7.5.2. The yield</a></li>
<li><a href="#org96f3c2f">7.5.3. Inferences and derivations</a></li>
</ul>
</li>
<li><a href="#org97446bf">7.6. Linear Grammars</a></li>
<li><a href="#org8369881">7.7. <span class="todo TODO">TODO</span> Ambiguous Grammars</a></li>
<li><a href="#org9f1d4e9">7.8. Chomsky Normal Form</a></li>
</ul>
</li>
<li><a href="#org87b09d8">8. Pushdown Automata</a>
<ul>
<li><a href="#orga43b5a5">8.1. Definition</a></li>
<li><a href="#org51c335f">8.2. Formal Definition</a></li>
<li><a href="#org0f40fd5">8.3. Instantaneous Descriptions</a></li>
<li><a href="#org971c656">8.4. The Languages of PDAs</a>
<ul>
<li><a href="#org4fc3416">8.4.1. Acceptance by Final State</a></li>
<li><a href="#org9e62e93">8.4.2. Acceptance by Empty Stack</a></li>
<li><a href="#org4dd57ea">8.4.3. From Empty Stack to Final State</a></li>
<li><a href="#orge242561">8.4.4. From Final State to Empty Stack</a></li>
</ul>
</li>
<li><a href="#org825d252">8.5. Equivalence of CFG and PDA</a>
<ul>
<li><a href="#org4e0b6b4">8.5.1. Deterministic Pushdown Automata</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org3899e06">9. Properties of Context-Free Languages</a>
<ul>
<li><a href="#org5878edf">9.1. Simplification of CFG</a></li>
<li><a href="#org6c254d3">9.2. Pumping Lemma for CFLs</a></li>
<li><a href="#org8fe26e3">9.3. Closure Properties of CFLs</a></li>
<li><a href="#org35b6c33">9.4. Decision Properties of CFLs</a>
<ul>
<li><a href="#org4105b4e">9.4.1. Running Time of Conversion to CNF</a></li>
<li><a href="#orga98ff9a">9.4.2. Testing for emptiness of CFL</a></li>
<li><a href="#org03e04fc">9.4.3. Testing Membership in a CFL</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org57efde8">10. Turing Machines</a>
<ul>
<li><a href="#org374af1f">10.1. Formal Notation</a></li>
<li><a href="#org98de437">10.2. Instantaneous Descriptions</a></li>
<li><a href="#org8b5c47e">10.3. Transition Diagrams</a></li>
<li><a href="#orge10b63f">10.4. The Language of a TM</a></li>
<li><a href="#org103c81e">10.5. Turing machines and halting</a></li>
<li><a href="#orge395f87">10.6. Programming Techniques for Turing Machines</a>
<ul>
<li><a href="#org80f9595">10.6.1. Storage in the State</a></li>
<li><a href="#orge38e1bc">10.6.2. Multiple Tracks</a></li>
<li><a href="#orgfc13d10">10.6.3. Subroutines</a></li>
<li><a href="#orgb6607a7">10.6.4. Multitape Turing Machines</a></li>
<li><a href="#orgabafb71">10.6.5. Equivalence of one-tape and multitape TMs</a></li>
<li><a href="#org8b38d50">10.6.6. Non-deterministic Turing Machines</a></li>
</ul>
</li>
<li><a href="#org7f378bb">10.7. Restricted Turing Machines</a>
<ul>
<li><a href="#orgd25756d">10.7.1. Turing Machines with Semi-infinite Tapes</a></li>
</ul>
</li>
<li><a href="#orgbb1b63c">10.8. Multistack Machines</a></li>
<li><a href="#orgfa2d399">10.9. Counter Machines</a></li>
<li><a href="#org1fb59fb">10.10. Turing Machines and Computers</a></li>
<li><a href="#org3b5efc4">10.11. Runtime of Computers vs Turing Machines</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgeb20bd4" class="outline-2">
<h2 id="orgeb20bd4"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
Finite automata are a useful model for many kinds of important
software and hardware. Automatas are found in compilers, software for
designing digital circuits, and many more systems.
</p>

<p>
<i>Grammars</i> provide useful models when designing software that processes
data with a recursive structure. The compiler's parser deals with
recursively nested features. Regular Expressions also denote the
structure of data, especially in text strings.
</p>

<p>
The study of automata addresses the following questions:
</p>

<ol class="org-ol">
<li>What can a computer do?</li>
<li>What can a computer do efficiently?</li>
</ol>

<p>
Automata theory also provides a tool for making formal proofs, of both
the inductive and deductive type.
</p>
</div>

<div id="outline-container-orgee8bc4f" class="outline-3">
<h3 id="orgee8bc4f"><span class="section-number-3">1.1</span> Formal Proofs</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Here the more common proof requirements are here:
</p>

<ol class="org-ol">
<li>Proving set equivalence: we can approach this by rephrasing it into
an iff statement.
<ol class="org-ol">
<li>Prove that if <code>x</code> is in <code>E</code>, then <code>x</code>  is in <code>F</code>.</li>
<li>Prove that if <code>x</code> is in <code>F</code>, then <code>x</code> is in <code>E</code>.</li>
</ol></li>
</ol>
</div>
</div>

<div id="outline-container-orgbcb8e13" class="outline-3">
<h3 id="orgbcb8e13"><span class="section-number-3">1.2</span> Inductive Proofs</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Suppose we are given a statement \(S(n)\) to prove. The inductive
approach involves:
</p>
<ol class="org-ol">
<li>The <i>basis</i>: where we show \(S(i)\) for a particular \(i\).</li>
<li>The <i>inductive step</i>: where we show if \(S(k)\) (or \(S(i), S(i+1),
   \dots, S(k)\)) then \(S(k+1)\).</li>
</ol>
</div>
</div>

<div id="outline-container-orge95f867" class="outline-3">
<h3 id="orge95f867"><span class="section-number-3">1.3</span> Structural Inductions</h3>
<div class="outline-text-3" id="text-1-3">
<p>
We can sometimes prove statements by construction. This is often the
case with recursively defined structures, such as with trees and
expressions. This works because we the recursive definition is invoked
at each step, so we are guaranteed that at each step of the
construction, the construction \(X_i\) is valid.
</p>
</div>
</div>
</div>

<div id="outline-container-orgc8bf09b" class="outline-2">
<h2 id="orgc8bf09b"><span class="section-number-2">2</span> Automata Theory</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-orgda3bcf1" class="outline-3">
<h3 id="orgda3bcf1"><span class="section-number-3">2.1</span> Definitions</h3>
<div class="outline-text-3" id="text-2-1">
<dl class="org-dl">
<dt>alphabet</dt><dd>An <i>alphabet</i> is a finite, nonempty set of symbols. E.g.
\(\Sigma = \{0, 1\}\) represents the binary alphabet</dd>
<dt>string</dt><dd>A <i>string</i> is a finite sequence of symbols chosen from some
alphabet. For example, \(01101\) is a string from the binary
alphabet. The empty string is represented by &epsilon;, and
sometimes \(\Lambda\). The
<i>length</i> of a string is denoted as such: \(|001| = 3\)</dd>
<dt>Powers</dt><dd>\(\Sigma^k\) represents strings of length \(k\).</dd>
<dt>Concatenation</dt><dd>\(xy\) denotes the concatenation of strings \(x\) and
\(y\).</dd>
<dt>Problem</dt><dd>a <i>problem</i> is the question of deciding whether a given
string is a member of some particular language.</dd>
</dl>
</div>
</div>
</div>

<div id="outline-container-orgebe0d9d" class="outline-2">
<h2 id="orgebe0d9d"><span class="section-number-2">3</span> Finite Automata</h2>
<div class="outline-text-2" id="text-3">
<p>
An automata has:
</p>
<ul class="org-ul">
<li>a set of states</li>
<li>its "control" moves from state to state in response to external inputs</li>
</ul>

<p>
A finite automata is one where the automaton can only be in a single
state at once (it is deterministic). Non-determinism allows us to
program solutions to problems using a higher-level language.
</p>

<p>
Determinism refers to the fact that on each input there is one and
only one state to which the automaton can transition from its current
state. Deterministic Finite Automata is often abbrieviated with <i>DFA</i>.
</p>
</div>

<div id="outline-container-org9fbd40f" class="outline-3">
<h3 id="org9fbd40f"><span class="section-number-3">3.1</span> Deterministic Finite Automata</h3>
<div class="outline-text-3" id="text-3-1">
<p>
A <i>dfa</i> consists of:
</p>

<ol class="org-ol">
<li>A finite set of <i>states</i>, often denoted \(Q\).</li>
<li>A finite set of <i>input symbols</i>, often denoted \(\Sigma\).</li>
<li>A <i>transition function</i> that takes as arguments a state and an input
symbol and returns a state, often denoted \(\delta\). If \(q\) is a state,
and \(a\) is an input symbol, then \(\delta(q,a)\) is that state \(p\) such
that there is an arc labeled \(a\) from \(q\) to \(p\).</li>
<li>A <i>start state</i>, one of the states in \(Q\).</li>
<li>A set of <i>final</i> or <i>accepting</i> states \(F\). The set \(F\) is a subset of
\(Q\).</li>
</ol>

<p>
In proofs, we often talk about a DFA in "five-tuple" notation:
</p>

\begin{equation}
  A = \left(Q, \Sigma, \delta, q_0, F \right)
\end{equation}
</div>

<div id="outline-container-orgb146678" class="outline-4">
<h4 id="orgb146678"><span class="section-number-4">3.1.1</span> Simpler Notations</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
The two preferred notation for describing automata are:
</p>
<dl class="org-dl">
<dt>transition diagrams</dt><dd>a graph</dd>
</dl>


<div class="figure">
<p><img src="images/theory_of_computation/Finite Automata/subset-construction-nfa-from-transition-table_2018-08-14_12-47-06.jpg" alt="subset-construction-nfa-from-transition-table_2018-08-14_12-47-06.jpg" />
</p>
</div>

<dl class="org-dl">
<dt>transition table</dt><dd>a tubular listing of the \(\delta\) function, which by
implication tells us the states and the input alphabet.</dd>
</dl>


<div class="figure">
<p><img src="images/theory_of_computation/Finite Automata/jTETt_2018-08-14_12-49-00.png" alt="jTETt_2018-08-14_12-49-00.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orge3bb3b1" class="outline-3">
<h3 id="orge3bb3b1"><span class="section-number-3">3.2</span> Language of a DFA</h3>
<div class="outline-text-3" id="text-3-2">
<p>
We can define the <i>language</i> of a DFA \(A = \left(Q, \Sigma, q_0, F\right)\).
This language is denoted \(L(A)\), and is defined by:
</p>

\begin{equation}
L(A) = \{ w | \delta(q_0, w) \text{ is in } F\}
\end{equation}

<p>
The language of \(A\) is the set of strings \(w\) that take the start
state \(q_0\) to one of the accepting states.
</p>
</div>
</div>

<div id="outline-container-org8af0f99" class="outline-3">
<h3 id="org8af0f99"><span class="section-number-3">3.3</span> Extending Transition Function to Strings</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Basis:
</p>
\begin{equation}
\hat{\delta}\left(q, \epsilon\right) = q
\end{equation}
<p>
Induction:
</p>
\begin{equation}
\hat{\delta}\left(q, xa\right) = \delta \left(\hat{\delta}\left(q, x\right), a \right)
\end{equation}
</div>
</div>
</div>

<div id="outline-container-orgdac5906" class="outline-2">
<h2 id="orgdac5906"><span class="section-number-2">4</span> Nondeterministic Finite Automata</h2>
<div class="outline-text-2" id="text-4">
<p>
A NFA has can be in several states at once, and this ability is
expressed as an ability to "guess" something about its input. It can
be shown that NFAs accept exactly the regular languages, just as DFAs
do. We can always convert an NFA to a DFA, although the latter may
have exponentially more states than the NFA.
</p>
</div>

<div id="outline-container-org5124294" class="outline-3">
<h3 id="org5124294"><span class="section-number-3">4.1</span> Definition</h3>
<div class="outline-text-3" id="text-4-1">
<p>
An NFA has:
</p>

<ol class="org-ol">
<li>A finite set of states \(Q\).</li>
<li>A finite set of input symbols \(\Sigma\).</li>
<li>A starting state \(q_0 \in Q\),</li>
<li>A set of final states \(F \subset Q\).</li>
<li>A transition function that takes a state in \(Q\) and an input symbol
in \(\Sigma\) as arguments and returns a <b>subset</b> of \(Q\).</li>
</ol>
</div>
</div>

<div id="outline-container-orge5b0317" class="outline-3">
<h3 id="orge5b0317"><span class="section-number-3">4.2</span> The Language of an NFA</h3>
<div class="outline-text-3" id="text-4-2">
<p>
if \(A = (Q, \Sigma, \delta, q_0, F)\) is an NFA, then
</p>

\begin{equation}
L(A) = \{w | \hat{\delta}(q_0, w) \cap F \neq \emptyset\}
\end{equation}

<p>
That is, \(L(A)\) is the set of strings \(w\) in \(\Sigma^*\) such that
\(\hat{\delta}(q_0, w)\).
</p>
</div>
</div>

<div id="outline-container-orgc8bfa6d" class="outline-3">
<h3 id="orgc8bfa6d"><span class="section-number-3">4.3</span> The Equivalence of DFA and NFA</h3>
<div class="outline-text-3" id="text-4-3">

<div class="figure">
<p><img src="images/theory_of_computation/Nondeterministic Finite Automata/screenshot_2018-08-14_13-44-15.png" alt="screenshot_2018-08-14_13-44-15.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org10c4f9f" class="outline-3">
<h3 id="org10c4f9f"><span class="section-number-3">4.4</span> Finite Automata with Epsilon-Transitions</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Transitions on &epsilon;, the empty string, allow NFAs to make a transition
spontaneously. This is sometimes referred to as &epsilon;-NFAs, and are
closely related to regular expressions.
</p>


<div class="figure">
<p><img src="images/theory_of_computation/Nondeterministic Finite Automata/screenshot_2018-08-15_11-45-07.png" alt="screenshot_2018-08-15_11-45-07.png" />
</p>
</div>

<p>
Of particular interest is the transition from \(q_0\) to \(q_1\), where the
\(+\) and \(-\) sign is optional.
</p>
</div>
</div>

<div id="outline-container-orgd8de73c" class="outline-3">
<h3 id="orgd8de73c"><span class="section-number-3">4.5</span> Epsilon-Closures</h3>
<div class="outline-text-3" id="text-4-5">
<p>
We &epsilon;-close a state \(q\) by following all transitions out of \(q\) that
are labelled &epsilon;, eventually finding all states that can be reached
from \(q\) along any path whose arcs are all labelled &epsilon;.
</p>

<p>
&epsilon;-closure allows us to explain easily what the transitions of an
&epsilon;-NFA look like when given a sequence of (non-&epsilon;) inputs. Suppose
that \(E = (Q, \Sigma, \delta, q_0, F)\) is an &epsilon;-NFA. We first define \(\hat{\delta}\),
the extended transition function, to reflect what happens on a
sequence of inputs.
</p>

<p>
<b>BASIS</b>: \(\hat{\delta}(q, \epsilon) = ECLOSE(q)\). If the label of the path is &epsilon;,
then we can follow only &epsilon;-labeled arcs extending from state \(q\).
</p>

<p>
<b>INDUCTION</b>: Suppose \(w\) is of the form \(xa\), where \(a\) is the last
symbol of \(w\). Note that \(a\) is a member of \(\Sigma\); it cannot be &epsilon;.
Then:
</p>

\begin{align}
  \text{Let } & \hat{\delta}(q, x) = \{p_1, p_2, \dots, p_k\} \\
   & \bigcup\limits_{i=1}^k \delta(p_i, a) = \{r_1, r_2, \dots, r_m\} \\
  \text{Then } & \hat{\delta}(q,w) = \bigcup\limits_{j=1}^m ECLOSE(r_j)
\end{align}
</div>
</div>

<div id="outline-container-orgf138646" class="outline-3">
<h3 id="orgf138646"><span class="section-number-3">4.6</span> Eliminating &epsilon;-Transitions</h3>
<div class="outline-text-3" id="text-4-6">
<p>
Given any &epsilon;-NFA \(E\), we can find a DFA \(D\) that accepts the same
language as \(E\).
</p>

<p>
Let \(E = (Q_E, \Sigma, \delta_E, q_0, F_E)\), then the equivalent DFA \(D = (Q_D, \Sigma,
\delta_D, q_D, F_D)\) is defined as follows:
</p>

<ol class="org-ol">
<li>\(Q_D\) is the set of subsets of \(Q_E\). All accessible states of \(D\)
are &epsilon;-closed subsets of \(Q_E\), i.e. \(S \subseteq Q_K s.t. S =
   ECLOSE(S)\). Any &epsilon;-transition out of one of the states in \(S\)
leads to another state in \(S\).</li>
<li>\(q_D = ECLOSE(q_0)\), we get the start state of \(D\) by closing the set
consisting of only the start state of \(E\).</li>
<li>\(F_D\) is those set of states that contain at least one accepting
state of \(E\). \(F_D = \{S | S \text{ is in } Q_D \text{ and } S \cap F_E
   \neq \emptyset \}\)</li>
<li>\(\delta_D(S,a)\) is computed for all \(a\) in \(\Sigma\) and sets \(S\) in \(Q_D\) by:
<ol class="org-ol">
<li>Let \(S = \{p_1, p_2, \dots, p_k\}\)</li>
<li>Compute \(\bigcup\limits_{i=1}^{k}\delta_E(p_i, a) = \{r_1, r_2, \dots, r_m\}\)</li>
<li>Then \(\delta_D(S, a) = \bigcup\limits_{j=1}^{m}ECLOSE(r_j)\)</li>
</ol></li>
</ol>
</div>
</div>
</div>


<div id="outline-container-orgbb6b2d7" class="outline-2">
<h2 id="orgbb6b2d7"><span class="section-number-2">5</span> Regular Expressions</h2>
<div class="outline-text-2" id="text-5">
<p>
Regular expressions may be thought of as a "programming language", in
which many important applications like text search applications or
compiler components can be expressed in.
</p>

<p>
Regular expressions can define the exact same languages that various
forms of automata describe: the regular languages. Regular expressions
denote languages. We define 3 operations on languages that the
operators of regular expressions represent.
</p>

<ol class="org-ol">
<li>The <i>union</i> of two languages \(L \bigcup M\), is the set of strings
that are either in \(L\) or \(M\).</li>
<li>The <i>concatenation</i> of languages \(L\) and \(M\) is the set of strings
that can be formed by taking any string in \(L\) and concatenating it
with any string in \(M\).</li>
<li>The closure (or star, or <i>Kleene closure</i>) is denoted \(L^*\) and
represents the set of those strings that can be formed by taking
any number of strings from \(L\), possibly with repetitions, and
concatenating all of them.</li>
</ol>

<p>
We can describe regular expressions recursively. For each expression
\(E\), we denote the language it represents with \(L(E)\).
</p>

<p>
<b>BASIS</b>:
</p>

<ol class="org-ol">
<li>The constants \(\epsilon\) and \(\emptyset\) are regular expressions, denoting the
languages \(\{\epsilon\}\) and \(\emptyset\) respectively.</li>
<li>If \(a\) is a symbol, then \(\mathbb{a}\) is a regular expression. This
expression denotes the language \(\{a\}\).</li>
</ol>

<p>
<b>INDUCTION</b>:
</p>

<ol class="org-ol">
<li>\(L(E) + L(F) = L(E) \bigcup L(F)\)</li>
<li>\(L(EF) = L(E)L(F)\)</li>
<li>\(L(E^*) = (L(E))^*\)</li>
<li>\(L((E)) = L(E)\)</li>
</ol>
</div>

<div id="outline-container-orgae01541" class="outline-3">
<h3 id="orgae01541"><span class="section-number-3">5.1</span> Precedence of regular expression operators</h3>
<div class="outline-text-3" id="text-5-1">
<p>
The precedence in order of highest to lowest, is:
</p>
<ol class="org-ol">
<li>star</li>
<li>dot (note that this operation is associative)</li>
<li>union (\(\plus\) operator)</li>
</ol>
</div>
</div>

<div id="outline-container-org1046182" class="outline-3">
<h3 id="org1046182"><span class="section-number-3">5.2</span> Equivalence of DFA and Regular Expressions</h3>
<div class="outline-text-3" id="text-5-2">
<p>
We show this by showing that:
</p>
<ol class="org-ol">
<li>Every language defined by a DFA is also defined by a regular
expression.</li>
<li>Every language defined by a regular expression is also defined by a
$\epilon$-NFA, which we have already shown is equivalent to a DFA.</li>
</ol>


<div class="figure">
<p><img src="images/theory_of_computation/Regular Expressions/screenshot_2018-08-28_12-46-13.png" alt="screenshot_2018-08-28_12-46-13.png" />
</p>
</div>
</div>

<div id="outline-container-orgeac88ba" class="outline-4">
<h4 id="orgeac88ba"><span class="section-number-4">5.2.1</span> From DFA to Regular Expression</h4>
<div class="outline-text-4" id="text-5-2-1">
<p>
We can number the finite states in a DFA \(A\) with \(1, 2, \dots, n\).
</p>

<p>
Let \(R_{ij}^{(k)}\) be the name of a regular expression whose language is the
set of strings \(w\) such that \(w\) is the label of a path from state \(i\)
to state \(j\) in a DFA \(A\), and the path has no intermediate node whose
number is greater than \(k\).
To construct the expression \(R_{ij}^{(k)}\), we use the following inductive
definition, starting at \(k= 0\), and finally reaching \(k=n\).
</p>

<p>
BASIS: \(k=0\).
Since the states are numbered \(1\) or above, the restriction on paths
is that the paths have no intermediate states at all. There are only 2
kinds of paths that meet such a condition:
</p>

<ol class="org-ol">
<li>An arc from node (state) \(i\) to node \(j\).</li>
<li>A path of length \(0\) that consists only of some node \(i\).</li>
</ol>

<p>
If \(i \ne j\), then only case \(1\) is possible. We must examine DFA \(A\)
and find input symbols \(a\) such that there is a transition from state
\(i\) to state \(j\) on symbol \(a\).
</p>

<ol class="org-ol">
<li>If there is no such symbol \(a\), then \(R_{ij}^{(0)} = \emptyset\).</li>
<li>If there is exactly one such symbol \(a\), then \(R_{ij}^{(0)} = \mathbb{a}\)</li>
<li>If there are symbols \(a_1, a_2, \dots, a_k\) that label arcs from
state \(i\) to state \(j\), then \(R_{ij}^{(0)} = \mathbb{a_1} + \mathbb{a_2} +
   \dots + \mathbb{a_k}\)</li>
</ol>

<p>
In case (a), the expression becomes \(\epsilon\), in case (c), the expression
becomes \(\epsilon + \mathbb{a_1} + \mathbb{a_2} + \dots + \mathbb{a_k}\).
</p>

<p>
INDUCTION: Suppose there is a path from state \(i\) to state \(j\) that
goes through no state higher than \(k\). Then either:
</p>

<ol class="org-ol">
<li>The path does not go through state \(k\) at all. In this case, the
label of the path is \(R_{ij}^{(k-1)}\).</li>
<li>The path goes through state \(k\) at least once. We can break the
path into several pieces:</li>
</ol>



<div class="figure">
<p><img src="images/theory_of_computation/Regular Expressions/screenshot_2018-08-28_12-58-35.png" alt="screenshot_2018-08-28_12-58-35.png" />
</p>
</div>

<p>
Then the set of labels for all paths of this type is represented by
the regular expression \(R_{ik}^{(k-1)}(R_{kk}^{(k-1)})^*R_{kj}^{(k-1)}\). Then, we can
combine the expressions for the paths of the two above:
</p>

\begin{equation}
R_{ij}^{(k)} = R_{ij}^{(k-1)} + R_{ik}^{(k-1)}(R_{kk}^{(k-1)})^*R_{kj}^{(k-1)}
\end{equation}

<p>
We can compute \(R_{ij}^{(n)}\) for all \(i\) and \(j\), and the language of the
automaton is then the sum of all expressions \(R_{ij}^{(n)}\) such that state
\(j\) is an accepting state.
</p>
</div>
</div>

<div id="outline-container-org7485bba" class="outline-4">
<h4 id="org7485bba"><span class="section-number-4">5.2.2</span> Converting DFAs to regular expressions by eliminating states</h4>
<div class="outline-text-4" id="text-5-2-2">
<p>
The above method of conversation always works, but is expensive. \(n^3\)
expressions have to be constructed for an n-state automaton, but the
length of the expression can grow by a factor of 4 on the average,
with each of the \(n\) inductive steps, and the expressions themselves
could reach on the order of \(4^n\) symbols.
</p>

<p>
The approach introduced here avoids duplicating work at some points,
by eliminating states. If we eliminate a state \(s\), then all paths
that went through \(s\) no longer exist in the automaton. To preserve
the language, we must include on an arc that goes directly from \(q\) to
\(p\), the labels of paths that went from some state \(q\) to \(p\) through
\(s\). This label now includes strings, but we can use a regular
expression to represent all such strings.
</p>

<p>
Hence, we can construct a regular expression from a finite automaton
as follows:
</p>

<ol class="org-ol">
<li>For each accepting state \(q\), apply the reduction process to
produce an equivalent automaton with regular-expression labels on
the arcs. Eliminate all states except \(q\) and the start state \(q_0\).</li>
<li>If \(q \neq q_0\), then a two-state automaton remains, as depicted. The
regular expression for the automaton is \((R + SU^*T)^*SU^*\).</li>
</ol>



<div class="figure">
<p><img src="images/theory_of_computation/Regular Expressions/screenshot_2018-08-28_23-30-02.png" alt="screenshot_2018-08-28_23-30-02.png" />
</p>
</div>

<ol class="org-ol">
<li>If the start state is also an accepting state, then we must perform
a state-elimination from the original automaton that gets rid of
every state but the start state, leaving a one-state automaton,
which accepts \(R^*\).</li>
</ol>



<div class="figure">
<p><img src="images/theory_of_computation/Regular Expressions/screenshot_2018-08-28_23-30-48.png" alt="screenshot_2018-08-28_23-30-48.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org751bfe1" class="outline-4">
<h4 id="org751bfe1"><span class="section-number-4">5.2.3</span> Converting regular expressions to automata</h4>
<div class="outline-text-4" id="text-5-2-3">
<p>
We can show every language defined by a regular expression is also
defined by a finite automaton, and we do so by converting any regular
expression \(R\) to an $&epsilon;$-NFA \(E\) with:
</p>

<ol class="org-ol">
<li>Exactly one accepting state</li>
<li>No arcs into initial state</li>
<li>No arcs out of the accepting state</li>
</ol>

<p>
The proof is conducted by structural induction on R, following the
recursive definition of regular expressions.
</p>

<p>
The basis of the induction involves constructing automatons for
regular expressions (a) \(\epsilon\), (b) \(\emptyset\) and (c) \(\mathbb{a}\). They are displayed
below:
</p>


<div class="figure">
<p><img src="images/theory_of_computation/Regular Expressions/screenshot_2018-08-28_23-36-18.png" alt="screenshot_2018-08-28_23-36-18.png" />
</p>
</div>

<p>
The inductive step consists of 4 cases: (a) The expression is \(R + S\)
for some smaller expressions \(R\) and \(S\). (b) The expression is \(RS\)
for smaller expressions \(R\) and \(S\). (c) The expression is \(R*\) for
some smaller expression \(R\). (d) The expression is (R) for some
expression R. The automatons for (a), (b), and (c) are shown below:
</p>



<div class="figure">
<p><img src="images/theory_of_computation/Regular Expressions/screenshot_2018-08-28_23-39-41.png" alt="screenshot_2018-08-28_23-39-41.png" />
</p>
</div>

<p>
The automaton for \(R\) also serves as the automaton for \((R)\).
</p>
</div>
</div>

<div id="outline-container-org7f8772c" class="outline-4">
<h4 id="org7f8772c"><span class="section-number-4">5.2.4</span> Algebraic law for regular expressions</h4>
<div class="outline-text-4" id="text-5-2-4">
<dl class="org-dl">
<dt>commutativity</dt><dd>\(x + y = y + x\).</dd>
<dt>associativity</dt><dd>\((x \times y) \times z = x \times (y \times z)\).</dd>
<dt>distributive</dt><dd>\(x \times (y + z) = x \times y + x \times z\)</dd>
</dl>
<ul class="org-ul">
<li>\(L + M = M + L\)</li>
<li>\((L + M) + N = L + (M + N)\)</li>
<li>\((LM)N = L(MN)\)</li>
<li>\(\emptyset + L = L + \emptyset = L\). \(\emptyset\) is the identity for union.</li>
<li>\(\epsilon L = L \epsilon = L\). \(\epsilon\) is the identity for concatenation.</li>
<li>\(\emptyset L = L\emptyset = \emptyset\). \(\emptyset\) is the annihilator for concatenation.</li>
<li>\(L(M + N) = LM + LN\) (left distributive)</li>
<li>\((M + N)L  = ML + NL\) (right distributive)</li>
<li>\(L + L = L\) (idempotence law)</li>
<li>\((L^*)^* = L^*\).</li>
<li>\(\emptyset^* = \epsilon\)</li>
<li>\(\epsilon^* = \epsilon\)</li>
<li>\(L^{+} = LL^* = L^*L\).</li>
<li>\(L^* = L^{+} + \epsilon\)</li>
<li>\(L? = \epsilon + L\)</li>
</ul>
</div>
</div>

<div id="outline-container-org61edc88" class="outline-4">
<h4 id="org61edc88"><span class="section-number-4">5.2.5</span> Discovering laws for regular expressions</h4>
<div class="outline-text-4" id="text-5-2-5">
<p>
The truth of a law reduces to the question of the equality of two
languages. We show set equivalence: a string in one language must be
in another, and vice-versa.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgfae69a5" class="outline-2">
<h2 id="orgfae69a5"><span class="section-number-2">6</span> Properties of Regular Languages</h2>
<div class="outline-text-2" id="text-6">
<p>
Regular languages exhibit the "closure" property. These properties let
us build recognizers for languages that are constructed from other
languages by certain operations. Regular languages also exhibit
"decision properties", which allow us to make decisions about whether
two automata define the same language. This means that we can always
minimize an automata to have as few states as possible for a
particular language.
</p>
</div>

<div id="outline-container-orge34ea24" class="outline-3">
<h3 id="orge34ea24"><span class="section-number-3">6.1</span> Pumping Lemma</h3>
<div class="outline-text-3" id="text-6-1">
<p>
We have established that the class of languages known as regular
languages are accepted by DFAs, NFAs and by $&epsilon;$-NFAs.
</p>

<p>
However, not every language is a regular language. An easy way to see
this is that the number of languages is infinite, but DFAs have finite
number of states, and are finite.
</p>

<p>
The pumping lemma lets us show that certain languages are not regular.
</p>

<div class="theorem">
<p>
<a id="orgf255ff0"></a>
Let \(L\) be a regular language. Then there exists a constant \(n\) (which
depends on \(L\)) such that for every string \(w\) in \(L\) such that
\(| w |  \ge n\), we can break \(w\) into three strings \(w = xyz\)
such that:
</p>

<ol class="org-ol">
<li>\(y \ne \epsilon\)</li>
<li>\(| xy | \le n\)</li>
<li>For all \(k \ge 0\), the string \(xy^k z\) is also in \(L\)</li>
</ol>

</div>

<p>
That is, we can always find a non-empty string \(y\) not too far from
the beginning of \(w\) that can be "pumped". This means repeating \(y\)
any number of times, or deleting it, keeps the resulting string in the
language \(L\).
</p>
</div>
</div>

<div id="outline-container-org4374c7e" class="outline-3">
<h3 id="org4374c7e"><span class="section-number-3">6.2</span> Closure of Regular Languages</h3>
<div class="outline-text-3" id="text-6-2">
<p>
If certain languages are regular, then languages formed from them by
certain operations are also regular. These are referred to as the
closure properties of regular languages. Below is a summary:
</p>

<ol class="org-ol">
<li>Union of 2 regular languages</li>
<li>Intersection of 2 regular languages</li>
<li>Complement of 2 regular languages</li>
<li>Difference of 2 regular languages</li>
<li>Reversal of a regular language</li>
<li>Closure (star) of a regular language</li>
<li>The concatenation of regular languages</li>
<li>A homomorphism (substitution of strings for symbols) of a regular language</li>
<li>The inverse homomorphism of a regular language</li>
</ol>

<p>
The above are all regular.
</p>
</div>
</div>
</div>

<div id="outline-container-org9931f22" class="outline-2">
<h2 id="org9931f22"><span class="section-number-2">7</span> Context-free Grammars and Languages</h2>
<div class="outline-text-2" id="text-7">
<p>
Context-free languages are a larger class of languages, that have
context-free grammars. We show how these grammars are defined, and how
they define languages.
</p>

<p>
Context-free grammars are recursive definitions. For example, the
context-free grammar for palindromes can be defined as:
</p>

<ol class="org-ol">
<li>\(P \rightarrow \epsilon\)</li>
<li>\(P \rightarrow 0\)</li>
<li>\(P \rightarrow 1\)</li>
<li>\(P \rightarrow 0P0\)</li>
<li>\(P \rightarrow 1P1\)</li>
</ol>

<p>
There are four important components in a grammatical description of a
language:
</p>

<ol class="org-ol">
<li>There is a finite set of symbols that form the strings of the
language. These alphabets are called the <i>terminals</i>, or <i>terminal
symbols</i>.</li>
<li>There is a finite set of variables, or <i>nonterminals</i> or <i>syntactic
categories</i>. Each variable represents a language. In the language
above, the only variable is \(P\).</li>
<li>One of the variables represents the language being defined, called
the <i>start symbol</i>.</li>
<li>There is a finite set of productions or rules that represent the
recursive definition of a language. Each production rule consists:
<ol class="org-ol">
<li>A variable that is being defined by the production (called the <i>head</i>).</li>
<li>The production symbol \(\rightarrow\).</li>
<li>A string of zero or more terminals and variables.</li>
</ol></li>
</ol>

<p>
We can represent any CFG as these 4 components, we denote CFG \(G = (V,
T, P, S)\).
</p>
</div>

<div id="outline-container-orga7bbf52" class="outline-3">
<h3 id="orga7bbf52"><span class="section-number-3">7.1</span> Derivations using a Grammar</h3>
<div class="outline-text-3" id="text-7-1">
<p>
We can apply the productions of a CFG to infer that certain strings
are in the language of a certain variable.
</p>

<p>
The process of deriving strings by applying productions from head to
body requires the definition of a new relation symbol, \(\Rightarrow\). Suppose \(G
= (V, T, P, S)\) is a CFG&gt; Let \(\alpha A \beta\) be a string of terminals and
variables, with \(A\) a variable. That is \(\alpha\) and \(\beta\) are strings in \((V
\cup T)^*\), and \(A\) is in \(V\). Let \(A \rightarrow \gamma\) be a production of \(G\). We say
that \(\alpha A \beta \Rightarrow_{G} \alpha \gamma B\). If \(G\) is understood, we can omit the
subscript.
</p>

<p>
We may extend the \(\Rightarrow\) relationship to represent zero, one or many
derivation steps, similar to the extended transition function.
</p>
</div>
</div>

<div id="outline-container-org43e154e" class="outline-3">
<h3 id="org43e154e"><span class="section-number-3">7.2</span> Leftmost and Rightmost Derivations</h3>
<div class="outline-text-3" id="text-7-2">
<p>
In order to restrict the number of choices we have in deriving a
string, it is often useful to require that at each step, we replace
the leftmost variable by one of its production bodies. Such a
derivation is called a <i>leftmost derivation</i>. Leftmost derivations are
indicated with \(\Rightarrow_{lm}\) and \(\Rightarrow_{lm}^*\).
</p>

<p>
Similarly, it is possible to require that at each step the rightmost
variable is replaced by one of its bodies. These are called <i>rightmost
derivations</i>. These are similarly denoted \(\Rightarrow_{rm}\) and \(\Rightarrow_{rm}^*\).
</p>
</div>
</div>

<div id="outline-container-orgc1ded42" class="outline-3">
<h3 id="orgc1ded42"><span class="section-number-3">7.3</span> The language of a Grammar</h3>
<div class="outline-text-3" id="text-7-3">
<p>
If \(G = (V, T, P, S)\) is a CFG, then the language of \(G\), denoted
\(L(G)\) is the set of terminal strings that have derivations from the
start symbol:
</p>

\begin{equation}
  L(G) = \right\{w in T^* | S \Rightarrow_{G}^* w \left\}
\end{equation}
</div>
</div>

<div id="outline-container-org75d54f8" class="outline-3">
<h3 id="org75d54f8"><span class="section-number-3">7.4</span> Sentential Forms</h3>
<div class="outline-text-3" id="text-7-4">
<p>
Derivations from the start symbol produce strings that have a special
role. These are called <i>sentential forms</i>. We also denote
left-sentential and right-sentential forms for leftmost derivations
and rightmost derivations respectively.
</p>
</div>
</div>

<div id="outline-container-org42077d7" class="outline-3">
<h3 id="org42077d7"><span class="section-number-3">7.5</span> Parse Trees</h3>
<div class="outline-text-3" id="text-7-5">
<p>
There is a tree representation of derivations, that clearly show how
symbols of a terminal string are grouped into substrings, each of
which belongs to the language of one of the variables of the grammar.
This tree is the data structure of choice when representing the source
of a program.
</p>
</div>

<div id="outline-container-org4a4d585" class="outline-4">
<h4 id="org4a4d585"><span class="section-number-4">7.5.1</span> Construction</h4>
<div class="outline-text-4" id="text-7-5-1">
<p>
The parse trees for a CFG \(G\) are trees with the following conditions:
</p>

<ol class="org-ol">
<li>Each interior node is labeled by variable in \(V\).</li>
<li>Each leaf is labeled by either a variable, a terminal, or \(\epsilon\).
However, if the leaf is labeled \(\epsilon\), then it must be the only child
of its parent.</li>
<li>If an interior node is labeled \(A\), and its children are labeled
\(X_1, X_2, \dots, X_k\), respectively from the left, then \(A \rightarrow X_1 X_2 \dots
   X_k\) is a production in \(P\).</li>
</ol>
</div>
</div>

<div id="outline-container-orgb2d6031" class="outline-4">
<h4 id="orgb2d6031"><span class="section-number-4">7.5.2</span> The yield</h4>
<div class="outline-text-4" id="text-7-5-2">
<p>
The yield of the tree is the concatenation of the leaves of the parse
tree from the left. This yield is a terminal string (all leaves are
labeled either with a terminal or with \(\epsilon\)). The root is labeled by
the start symbol.
</p>
</div>
</div>

<div id="outline-container-org96f3c2f" class="outline-4">
<h4 id="org96f3c2f"><span class="section-number-4">7.5.3</span> Inferences and derivations</h4>
<div class="outline-text-4" id="text-7-5-3">
<p>
The following statements are equivalent:
</p>

<ol class="org-ol">
<li>The recursive inference procedure determines that terminal string
\(w\) is in the language of variable \(A\).</li>
<li>\(A \Rightarrow^* w\)</li>
<li>\(A \Rightarrow_{lm}^* w\)</li>
<li>\(A \Rightarrow_{rm}^* w\)</li>
<li>There is a parse tree with root \(A\) and yield \(w\).</li>
</ol>

<p>
We can prove these equivalences using the following arcs:
</p>


<div class="figure">
<p><img src="images/theory_of_computation/Context-free Grammars and Languages/screenshot_2018-09-16_16-21-01.png" alt="screenshot_2018-09-16_16-21-01.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org97446bf" class="outline-3">
<h3 id="org97446bf"><span class="section-number-3">7.6</span> Linear Grammars</h3>
<div class="outline-text-3" id="text-7-6">
<p>
Right linear grammars have all the productions of form:
</p>

<ol class="org-ol">
<li>\(A \rightarrow wB\) for \(B \in V\) and \(w \in T^*\)</li>
<li>\(A \rightarrow  w\), for \(w \in T^*\)</li>
</ol>

<p>
Every regular language can be generated by some right-linear grammar.
Suppose \(L\) is accepted by DFA \(A = (Q, \Sigma, \delta, q_0, F)\), Then, let \(G =
(Q, \Sigma, P, q_0)\) where,
</p>

<ol class="org-ol">
<li>For \(q, p \in Q\), \(a \in \Sigma\), if \(\delta(q, a) = p\), then we have a
production in \(P\) of the form \(q \rightarrow ap\).</li>
<li>We also have productions \(q \rightarrow \epsilon\) for each \(q \in F\).</li>
</ol>

<p>
We can prove by induction on \(|w|\) that \(\hat{\delta}(q_0, w) = p\)  iff \(q_0
\Rightarrow^* wp\). This would give \(\hat{\delta}(q_0, w) \in F\)  iff \(q_0 \Rightarrow^* w\).
</p>
</div>
</div>

<div id="outline-container-org8369881" class="outline-3">
<h3 id="org8369881"><span class="section-number-3">7.7</span> <span class="todo TODO">TODO</span> Ambiguous Grammars</h3>
</div>

<div id="outline-container-org9f1d4e9" class="outline-3">
<h3 id="org9f1d4e9"><span class="section-number-3">7.8</span> Chomsky Normal Form</h3>
<div class="outline-text-3" id="text-7-8">
<p>
Chomsky normal form is useful in giving algorithms for working with
context-free grammars. A context-free grammar is in Chomsky normal
form if every rule is of the form:
</p>

<ol class="org-ol">
<li>\(A \rightarrow BC\)</li>
<li>\(A \rightarrow a\)</li>
</ol>

<p>
weher \(a\) is any terminal and \(A\), \(B\), \(C\) are any variables, except
\(B\) and \(C\) cannot be the start variable. \(S \rightarrow \epsilon\) is also allowed.
</p>

<p>
Any context-free language is generated by a context-free grammar in
Chomsky normal form. This is because we can convert any grammar into
Chomsky normal form.
</p>
</div>
</div>
</div>

<div id="outline-container-org87b09d8" class="outline-2">
<h2 id="org87b09d8"><span class="section-number-2">8</span> Pushdown Automata</h2>
<div class="outline-text-2" id="text-8">
<p>
Pushdown automata are equivalent in power to context-free grammars
This equivalence is useful because it gives us two options for proving
that a language is context-free. Certain languages are more easily
described in terms of recognizers, while others aremore easily
described in terms of generators.
</p>
</div>

<div id="outline-container-orga43b5a5" class="outline-3">
<h3 id="orga43b5a5"><span class="section-number-3">8.1</span> Definition</h3>
<div class="outline-text-3" id="text-8-1">
<p>
It is in essence a nondeterministic finite automaton with
&epsilon;-transitions permitted, with one additional capability: a stack on
which it can store a string of "stack symbols".
</p>

<p>
We can view the pushdown automaton informally as the device suggested
below:
</p>


<div class="figure">
<p><img src="images/Pushdown Automata/screenshot_2018-09-18_12-36-26.png" alt="screenshot_2018-09-18_12-36-26.png" />
</p>
</div>

<p>
A "finite-state-control" reads inputs, one symbol at a time. The
automaton is allowed to observe the symbol at the top of the stack,
and to base its transition on its current state. It :
</p>

<ol class="org-ol">
<li>Consumes from the input the symbol it uses in the transition. If &epsilon;
is used for the input, then no input symbol is consumed.</li>
<li>Goes to a new state</li>
<li>Replaces the symbol at the top of the stack by any string. This
corresponds to &epsilon;, which corresponds to a pop of the stack. It could
also replace the top symbol by one other symbol. Finally the top
stack symbol could be replaced by 2 or more symbols, which has the
effect of changing the top stack symbol, and pushing one or more
new symbols onto the stack.</li>
</ol>
</div>
</div>

<div id="outline-container-org51c335f" class="outline-3">
<h3 id="org51c335f"><span class="section-number-3">8.2</span> Formal Definition</h3>
<div class="outline-text-3" id="text-8-2">
<p>
We can specify a PDA \(P\) as follows:
</p>

\begin{equation}
  P = (Q,\Sigma, \Gamma, \delta, q_0, Z_0, F)
\end{equation}

<ul class="org-ul">
<li>\(Q\) is the finite set of states</li>
<li>\(\Sigma\) is the finite set of input symbols</li>
<li>\(\Gamma\) is the finite stack alphabet</li>
<li>\(\delta\) is the transition function, taking a triple $&delta;(q,a,X), where:
<ul class="org-ul">
<li>\(q\) is a state in \(Q\)</li>
<li>\(a\) is either an input symbol in \(\Sigma\) or \(\epsilon\).</li>
<li>\(X\) is a stack symbol, that is a member of \(\Gamma\)</li>
</ul></li>
<li>\(q_0\) the start state</li>
<li>\(Z_0\) the start symbol. Initially, the PDA's stack consists of this
symbol, nothing else</li>
<li>\(F\) is the set of accepting states</li>
</ul>

<p>
The formal definition of a PDA contains no explicit mechanism to allow
the PDA to test for an empty stack. The PDA is able to get the same
effect by initially placing a special symbol $ on the stack. If it
sees the $ again, it knows that the stack effectively is empty.
</p>

<p>
We can also draw transition diagrams for PDAs. An example is shown
below.
</p>


<div class="figure">
<p><img src="Pushdown Automata/screenshot_2018-10-11_20-52-24.png" alt="screenshot_2018-10-11_20-52-24.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org0f40fd5" class="outline-3">
<h3 id="org0f40fd5"><span class="section-number-3">8.3</span> Instantaneous Descriptions</h3>
<div class="outline-text-3" id="text-8-3">
<p>
We represent the configuration of a PDA by a triple \((q, w, \gamma)\),
where:
</p>

<ol class="org-ol">
<li>\(q\) is the state.</li>
<li>\(w\) is the remaining input</li>
<li>\(\gamma\) is the stack contents</li>
</ol>

<p>
Conventionally, we show the top of the stack at the left, and the
bottom at the right. This triple is called the <i>instantaneous
description</i>, or ID, of the pushdown automaton.
</p>

<p>
For finite automata, the \(\hat{\gamma}\) notation was sufficient to
represent sequences of instantaneous descriptions through which a
finite automaton moved. However, for PDAs we need a notation that
describes changes in the state, input and stack. Hence, we define
\(\vdash\) as follows. Supposed \(\delta(q, a, X)\) contains \((p, \alpha)\). Then for
all stings \(w\) in \(\Sigma^*\) and \(\beta\) in \(\Gamma^*\):
</p>

\begin{equation}
  (q, aw, X\beta) \vdash (p, w, \alpha\beta)
\end{equation}

<p>
We use \(\vdash^*\) to represent zero or more moves of the PDA.
</p>
</div>
</div>

<div id="outline-container-org971c656" class="outline-3">
<h3 id="org971c656"><span class="section-number-3">8.4</span> The Languages of PDAs</h3>
<div class="outline-text-3" id="text-8-4">
<p>
The class of languages for PDAs that accept by final state and accept
by empty stack are the same. We can show how to convert between the
two.
</p>
</div>

<div id="outline-container-org4fc3416" class="outline-4">
<h4 id="org4fc3416"><span class="section-number-4">8.4.1</span> Acceptance by Final State</h4>
<div class="outline-text-4" id="text-8-4-1">
<p>
Let \(P = (Q, \Sigma, \Gamma, \delta, q_0, Z_0, F)\) be a PDA. Then \(L(P)\), the language
accepted by P by final state, is:
</p>

\begin{equation}
\{w | (q_0, w, Z_0) \vdash^* (q, \epsilon, \alpha) \}
\end{equation}

<p>
for some state \(q\) in \(F\) and any stack string \(\alpha\).
</p>
</div>
</div>

<div id="outline-container-org9e62e93" class="outline-4">
<h4 id="org9e62e93"><span class="section-number-4">8.4.2</span> Acceptance by Empty Stack</h4>
<div class="outline-text-4" id="text-8-4-2">
<p>
Let \(P = (Q, \Sigma, \Gamma, \delta, q_0, Z_0, F)\) be a PDA. We define \(N(P) = \{w |
(q_0, w, Z_0) \vdash^* (q, \epsilon, \epsilon)\}\). That is \(N(P)\) is the set of inputs
\(w\) that \(P\) can consume and at the same time empty its stack.
</p>
</div>
</div>

<div id="outline-container-org4dd57ea" class="outline-4">
<h4 id="org4dd57ea"><span class="section-number-4">8.4.3</span> From Empty Stack to Final State</h4>
<div class="outline-text-4" id="text-8-4-3">
<p>
<b>Theorem:</b> 
</p>

<p>
If \(L = N(P_N)\) for some PDA \(P_N\), then there is a PDA \(P_F\)
such that \(L = L(P_F)\).
</p>

<p>
<b>Proof</b>:
</p>

<p>
We use a new symbol \(X_0\), not a symbol of \(\Gamma\); \(X_0\) is both the start
symbol of \(P_F\) and a marker on the bottom of the stack that lets us
know when \(P_N\) has reached an empty stack, then it knows that \(P_N\)
would empty its stack on the same input.
</p>

<p>
We also use a new start state \(p_0\), whose sole function is to push
\(Z_0\), the start symbol of \(P_N\), onto the top of the stack and enter
\(q_0\), the start state of \(P_N\). \(P_F\) simulates \(P_N\), until the stack of
\(P_N\) is empty, which \(P_F\) detects because it sees \(X_0\) on the top of
the stack.
</p>



<div class="figure">
<p><img src="Pushdown Automata/screenshot_2018-10-11_21-06-26.png" alt="screenshot_2018-10-11_21-06-26.png" />
</p>
</div>

<p>
Hence, we can specify \(P_F = (Q \cup \{p_0, p_f\}, \Sigma, \Gamma \cup \{X_0\}, \delta_F, p_0,
X_0, \{p_f\}\):
</p>

<ol class="org-ol">
<li>\(\delta_F(p_0, \epsilon, x_0) = \{(q_0, Z_0, X_0\}\).</li>
<li>For all states \(q\) in \(Q\), inputs \(a\) in \(\Sigma\) or \(a = \epsilon\), and the
stack symbols \(Y\) in \(\Gamma\), \(\delta_F(q,a,Y)\) contains all the pairs in
\(\delta_N(qa,Y)\).</li>
<li>\(\delta_F(q, \epsilon, X_0\) contains \((p_f, \epsilon)\) iff w is in \(N(P_N)\).</li>
</ol>
</div>
</div>

<div id="outline-container-orge242561" class="outline-4">
<h4 id="orge242561"><span class="section-number-4">8.4.4</span> From Final State to Empty Stack</h4>
<div class="outline-text-4" id="text-8-4-4">
<p>
Whenever \(P_F\) enters an accepting state after consuming input \(w\),
\(P_N\) will empty its stack after consuming \(w\).
</p>


<div class="figure">
<p><img src="Pushdown Automata/screenshot_2018-10-11_21-10-12.png" alt="screenshot_2018-10-11_21-10-12.png" />
</p>
</div>

<p>
To avoid simulating a situation where \(P_F\) accidentally empties its
stack without accepting, \(P_N\) also utilizes a marker \(X_0\) on the
bottom of its stack.
</p>

<p>
That is \(P_N = (Q \cup \{p_0, p\}, \Sigma, \Gamma \cup \{X_0\}, \delta_N, p_0, X_0)\), where \(\delta_N\)
is:
</p>

<ol class="org-ol">
<li>\(\delta_N(p_0, \epsilon, x_0) = \{(q_0, Z_0, X_0)\}\)</li>
<li>For all states \(q\) in \(Q\), input symbols \(a\) in \(\Sigma\) or \(a = \epsilon\), \(Y\)
in \(\Gamma\), \(\delta_N(q, a, Y)\) contains every pair that is in \(\delta_F(q, a Y)\).</li>
<li>For all accepting states \(q\) in \(F\), and stack symbols \(Y\) in \(\Gamma\)
or \(Y = X_0\), \(\delta_N(q, \epsilon, Y)\) contains \((p, \epsilon)\). Whenever \(P_F\)
accepts, \(P_N\) can start emptying its stack without consuming any input.</li>
<li>For all stack symbols \(Y\) in \(\Gamma\) or \(Y = X_0\), \(\delta_N(p, \epsilon, Y) = \{(p,
   \epsilon)\}\). Once in state \(p\), which only occurs when \(P_F\) is accepted,
\(P_N\) pops every symbol on its stack, until the stack is empty.</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org825d252" class="outline-3">
<h3 id="org825d252"><span class="section-number-3">8.5</span> Equivalence of CFG and PDA</h3>
<div class="outline-text-3" id="text-8-5">
<p>
Let \(A\) be a CFL. From the definition we know that \(A\) has a CFG, \(G\),
generating it. We show how to convert \(G\) into an equivalent PDA.
</p>

<p>
The PDA \(P\) we now describe will work by accepting its input \(w\), if
\(G\) generates that input, by determining whether there is a derivation
for \(w\). Each step of the derivation yields an intermediate string of
variables and terminals. We design \(P\) to determine whether some
series of substitutions of \(G\) can lead from the start variable to
\(w\).
</p>

<p>
The PDA \(P\) begins by writing the start variable on its stack. It goes
through a series of intermediate strings, making one substitution
after another. Eventually it may arrive at a string that contains only
terminal symbols, meaning that it has used the grammar to derive a
string. Then \(P\) accepts if this string is identical to the string it
has received as input.
</p>

<ol class="org-ol">
<li>Place the marker symbol $ and the start variable on the stack</li>
<li>Repeat:
<ul class="org-ul">
<li>If the top of stack is a variable symbol \(A\), nondeterministically
select one of the rules for \(A\) and substitute \(A\) by the string
on the right-hand side of the rule</li>
<li>If the top of stack is a terminal symbol \(a\), read the next
symbol from the input and compare it to \(a\). If they match,
continue. Else, reject the branch of nondeterminism.</li>
<li>If the top of stack is the symbol $, enter the accept state.</li>
</ul></li>
</ol>

<p>
Now we prove the reverse direction. We have a PDA \(P\) and want to make
a CFG \(G\) that generates all the strings that \(P\) accepts.
</p>

<p>
For each pair of states \(p\) and \(q\), the grammar will have a variable
\(A_{pq}\).
</p>

<p>
First, we simplify the task by modifying P slightly to give it the
following three features.
</p>

<ol class="org-ol">
<li>It has a single accept state, \(q_{accept}\).</li>
<li>It empties its stack before accepting.</li>
<li>Each transition either pushes a symbol onto the stack (a push move)
or pops one off the stack (a pop move), but it does not do both at
the same time.</li>
</ol>

<p>
Giving \(P\) features 1 and 2 is easy. To give it feature 3, we replace
each transition that simultaneously pops and pushes with a two
transition sequence that goes through a new state, and we replace each
transition that simultaneously pops and pushes with a two transition
sequence that goes through a new state, and we replace each transition
that neither pops nor pushes with a two transition sequence that
pushes then pops an arbitrary stack symbol.
</p>


<p>
To design \(G\) so that \(A_{pq}\) generates all strings that take \(P\) from
\(p\) to \(q\), regardless of the stack contents at \(p\), leaving the stack
at \(q\) in the same condition as it was at \(p\).
</p>

<p>
First, we simplify our task by modifying \(P\) slightly to give it the
following three features.
</p>
</div>

<div id="outline-container-org4e0b6b4" class="outline-4">
<h4 id="org4e0b6b4"><span class="section-number-4">8.5.1</span> Deterministic Pushdown Automata</h4>
<div class="outline-text-4" id="text-8-5-1">
<p>
DPDAs accept a class of languages between the regular languages and
the CFLs. 
</p>

<p>
We can easily show that DPDAs accept all regular languages by making
it simulate a DFA (ignoring the stack).
</p>

<p>
While DPDAs cannot represent all CFLs, it is able to represent
languages that have unambiguous grammars.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org3899e06" class="outline-2">
<h2 id="org3899e06"><span class="section-number-2">9</span> Properties of Context-Free Languages</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-org5878edf" class="outline-3">
<h3 id="org5878edf"><span class="section-number-3">9.1</span> Simplification of CFG</h3>
<div class="outline-text-3" id="text-9-1">
<p>
The goal is to reduce all CFLs to Chomsky Normal Form. To get there,
we must make several preliminary simplifications, which are useful in
their own ways.
</p>

<ol class="org-ol">
<li>Elimination of useless symbols

<ol class="org-ol">
<li>Remove all <i>non-reachable</i> symbols: starting from \(S\), if it is
impossible to reach a symbol, then it is non-reachable and can be
removed. Example: \(S \rightarrow a, B \rightarrow b\), then \(B\) is not reachable.</li>
<li>Remove all <i>non-generating</i> symbols: for each symbol, check if the
symbol can ever generate a terminating string. If not, remove
it. Example: \(A \rightarrow A | Ab\), \(A\) is non-generating and can be removed.</li>
<li>It is important to remove non-generating symbols first, because
it can lead to more non-reachable symbols.</li>
</ol></li>

<li>Removal of unit productions

<ol class="org-ol">
<li>Unit productions look like \(A \rightarrow B\). We do unit production
elimination to get a grammar into Chomsky Normal form.</li>

<li>To eliminate unit productions, we substitute the unit symbol
into the production.</li>
</ol></li>

<li>Removal of \(\epsilon\) productions

<ol class="org-ol">
<li>Start from any symbol, remove the \(\epsilon\) production, by
substituting all possibilities. E.g. \(S \rightarrow AB | AC\), and we
eliminate $&epsilon;$-productions from \(B\): \(S \rightarrow AB | A | AC\).</li>
<li>Eliminate until no more $&epsilon;$-productions.</li>
<li>When an $&epsilon;$-production is eliminated from a symbol, it does not
need to be reapplied if it appears again in the same symbol.</li>
</ol></li>

<li>Conversion to Chomsky Normal Form</li>
</ol>

<p>
After performing the preliminary simplifications, we can then:
</p>

<ol class="org-ol">
<li>Arrange that all bodies of length 2 or more consist only of variables</li>
<li>Break bodies of length 3 or more into a cascade of productions</li>
</ol>
</div>
</div>

<div id="outline-container-org6c254d3" class="outline-3">
<h3 id="org6c254d3"><span class="section-number-3">9.2</span> Pumping Lemma for CFLs</h3>
<div class="outline-text-3" id="text-9-2">
<p>
In any sufficiently long string in a CFL, it is possible to find at
most 2 short, nearby substrings, that we can pump in tandem. First, we
use several results, that we will state below.
</p>

<ol class="org-ol">
<li>Conversion to CNF converts a parse tree into a binary tree.</li>
<li>For a CNF grammar \(G = (V, T, P, S)\), if the length of the longest
path is \(n\), then \(|w| \le 2^{n-1}\) for all terminal strings \(w\).</li>
</ol>

<div class="theorem">
<p>
<a id="orga31e26f"></a>
Let \(L\) be a CFL. Then there exists a constant \(n\) (which
depends on \(L\)) such that for every string \(z\) in \(L\) such that
\(| z |  \ge n\), we can break \(z\) into three strings \(z = uvwxy\)
such that:
</p>

<ol class="org-ol">
<li>$ vx &ne; &epsilon;$</li>
<li>\(| vwx | \le n\)</li>
<li>For all \(i \ge 0\), the string \(uv^i wx^i y\) is also in \(L\)</li>
</ol>

</div>


<div class="figure">
<p><img src="Pumping Lemma for CFGs/600px-Pumping_lemma_for_context-free_languages.svg_2018-10-09_12-22-03.png" alt="600px-Pumping_lemma_for_context-free_languages.svg_2018-10-09_12-22-03.png" />
</p>
</div>

<p>
If \(s\) is sufficiently long, its derivation tree w.r.t. a Chomsky
normal form grammar must contain some nonterminal \(N\) twice on some
tree path (upper picture). Repeating \(n\) times the derivation part \(N
\Rightarrow \dots \Rightarrow vNx\) obtains a derivation for \(u v^n w x^n y\).
</p>
</div>
</div>

<div id="outline-container-org8fe26e3" class="outline-3">
<h3 id="org8fe26e3"><span class="section-number-3">9.3</span> Closure Properties of CFLs</h3>
<div class="outline-text-3" id="text-9-3">
<p>
First, we introduce the notion of substitutions. Let \(\Sigma\) be an
alphabet, and suppose that for every symbol \(a\) in \(\Sigma\), we choose a
language \(L_a\). These chosen languages can be over any alphabets, not
necessarily \(\Sigma\) and not necessarily the same. The choice of languages
defines a function \(s\) on \(\Sigma\), and we shall refer to \(L_a\) as \(s(a)\)
for each symbol \(a\).
</p>

<p>
If \(w = a_1 a_2 \dots a_n\) in \(\Sigma^*\), then \(s(w)\) is the language of
all strings \(x_1 x_2 \dots x_n\) such that the string \(x_i\) is in the
language \(s(a_i)\). \(s(L)\) is the union of \(s(w)\) for all strings \(w\) in
\(L\).
</p>

<p>
The substitution theorem states that if we can find a substitution
function \(a\) on a CFL, then the resultant language \(s(a)\) is also a
CFL.
</p>

<p>
CFLs are closed under:
</p>

<ol class="org-ol">
<li>Union</li>
</ol>

<p>
\(L_1 \cup L_2\) is the language \(s(L)\), where \(L\) is the language \(\{1,
2\}\), and \(s(1) = L_1\) and \(s(2) = L_2\).
</p>

<ol class="org-ol">
<li>Concatenation</li>
</ol>

<p>
\(L_1 L_2\) is the language \(s(L)\), \(L = {12}\), and \(s(1) = L_1\) and \(s(2)
= L_2\).
</p>

<ol class="org-ol">
<li>Closure, and positive closure (asterisk and plus)</li>
</ol>

<p>
\(L\) is the language \({1}^*\), and \(s\) is the substitution \(s(1) = L_1\),
then \(L_1^* = s(L)\). Similarly, for positive closure, \(L = \{1\}^+\) and
\(L_1^+ = s(L)\).
</p>

<ol class="org-ol">
<li>Homomorphism</li>
</ol>
<p>
\(s(a) = \{h(a)\}\), for all \(a\) in \(\Sigma\). Then \(h(L) = s(L)\).
</p>

<p>
These are the base closure properties. CFLs are also closed under
reversal. We can prove this by constructing a grammar for the reversed
CFL.
</p>

<p>
CFLs are not closed under intersection. However, CFLs are closed under
the operation of "intersection with a regular language". To prove
this, we use the PDA representation of CFLs, and FA representation of
regular languages. We can run the FA in parallel with the PDA.
</p>

<p>
CFLs are also closed under inverse homomorphism. The proof is similar
to that of regular languages, but using a PDA, but is more complex
because of the stack introduced in the PDA.
</p>
</div>
</div>

<div id="outline-container-org35b6c33" class="outline-3">
<h3 id="org35b6c33"><span class="section-number-3">9.4</span> Decision Properties of CFLs</h3>
<div class="outline-text-3" id="text-9-4">
<p>
First, we consider the complexity of converting from a CFG to a PDA,
and vice versa. Let \(n\) be the length of the entire representation of
a PDA or CFG.
</p>

<p>
Below, we list algorithms linear in the size of the input:
</p>

<ul class="org-ul">
<li>Converting a CFG to a PDA</li>
<li>Converting a PDA that accepts by final state to one that accepts by
empty stack</li>
<li>Converting a PDA that accepts by empty stack to one that accepts by
final state</li>
</ul>

<p>
The running time of conversion from a PDA to a grammar is much more
complex. The upper bound on the number of states and stack symbols is
\(n^3\), so there cannot be more than \(n^3\) variables of the form \([pXq\)
constructed for the grammar. However, the running time of conversion
could still be exponential because there are no limits to the number
of symbols put on the stack.
</p>

<p>
However, we can break the pushing of a long string of stack symbols
into a sequence of at most \(n\) steps that each pushes one symbol. Then
each transition has no more than 2 stack symbols, and the total length
of all the transition rules has grown by at most a constant factor,
i.e. it is still \(O(n)\). There are \(O(n)\) transition rules, and each
generates \(O(n^2)\) productions, since there are only 2 states that need
to be chosen in the productions that come from each rule. Hence, the
constructed grammar has length \(O(n^3)\), and can be constructed in
cubic time.
</p>
</div>

<div id="outline-container-org4105b4e" class="outline-4">
<h4 id="org4105b4e"><span class="section-number-4">9.4.1</span> Running Time of Conversion to CNF</h4>
<div class="outline-text-4" id="text-9-4-1">
<ol class="org-ol">
<li>Detecting reachable and generating symbols can be done in \(O(n)\)
time, and removing useless symbols takes \(O(n)\) time and does not
increase the size of the grammar</li>
<li>Constructing unit pairs and eliminating unit productions takes
\(O(n^2)\) time, and results in a grammar whose length is \(O(n)\).</li>
<li>Replacing terminals by variables in production bodies takes \(O(n)\)
time and results in a grammar whose length is \(O(n)\).</li>
<li>Breaking production bodies of length 3 or more takes \(O(n)\) time
and results in a grammar of length \(O(n)\).</li>
</ol>

<p>
However eliminating $&epsilon;$-productions is tricky. If we have a production
body of length \(k\), we can construct from that one production \(2^{k-1}\)
productions for the new grammar, so this part of the construction
could take \(O(2^n)\) time. However, we can break all long production
bodies into a sequence of productions with bodies of length 2. This
step takes \(O(n)\) time and grows only linearly, and makes eliminating
$&epsilon;$-productions run in \(O(n)\) time.
</p>

<p>
In all, converting to CNF form is a \(O(n^2)\) algorithm.
</p>
</div>
</div>

<div id="outline-container-orga98ff9a" class="outline-4">
<h4 id="orga98ff9a"><span class="section-number-4">9.4.2</span> Testing for emptiness of CFL</h4>
<div class="outline-text-4" id="text-9-4-2">
<p>
This is equivalent to testing if \(S\) is generating. The algorithm goes
as follows:
</p>

<p>
We maintain an array indexed by variable, which tells whether or not
we have established that a variable is generating.For each variable
there is a chain of all the positions in which the variable occurs
(solid lines). The dashed lines suggest links from the productions to
their counts.
</p>


<div class="figure">
<p><img src="Properties of Context-Free Languages/screenshot_2018-10-12_17-24-55.png" alt="screenshot_2018-10-12_17-24-55.png" />
</p>
</div>

<p>
For each production, we count the number of positions holding
variables whose ability to generate is not yet accounted for. Each
time the count for a head variable reaches 0, we put the variable on a
queue of generating variables whose consequences need to be explored.
</p>

<p>
This algorithm takes \(O(n)\) time:
</p>

<ol class="org-ol">
<li>There are at most \(n\) variables in a grammar of size \(n\), creation
and initialization of the array can be done in \(O(n)\) time.</li>
<li>Initialization of the links and counts can be done in \(O(n)\) time.</li>
<li>When we discover a production has count 0:
<ol class="org-ol">
<li>Discover a production has count \(0\), finding which variable is
at the head, checking whether it is already known to be
generating, and putting it on the queue if not. All these steps
are \(O(1)\) for each production so \(O(n)\) in total.</li>
<li>work done when visiting the production bodies that have the head
variable \(A\). This work is proportional to the number of
positions with \(A\). Hence, \(O(n)\).</li>
</ol></li>
</ol>
</div>
</div>

<div id="outline-container-org03e04fc" class="outline-4">
<h4 id="org03e04fc"><span class="section-number-4">9.4.3</span> Testing Membership in a CFL</h4>
<div class="outline-text-4" id="text-9-4-3">
<p>
First, we can easily see that algorithms exponential in \(n\) can
decide membership. We can convert the grammar to CNF form. As the
parse trees are binary trees, there will be exactly \(2n-1\) nodes
labeled by variables in the tree for a string \(w\) of length \(n\). The
number of possible trees and node-labelings is only exponential in
\(n\).
</p>

<p>
Fortunately, more efficient techniques exist, based on the idea of
dynamic programming. Once such algorithm is the CYK Algorithm.
</p>

<p>
We construct a triangular table, and begin the fill the table
row-by-row upwards. The horizontal axis corresponds to the positions
of the string \(w = a_1 a_2 \dots a_n\), and the table entry \(X_ij\) is the
set of variables $A such that \(A \overset{*}{\Rightarrow} a_i a_{i+1} \dots a_j\). We
are interested in whether \(S\) is in the set \(X_{1n}\).
</p>



<div class="figure">
<p><img src="Properties of Context-Free Languages/screenshot_2018-10-12_17-37-31.png" alt="screenshot_2018-10-12_17-37-31.png" />
</p>
</div>

<p>
It takes \(O(n)\) time to compute any one entry of the table. Hence, the
table-construction process takes \(O(n^3)\) time.
</p>

<p>
The algorithm for computing \(X_{ij}\) is as such:
</p>

<p>
<b>BASIS</b>: We compute the first row as follows. Since the string beginning
 and ending at position \(i\) is just the terminal \(a_i\), and the grammar
 is in \(CNF\),the only way to derive the string \(a_i\) is to use a
 production of the form \(A \rightarrow a_i\). Hence \(X_ii\) is the set of variables
 \(A\) such that \(A \rightarrow a_i\) is a production of \(G\).
</p>

<p>
<b>INDUCTION</b>: To compute \(X_{ij}\) that is in row \(j - i + 1\), we would have
 computed all the \(X\) in the rows below i.e. we know about all strings
 shorter than \(a_i a_{i+1} \dots a_j\), and we know all the proper prefix
 and proper suffixes of that string.
</p>

<p>
For \(A\) to be in \(X_{ij}\), we must find variables \(B\), \(C\), and integer
\(k\) such that:
</p>

<ol class="org-ol">
<li>\(i \le k < j\)</li>
<li>\(B\) is in \(X_{ik}\)</li>
<li>\(C\) is in \(I_{k+1,j}\)</li>
<li>\(A \rightarrow BC\) is a production of \(G\)</li>
</ol>

<p>
Finding such variables \(A\) requires us to compare at most \(n\) pairs of
previously computed sets. Hence it can be done in \(O(n)\) time.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org57efde8" class="outline-2">
<h2 id="org57efde8"><span class="section-number-2">10</span> Turing Machines</h2>
<div class="outline-text-2" id="text-10">
<p>
We now look at what languages can be defined by any computational
device. The Turing Machine is an accurate model for what any physical
computing device is capable of doing. More technically, turing
machines facilitate proving everyday questions to be undecidable or
intractable.
</p>

<p>
Turing machines are finite automata with a tape of infinite length on
which it may read and write data. The machine consists of a <i>finite
control</i>, which can be in any of a finite set of states. There is a
tape divided into squares or <i>cells</i>; each cell can hold any one of a
finite number of symbols.
</p>


<div class="figure">
<p><img src="Turing Machines/screenshot_2018-10-16_12-52-06.png" alt="screenshot_2018-10-16_12-52-06.png" />
</p>
<p><span class="figure-number">Figure 19: </span>A Turing Machine</p>
</div>

<p>
Initially, the input, which is a finite-length string of symbols
chosen from the input alphabet, is placed on the tape. All other tape
cells, extending infinitely to the left and right, initially hold a
special symbol called the <i>blank</i>. The blank is a tape symbol, but not
an input symbol, and there may be other tape symbols.
</p>

<p>
There is a <i>tape head</i> that is always positioned at one of the tape
cells. The Turing machine is said to be scanning that cell.
</p>

<p>
A move of the Turing machine is a function of the state of the finite
control and the tape symbol scanned.
</p>

<ol class="org-ol">
<li><b>Change state</b>: The next state optionally may be the same as the
current state.</li>
<li><b>Write a tape symbol in the cell scanned</b>. This tape symbol replaces
whatever symbol was in that cell.</li>
<li><b>Move the tape head left or right.</b></li>
</ol>
</div>

<div id="outline-container-org374af1f" class="outline-3">
<h3 id="org374af1f"><span class="section-number-3">10.1</span> Formal Notation</h3>
<div class="outline-text-3" id="text-10-1">
<p>
The formal notation used for a Turing Machine (TM) is by a 7-tuple:
</p>

\begin{equation}
  M = (Q, \Sigma, \Gamma, \delta, q_0, B, F)
\end{equation}

<p>
where:
</p>

<dl class="org-dl">
<dt>Q</dt><dd>The finite set of states of the finite control</dd>
<dt>&Sigma;</dt><dd>The finite set of input symbols</dd>
<dt>&Gamma;</dt><dd>The complete set of tape symbols. &Sigma; is a subset of &Gamma;.</dd>
<dt>&delta;</dt><dd>The transition function. The arguments of \(\delta(q, X)\) are a state \(q\)
and a tape symbol \(X\). The value of \(\delta(q, X)\) is a triple
\((p, Y, D)\) where \(p\) is the next state, \(Y\) is the symbol
in \(\Gamma\) written in the cell being scanned, \(D\) is a
direction, either left or right.</dd>
<dt>q<sub>0</sub></dt><dd>The start state, a member of Q</dd>
<dt>B</dt><dd>the blank symbol, in &Gamma; but not &Sigma;</dd>
<dt>F</dt><dd>the set of final or accepting states</dd>
</dl>
</div>
</div>

<div id="outline-container-org98de437" class="outline-3">
<h3 id="org98de437"><span class="section-number-3">10.2</span> Instantaneous Descriptions</h3>
<div class="outline-text-3" id="text-10-2">
<p>
We use the string \(X_1 X_2 \dots X_{i-1} q X_i X_{i+1} \dots X_n\) to represent
an ID in which:
</p>

<ol class="org-ol">
<li>\(q\) is the state of the Turing machine.</li>
<li>The tape head is scanning the $i$th symbol from the left.</li>
<li>\(X_1 X_2 \dots X_n\) is the portion of the tape between the leftmost
and the rightmost nonblank. As an exception, if the head is to the
left of the leftmost nonblank, or to the right of the rightmost
nonblank, then some prefix or suffix of \(X_1 X_2 \dots X_n\) will be
blank, and \(i\) will be 1 or n, respectively.</li>
</ol>

<p>
We describe moves of a Turing machine \(M = (Q, \Sigma, \Gamma, \delta, q_0, B, F)\) by
the \(\vdash\) notation that was used for PDAs. Suppose \(\delta(q, X_i) = (p,
Y, L)\). Then \(X_1 X_2 \dots X_{i-1} q X_{i+1} \dots X_n \vdash X_1 X_2 \dots X_{i-2}
p X_{i-1} Y X_{i+1} \dots X_n\).
</p>

<p>
There are 2 important exceptions, when \(i=1\) and M moves to the blank
to the left of \(X_1\) and when \(i=n\) and \(Y=B\).
</p>
</div>
</div>

<div id="outline-container-org8b5c47e" class="outline-3">
<h3 id="org8b5c47e"><span class="section-number-3">10.3</span> Transition Diagrams</h3>
<div class="outline-text-3" id="text-10-3">
<p>
Turing machines can be denoted graphically, as with PDAs.
</p>

<p>
An arc from state \(q\) to state \(p\) is labelled by one or more items of
the form \(X/YD\), where \(X\) and \(Y\) are tape symbols, and \(D\) is a
direction from either \(L (\leftarrow)\) or \(R (\rightarrow)\).
</p>


<div class="figure">
<p><img src="Turing Machines/screenshot_2018-10-16_16-09-40.png" alt="screenshot_2018-10-16_16-09-40.png" />
</p>
<p><span class="figure-number">Figure 20: </span>Transition diagram for TM accepting \(0^n 1^n\)</p>
</div>
</div>
</div>

<div id="outline-container-orge10b63f" class="outline-3">
<h3 id="orge10b63f"><span class="section-number-3">10.4</span> The Language of a TM</h3>
<div class="outline-text-3" id="text-10-4">
<p>
\(L(M)\) is the set of strings \(w\) in \(\Sigma^*\) such that \(q_0 w \vdash \alpha p \beta\)
for some state in \(p\) in \(F\) and any tape strings \(\alpha\) and \(\beta\). The set
of languages we can accept using a TM is often called the <i>recursively
enumerable languages</i> or RE languages.
</p>
</div>
</div>

<div id="outline-container-org103c81e" class="outline-3">
<h3 id="org103c81e"><span class="section-number-3">10.5</span> Turing machines and halting</h3>
<div class="outline-text-3" id="text-10-5">
<p>
A TM halts if it enters a state \(q\), scanning a tape symbol \(X\), and
there is no move in this situation: \(\delta(q, X)\) is undefined. This is
another notion of "acceptance". We can assume that a TM halts if it
accepts. That is, without changing the language accepted, we can make
\(\delta(q, X)\) undefined whenever \(q\) is an accepting state. However, it is
not always possible to require that a TM halts even if it does not
accept.
</p>

<p>
Languages that halt eventually, regardless of whether they accept, are
called <i>recursive</i>. If an algorithm to solve a given problem exists,
then we say the problem is decidable, so TMs that always halt figure
importantly into decidability theory.
</p>
</div>
</div>

<div id="outline-container-orge395f87" class="outline-3">
<h3 id="orge395f87"><span class="section-number-3">10.6</span> Programming Techniques for Turing Machines</h3>
<div class="outline-text-3" id="text-10-6">
</div>
<div id="outline-container-org80f9595" class="outline-4">
<h4 id="org80f9595"><span class="section-number-4">10.6.1</span> Storage in the State</h4>
<div class="outline-text-4" id="text-10-6-1">
<p>
We can use the finite control not only to represent a position in the
"program" of a TM, but to hold a <b>finite amount of data</b>. We extend
the state as a tuple \([q, A, B, C]\), and having multiple tracks.
</p>
</div>
</div>

<div id="outline-container-orge38e1bc" class="outline-4">
<h4 id="orge38e1bc"><span class="section-number-4">10.6.2</span> Multiple Tracks</h4>
<div class="outline-text-4" id="text-10-6-2">
<p>
One can also think of the tape of a Turing machine as composed of
several tracks. Each track can hold one symbol, and the tape alphabet
of the TM consists of tuples, with each component for each "track". A
common use of multiple tracks is to treat one track as holding the
data, and another track as holding a mark. We can check off each
symbol as we "use" it, or we can keep track of a small number of
positions within the data by only marking these positions.
</p>
</div>
</div>

<div id="outline-container-orgfc13d10" class="outline-4">
<h4 id="orgfc13d10"><span class="section-number-4">10.6.3</span> Subroutines</h4>
<div class="outline-text-4" id="text-10-6-3">
<p>
A Turing machine subroutine is a set of states that perform some
useful process. This set of states includes a start state and another
state to pass control to whatever other set of states called the
subroutine. Since the TM has no mechanism for remembering a "return
address", that is, a state to go to after it finishes, should our
design of a TM call for one subroutine to be called from several
states, we can make copies of the subroutine, using a new set of
states for each copy. The "calls" are made to the start states of
different copies of the subroutine, and each copy "returns" to a
different state.
</p>
</div>
</div>

<div id="outline-container-orgb6607a7" class="outline-4">
<h4 id="orgb6607a7"><span class="section-number-4">10.6.4</span> Multitape Turing Machines</h4>
<div class="outline-text-4" id="text-10-6-4">
<p>
The device has a finite control (state), and some finite number of
tapes. Each tape is divided into cells, and each cell can hold any
symbol of the finite tape alphabet.
</p>

<p>
Initially, the head of the first tape is at the left end of the input,
but all other tape heads are at some arbitrary cell.
</p>


<div class="figure">
<p><img src="Turing Machines/screenshot_2018-10-16_16-29-52.png" alt="screenshot_2018-10-16_16-29-52.png" />
</p>
<p><span class="figure-number">Figure 21: </span>A multitape Turing machine</p>
</div>

<p>
A move on the multitape TM depends on the state and symbol scanned by
each of the tape heads. On each move:
</p>

<ol class="org-ol">
<li>the control enters a new state, which could be the same as a
previous state.</li>
<li>On each tape, a new tape symbol is written on the cell scanned.
This symbol could be the same as the previous symbol.</li>
<li>Each tape head makes a move, which can be either left, right or stationary.</li>
</ol>
</div>
</div>

<div id="outline-container-orgabafb71" class="outline-4">
<h4 id="orgabafb71"><span class="section-number-4">10.6.5</span> Equivalence of one-tape and multitape TMs</h4>
<div class="outline-text-4" id="text-10-6-5">
<p>
Suppose language \(L\) is accepted by a k-tape TM \(M\). We simulate \(M\)
with a one-tape TM \(N\) whose tape we think of as having 2k tracks.
Half of these tracks hold the tapes of \(M\), and the other half of the
tracks each hold only a single marker that indicates where the head
for the corresponding tape of \(M\) is currently located.
</p>



<div class="figure">
<p><img src="Turing Machines/screenshot_2018-10-16_16-33-42.png" alt="screenshot_2018-10-16_16-33-42.png" />
</p>
<p><span class="figure-number">Figure 22: </span>Simulation of two-tape Turing machine by a one-tape Turing machine</p>
</div>

<p>
To simulate a move of \(M\), \(N\)'s head must visit the \(k\) head markers.
So that \(N\) does not get lost, it must remember how many head markers
are to its left at all times. That count is stored as a component of
\(N\)'s finite control. After visiting each head marker and storing the
scanned symbol in a component of the finite control, \(n\) knows what
tape symbols have been scanned by each of \(M\)'s heads. \(N\) also knows
the state of \(M\), which it stores in \(N\)'s own finite control. Thus,
\(N\) knows what move \(M\) will make. 
</p>

<p>
\(N\) can now revisit each of the head markers on its tape, changing the
symbol in the track representing the corresponding tapes of \(M\), and
move the head markers left or right, if necessary. \(N\)'s accepting
states are all the states that record \(M\)'s states as one of the
accepting states of \(M\). When the simulated \(M\) accepts, \(N\) also
accepts, and \(N\) does not accept otherwise.
</p>

<p>
The time taken by the one-tape TM \(N\) to simulate \(n\) moves of the
k-tape TM \(M\) is \(O(n^2)\).
</p>
</div>
</div>

<div id="outline-container-org8b38d50" class="outline-4">
<h4 id="org8b38d50"><span class="section-number-4">10.6.6</span> Non-deterministic Turing Machines</h4>
<div class="outline-text-4" id="text-10-6-6">
<p>
A NTM differs from the deterministic variety by having a transition
\(\delta\) such that for each state \(q\) and tape symbol \(X\), \(\delta(q,X)\) is a
set of triples \(\{(q_1, Y_1, D_1), \dots, (q_k, Y_k, D_k)\}\).
</p>

<p>
The NTM can choose at each step any of the triples to be the next
move. We can show that NTM and TM are equivalent. The proof involves
showing that for every NTM \(M_N\), we can construct a DTM \(M_D\) that
explores the ID's that \(M_N\) can reach by any sequence of its choices.
If \(M_D\) has an accepting state, then \(M_D\) enters an accepting state of
its own. \(M_D\) must be systematic, putting new ID's on a queue, rather
than a stack, so \(M_D\) would have simulated all sequences up to k moves
of \(M_N\) after some finite time.
</p>

<p>
\(M_D\) is designed as a multi-tape TM. The first tape of \(M_D\) holds a
sequence of ID's of \(M_N\), including the state of \(M_N\). One ID of \(M_N\)
is marked as the current ID, whose successor ID's are in the process
of being discovered. All IDs to the left of the current one have been
explored and can be ignored subsequently.
</p>



<div class="figure">
<p><img src="Turing Machines/screenshot_2018-10-16_16-41-48.png" alt="screenshot_2018-10-16_16-41-48.png" />
</p>
<p><span class="figure-number">Figure 23: </span>Simulation of NTM by a DTM</p>
</div>

<p>
To process the current ID, \(M_D\) does:
</p>

<ol class="org-ol">
<li>\(M_D\) examines the state and scanned symbol of the current ID. Built
into the finite control of \(M_D\) is the knowledge of what choices of
move \(M_N\) has for each state and symbol. If the state in the
current ID is accepting, then \(M_D\) accepts and simulates \(M_N\) no further.</li>
<li>However, if the state is not accepting, and the state-symbol
combination has \(k\) moves, then \(M_D\) uses its second tape to copy
the ID and the make k copies of that ID at the end of the sequence
of ID's on tape 1.</li>
<li>\(M_D\) modifies each of those k ID's according to a different one of
the k choices of move that \(M_N\) has from its current ID.</li>
<li>\(M_D\) returns to the marked current ID, erases the mark and moves to
the next ID to the right. The cycle the repeats with step (1).</li>
</ol>

<p>
This can be viewed as a breadth-first search on all possible IDs
reached.
</p>
</div>
</div>
</div>

<div id="outline-container-org7f378bb" class="outline-3">
<h3 id="org7f378bb"><span class="section-number-3">10.7</span> Restricted Turing Machines</h3>
<div class="outline-text-3" id="text-10-7">
</div>
<div id="outline-container-orgd25756d" class="outline-4">
<h4 id="orgd25756d"><span class="section-number-4">10.7.1</span> Turing Machines with Semi-infinite Tapes</h4>
<div class="outline-text-4" id="text-10-7-1">
<p>
We can assume the tape to be semi-infinite, having no cells to the
left of the initial head position, an still retain the full power of
TMs.
</p>

<p>
The trick behind the construction is to use two tracks on the
semi-infinite tape. The upper track represents the cells of the
original TM that are at or to the right of the initial head position,
but in reverse order. The upper track represents \(X_0 , X_1 \dots\) where
\(X_0\) is the initial position of the head; \(X_1, X_2\) so on are the cells
to its right. The \(*\) on the leftmost cell bottom track serves as an
end marker and prevents the head of the semi-infinite TM from falling
off the end of the tape.
</p>



<div class="figure">
<p><img src="Turing%20Machines/screenshot_2018-10-30_09-22-16.png" alt="screenshot_2018-10-30_09-22-16.png" />
</p>
</div>

<p>
Another restriction we make is to never write a blank. This combined
with the semi-infinite tape restriction means that the tape is at all
times a prefix of non-blank symbols followed by an infinity of blanks.
</p>

<p>
We can construct an equivalent TM \(M_2\) from a TM \(M_1\) by restricting
that \(M_1\) never writes a blank, by creating a new tape symbol \(B'\)
that functions as a blank, but is not the blank \(B\).
</p>
</div>
</div>
</div>

<div id="outline-container-orgbb1b63c" class="outline-3">
<h3 id="orgbb1b63c"><span class="section-number-3">10.8</span> Multistack Machines</h3>
<div class="outline-text-3" id="text-10-8">
<p>
A $k$-stack machine is a deterministic PDA with \(k\) stacks. The
multistack machine has a finite control, which is in one of a finite
state set of states. A move of the multistack machine is based on:
</p>

<ol class="org-ol">
<li>The state of the finite control</li>
<li>The input symbol read, which is chosen from the finite input
alphabet.</li>
</ol>

<p>
Each move allows the multistack machine to:
</p>

<ol class="org-ol">
<li>Change to a new state</li>
<li>Replace the top symbol of each stack with a string of zero or more
stack symbols.</li>
</ol>

<p>
If a language is accepted by a TM, it is also accepted by a two-stack
machine.
</p>
</div>
</div>

<div id="outline-container-orgfa2d399" class="outline-3">
<h3 id="orgfa2d399"><span class="section-number-3">10.9</span> Counter Machines</h3>
<div class="outline-text-3" id="text-10-9">
<p>
Counter machines have the same structure as the multistack machine,
but in place of each stack is a counter. Counters holdd any
non-negative integer, but we can only distinguish between zero and
non-zero counters. That is, the move of the counter machine depends on
its state, input symbol, and which, if any, of the counters are zero.
</p>

<p>
Each move can:
</p>

<ol class="org-ol">
<li>Change state</li>
<li>Add or subtract 1 from any of its counters independently</li>
</ol>

<p>
A counter machine can be thought of as a restricted multistack
machine, where:
</p>

<ol class="org-ol">
<li>There are only 2 stack symbols, which is the bottom-of-stack marker
\(Z_0\), and \(X\).</li>
<li>\(Z_0\) is initially on each stack.</li>
<li>We may replace \(Z_0\) only with \(X^i Z_0\), \(i \ge 0\).</li>
<li>We may replace \(X\) only with \(X^i\), \(i \ge 0\).</li>
</ol>

<p>
We observe the following properties:
</p>

<ol class="org-ol">
<li>Every language accepted by a counter machine is recursively
enumerable.</li>
<li>Every language accepted by a one-counter machine is a CFL. This is
made immediately obvious by considering that it is a one-stack
machine (a PDA).</li>
<li>Every language accepted by a two-counter machine is RE.</li>
</ol>

<p>
The proof is done by showing that three counters is enough to simulate
a TM, and that two counters can simulate three counters.
</p>

<p>
Proof outline: Suppose there are \(r-1\) symbols used by the stack
machine. We can identify the symbols with the digits \(1\) through
\(r-1\), and think of the stack as an integer in base \(r\). We use two
counters to hold the integers that represent each of the two stacks in
a two-stack machine. The third counter is used to adjust the other two
counter, by either dividing or multiplying a count by \(r\), where
\(r-1\) tape symbols are used by the stack machine.
</p>
</div>
</div>

<div id="outline-container-org1fb59fb" class="outline-3">
<h3 id="org1fb59fb"><span class="section-number-3">10.10</span> Turing Machines and Computers</h3>
<div class="outline-text-3" id="text-10-10">
<ol class="org-ol">
<li>A computer can simulate a Turing machine, and</li>
<li>A Turing machine can simulate a computer, and can do so in an
amount of time that is at most some polynomial in the number of
steps taken by the computer</li>
</ol>


<div class="figure">
<p><img src="Turing%20Machines/screenshot_2018-10-30_10-13-41.png" alt="screenshot_2018-10-30_10-13-41.png" />
</p>
</div>

<p>
We can prove the latter by using a TM to simulate the instruction
cycle of the computer, as follows:
</p>

<ol class="org-ol">
<li>Search the first tape for an address matching the instruction
number on tape 2.</li>
<li>Examine the instruction address value, if it requires the value of
some address, that address is part of the instruction. Mark the
position of the instruction using a second tape, not shown in the
picture. Search for the memory address on the first tape, an dcopy
its value onto tape 3, the tape holding the memory address</li>
<li>Execute the instruction. Examples of instructions are:
<ol class="org-ol">
<li>Copying values to some other address</li>
<li>Adding value to some other address</li>
<li>The jump instruction</li>
</ol></li>
</ol>
</div>
</div>

<div id="outline-container-org3b5efc4" class="outline-3">
<h3 id="org3b5efc4"><span class="section-number-3">10.11</span> Runtime of Computers vs Turing Machines</h3>
<div class="outline-text-3" id="text-10-11">
<p>
Running time is an important consideration, because we want to know
what can be computed with enough efficiency. We divide between
tractable and intractable problems, where tractable problems can be
solved efficiently.
</p>

<p>
Hence, we need to assure ourselves that a problem can be solved in
polynomial time on a typical computer, can be solved in polynomial
time on a TM, and conversely.
</p>

<p>
A TM can simulate \(n\) steps of a computer in \(O(n^3)\) time. Consider
the multiplication instruction: if a computer were to start with a
word holding the integer 2, and multiply the word by itself \(n\) times,
it would hold the number \(2^{2^n}\), and would take \(2^n + 1\) bits to
represent, exponential in \(n\).
</p>

<p>
To resolve this issue, one can assert that words retain a maximum
length. We take another approach, imposing the restriction that instructions
may use words of any length, but can only produce words one bit longer
than its arguments.
</p>

<p>
The prove the polynomial time relationship, we note that after \(n\)
instructions have been executed, the number of words mentioned on the
memory tape of the TM is \(O(n)\), and hence requires \(O(n)\) TM cells to
represent. The tape is hence \(O(n^2)\) cells long, and the TM can locate
the finite number of words required by one computer instruction in
\(O(n^2)\) time.
</p>

<p>
Hence, if a computer:
</p>

<ol class="org-ol">
<li>Has only instructions that increase the maximum word length by at
most 1, and</li>
<li>Has only instructions that a multitape TM can perform on wordsd of
length \(k\) in \(O(k^2)\) steps or less, then</li>
</ol>

<p>
the Turing machine can simulate \(n\) steps of the computer in \(O(n^3)\)
of its own steps.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-11-26 Mon 09:55</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
