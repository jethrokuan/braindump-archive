<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-11-26 Mon 09:54 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Artificial Intelligence</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Artificial Intelligence</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org533ec48">1. What is Artificial Intelligence?</a>
<ul>
<li><a href="#orgff545a6">1.1. Acting Humanly: Turing Test</a></li>
<li><a href="#orgb881d00">1.2. Thinking Humanly</a></li>
<li><a href="#orgeda76f3">1.3. Thinking rationally</a></li>
<li><a href="#org98014e8">1.4. Acting Rationally</a></li>
</ul>
</li>
<li><a href="#orgf0d683d">2. Intelligent Agents</a>
<ul>
<li><a href="#orge8126a0">2.1. Rational Agents</a></li>
<li><a href="#orgedfdac7">2.2. Exploration vs Exploitation</a></li>
<li><a href="#org46d5020">2.3. Specifying Task Environment (PEAS)</a></li>
<li><a href="#orgc42c669">2.4. Properties of Task Environments</a></li>
<li><a href="#org1312789">2.5. Table-Driven Agent</a></li>
<li><a href="#org105b037">2.6. Reflex agents</a></li>
<li><a href="#org0808ddb">2.7. Model-based Reflex Agents</a></li>
<li><a href="#orgd47e249">2.8. Goal-based agents</a></li>
<li><a href="#orgda9b055">2.9. Utility-based agents</a></li>
<li><a href="#orgf2d2103">2.10. Learning agents</a></li>
<li><a href="#org76ab81e">2.11. State representations</a>
<ul>
<li><a href="#org2d9803d">2.11.1. Atomic Representation</a></li>
<li><a href="#org68d347c">2.11.2. Factored Representation</a></li>
<li><a href="#org1ee377e">2.11.3. Structured Representations</a></li>
<li><a href="#orga6daf98">2.11.4. Implications</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org9c2be81">3. Problem-Solving</a></li>
<li><a href="#orgf9c8135">4. Classical Search</a>
<ul>
<li><a href="#orgf79408e">4.1. How Search Algorithms Work</a></li>
<li><a href="#org5669856">4.2. Measuring Performance</a></li>
<li><a href="#orgef5a053">4.3. Uninformed Search Strategies</a>
<ul>
<li><a href="#org98adbfe">4.3.1. Breadth-first Search</a></li>
<li><a href="#org57e65cc">4.3.2. Uniform-cost Search</a></li>
<li><a href="#org39629c8">4.3.3. Depth-first Search</a></li>
<li><a href="#org8cbb4d2">4.3.4. Depth-limited Search</a></li>
<li><a href="#org2366c86">4.3.5. Iterative Deepening Depth-first Search</a></li>
<li><a href="#orgfd81acf">4.3.6. Bidirectional Search</a></li>
</ul>
</li>
<li><a href="#org000f0e3">4.4. Informed Search Strategies</a>
<ul>
<li><a href="#org809b0e3">4.4.1. Greedy best-first search</a></li>
<li><a href="#org4cad8a8">4.4.2. A* search</a></li>
</ul>
</li>
<li><a href="#org3efb2fa">4.5. Learning to Search Better</a></li>
</ul>
</li>
<li><a href="#org3074d03">5. Heuristic Functions</a>
<ul>
<li><a href="#orgea81dad">5.1. Generating Admissible Heuristics</a>
<ul>
<li><a href="#org44d7057">5.1.1. From Relaxed Problems</a></li>
<li><a href="#org36c4337">5.1.2. From Subproblems: Pattern Databases</a></li>
<li><a href="#orgd53c5a8">5.1.3. From Experience</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orga4fa459">6. Beyond Classical Search</a>
<ul>
<li><a href="#org1a4049c">6.1. Hill-climbing Search</a>
<ul>
<li><a href="#org74643a9">6.1.1. Variants</a></li>
</ul>
</li>
<li><a href="#org5f3ba02">6.2. Simulated Annealing</a></li>
<li><a href="#orga595642">6.3. Local Beam Search</a></li>
<li><a href="#orgee1a02f">6.4. Genetic Algorithms</a></li>
<li><a href="#orgd42836c">6.5. Local Search in Continuous Spaces</a></li>
<li><a href="#orgc7c1540">6.6. Searching with Non-deterministic Actions</a>
<ul>
<li><a href="#org11dd4f8">6.6.1. AND-OR search trees</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org3602928">7. Adversarial Search</a>
<ul>
<li><a href="#org2b86968">7.1. Optimal Strategy</a></li>
<li><a href="#org5fe5fac">7.2. Alpha-Beta Pruning</a></li>
</ul>
</li>
<li><a href="#org0a029c4">8. Classical Planning</a>
<ul>
<li><a href="#orgc4c1a43">8.1. Complexities of classical planning</a></li>
<li><a href="#orga4be898">8.2. State-space search for planning</a></li>
<li><a href="#org0bd432e">8.3. Heuristics for planning</a></li>
<li><a href="#org482dd0a">8.4. Other classical planning approaches</a></li>
</ul>
</li>
<li><a href="#org597c608">9. Decision Theory</a>
<ul>
<li><a href="#org670196e">9.1. Combining beliefs and desires under uncertainty</a></li>
<li><a href="#org1646b13">9.2. Axioms of Utility Theory</a></li>
<li><a href="#org420ffdb">9.3. Utility assessment and utility scales</a></li>
<li><a href="#org6499302">9.4. Multi-attribute utility functions</a>
<ul>
<li><a href="#orgc0e912a">9.4.1. Dominance</a></li>
<li><a href="#org25f2fe5">9.4.2. Preference structure and multi-attribute utility</a></li>
<li><a href="#org13eef06">9.4.3. Decision Networks</a></li>
<li><a href="#org3b48794">9.4.4. Information Value Theory</a>
<ul>
<li><a href="#org3db5262">9.4.4.1. Properties of the value of information</a></li>
<li><a href="#org78d30da">9.4.4.2. Information Gathering Agents</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#orge7f7d59">10. Making Complex Decisions</a>
<ul>
<li><a href="#orga843c28">10.1. Markov Decision Process (MDP)</a></li>
<li><a href="#org14380ef">10.2. Utilities over time</a></li>
<li><a href="#orgad660b3">10.3. Optimal policies and the utilities of states</a></li>
<li><a href="#org38f8952">10.4. Value Iteration</a></li>
<li><a href="#org269ddae">10.5. Policy Iteration</a></li>
<li><a href="#org6e4e56c">10.6. Summary</a></li>
<li><a href="#org73b1f1b">10.7. Partially Observable MDPs (POMDPs)</a></li>
<li><a href="#org2033b97">10.8. Value iteration for POMDPs</a></li>
<li><a href="#orgb713627">10.9. <span class="todo TODO">TODO</span> Online agents for POMDPs</a></li>
<li><a href="#org598e994">10.10. Monte Carlo Tree Search</a></li>
</ul>
</li>
<li><a href="#org3e855ab">11. Reinforcement Learning</a>
<ul>
<li><a href="#orgb233f3d">11.1. Passive Reinforcement Learning</a>
<ul>
<li><a href="#orgeea71fb">11.1.1. Direct Utility Estimation (MC Learning)</a></li>
<li><a href="#org23590df">11.1.2. Adaptive Dynamic Programming</a></li>
<li><a href="#orgf46bf0e">11.1.3. Temporal-difference Learning</a></li>
</ul>
</li>
<li><a href="#org784b356">11.2. Active Reinforcement Learning</a>
<ul>
<li><a href="#org1c5ec1f">11.2.1. Potential Pitfalls</a></li>
<li><a href="#org52fdacd">11.2.2. Learning an action-utility function</a></li>
<li><a href="#orgf810f0e">11.2.3. Q-learning</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orge07f974">12. RANDOM</a>
<ul>
<li><a href="#orgfd9eff2">12.1. Simon's Ant</a></li>
</ul>
</li>
<li><a href="#orgcd98514">13. REFILE</a></li>
<li><a href="#orged91972">14. Papers</a>
<ul>
<li><a href="#org6b319e1">14.1. Improving Policy Gradient by Exploring Under-Appreciated Rewards</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org533ec48" class="outline-2">
<h2 id="org533ec48"><span class="section-number-2">1</span> What is Artificial Intelligence?</h2>
<div class="outline-text-2" id="text-1">
<p>
Designing agents that act rationally (e.g. through maximising a reward
function).
</p>

<p>
Humans often act in ways that do not maximise their own benefit
(irrational).
</p>
</div>
<div id="outline-container-orgff545a6" class="outline-3">
<h3 id="orgff545a6"><span class="section-number-3">1.1</span> Acting Humanly: Turing Test</h3>
<div class="outline-text-3" id="text-1-1">
<p>
A computer would require:
</p>

<ul class="org-ul">
<li>natural language processing</li>
<li>knowledge representation</li>
<li>automated reasoning</li>
<li>machine learning</li>
</ul>
</div>
</div>
<div id="outline-container-orgb881d00" class="outline-3">
<h3 id="orgb881d00"><span class="section-number-3">1.2</span> Thinking Humanly</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Cognitive science brings together computer models and experimental
techniques in psychology to construct testable and provable theories
of the human mind.
</p>
</div>
</div>
<div id="outline-container-orgeda76f3" class="outline-3">
<h3 id="orgeda76f3"><span class="section-number-3">1.3</span> Thinking rationally</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Taking informal knowledge and expressing it in logical terms.
</p>
</div>
</div>
<div id="outline-container-org98014e8" class="outline-3">
<h3 id="org98014e8"><span class="section-number-3">1.4</span> Acting Rationally</h3>
<div class="outline-text-3" id="text-1-4">
<p>
A rational agent is one that acts so as to achieve the best outcome
or,when there is uncertainty, the best expected outcome.
</p>

<p>
An agent is a function from percept histories to actions, i.e. \(f: P^*
\rightarrow A\). We seek the best-performing agent for a certain task;
must consider computation limits.
</p>
</div>
</div>
</div>
<div id="outline-container-orgf0d683d" class="outline-2">
<h2 id="orgf0d683d"><span class="section-number-2">2</span> Intelligent Agents</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>Agents perceive the environment through sensors</li>
<li>Agents act upon the environment through actuators</li>
</ul>
</div>
<div id="outline-container-orge8126a0" class="outline-3">
<h3 id="orge8126a0"><span class="section-number-3">2.1</span> Rational Agents</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>For each possible percept sequence, select an action that is
expected to maximise its performance measure. The performance
measure is a function of a given sequence of environment states.</li>
<li>Given the evidence provided by the percept sequence and whatever
built-in knowledge the agent has.</li>
<li>Agents can perform actions that help them gather useful information
(exploration)</li>
<li>An agent is <i>autonomous</i> if its behaviour is determined by its own
experience (with ability to learn and adapt)</li>
</ul>
</div>
</div>
<div id="outline-container-orgedfdac7" class="outline-3">
<h3 id="orgedfdac7"><span class="section-number-3">2.2</span> Exploration vs Exploitation</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Doing actions that modify future percepts (information gathering) is
an important part of rationality. In most scenarios, agents don't know
the entire environment <i>a priori</i>.
</p>
</div>
</div>
<div id="outline-container-org46d5020" class="outline-3">
<h3 id="org46d5020"><span class="section-number-3">2.3</span> Specifying Task Environment (PEAS)</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>Performance measure</li>
<li>Environment</li>
<li>Actuators</li>
<li>Sensors</li>
</ul>
</div>
</div>
<div id="outline-container-orgc42c669" class="outline-3">
<h3 id="orgc42c669"><span class="section-number-3">2.4</span> Properties of Task Environments</h3>
<div class="outline-text-3" id="text-2-4">
<dl class="org-dl">
<dt>Fully observable</dt><dd>whether an agent's sensors gives it access to
the complete state of the environment at any given point in time</dd>
<dt>Deterministic</dt><dd>if the next state is completely determined by the
current environment. Otherwise it is <b>stochastic</b>.</dd>
<dt>Episodic</dt><dd>whether an agents experience is divided into atomic
episodes. In each episode, an agent receives a percept
and performs a single action. In <b>sequential</b>
environments short-term actions can have long-term
consequences. For this reason, episodic environments are
generally simpler.</dd>
<dt>Static</dt><dd>whether the environment can change while the agent is
deliberating.</dd>
<dt>Discrete</dt><dd>whether the state of the environment, how time is
handled, and the percepts and actions of the agent
discretely quantized.</dd>
<dt>Single agent</dt><dd>in some environments, for example chess, there are
multiple agents acting in the same environment.
<dl class="org-dl">
<dt>cooperative</dt><dd>if the two agents need to work together.</dd>
<dt>competitive</dt><dd>if the two agents are working against each other.</dd>
</dl></dd>
<dt>Known</dt><dd>whether the agent knows the outcome of its actions.</dd>
</dl>
</div>
</div>
<div id="outline-container-org1312789" class="outline-3">
<h3 id="org1312789"><span class="section-number-3">2.5</span> Table-Driven Agent</h3>
<div class="outline-text-3" id="text-2-5">
<p>
Simple to implement, and works. However, the number of table entries
is exponential in time: \(\text{#percepts}^\text{time}\). Hence it is
doomed to failure. The key challenge to AI is to produce rational
behaviour from a small program rather than a vast table.
</p>
</div>
</div>
<div id="outline-container-org105b037" class="outline-3">
<h3 id="org105b037"><span class="section-number-3">2.6</span> Reflex agents</h3>
<div class="outline-text-3" id="text-2-6">
<p>
A simple reflex agent is one that selects actions on the basis of the
<i>current</i> percept, ignoring the rest of the percept history. A
<i>condition-action</i> rule is triggered upon processing the current
percept. E.g. <b>if</b> the car in front is braking, <b>then</b> brake too.
</p>

<p>
Basing actions on only the current percept can be highly limiting, and
can also lead to infinite loops. Randomized actions of the right kind
can help escape these infinite loops.
</p>
</div>
</div>
<div id="outline-container-org0808ddb" class="outline-3">
<h3 id="org0808ddb"><span class="section-number-3">2.7</span> Model-based Reflex Agents</h3>
<div class="outline-text-3" id="text-2-7">
<p>
The agent maintains some <b>internal state</b> that depends on percept
history and reflects at least some of the unobserved aspects of the
current state. Information about how the world evolves independently
from the agent is encoded into the agent. This knowledge is called a
<b>model</b> of the world, and this agent is hence a <b>model-based</b> agent.
</p>
</div>
</div>
<div id="outline-container-orgd47e249" class="outline-3">
<h3 id="orgd47e249"><span class="section-number-3">2.8</span> Goal-based agents</h3>
<div class="outline-text-3" id="text-2-8">
<p>
Knowing about the current state of the environment may not be enough
to decide on what to do. Agents may need <b>goal</b> information that
describes situations that are desirable. Sometimes goal-based action
selection is straightforward, but in others <b>searching</b> and <b>planning</b>
are required to achieve the goal. Goal-based agents are flexible
because the knowledge that supports its decisions is represented
explicitly and can be modified, although it is less efficient.
</p>
</div>
</div>
<div id="outline-container-orgda9b055" class="outline-3">
<h3 id="orgda9b055"><span class="section-number-3">2.9</span> Utility-based agents</h3>
<div class="outline-text-3" id="text-2-9">
<p>
Goals provide a binary distinction between good and bad states. A more
general performance measure should allow a comparison between world
states according to exactly how good it is to the agent. An agent's
<b>utility function</b> is an internalisation of the performance measure.
An agent chooses actions to maximise its expected utility. A
utility-based agents has to model and keep track of its environment.
</p>
</div>
</div>
<div id="outline-container-orgf2d2103" class="outline-3">
<h3 id="orgf2d2103"><span class="section-number-3">2.10</span> Learning agents</h3>
<div class="outline-text-3" id="text-2-10">
<p>
A learning agent can be divided into four conceptual components.
</p>
<dl class="org-dl">
<dt>learning element</dt><dd>responsible for making improvements</dd>
<dt>performance element</dt><dd>responsible for selecting extrenal actions</dd>
<dt>problem generator</dt><dd>suggests actions that will lead to new and
informative experiences</dd>
</dl>

<p>
the learning element takes in feedback from the <b>critic</b> on how the
agent is doing and determines show the performance element should be
modified to do better in the future.
</p>
</div>
</div>
<div id="outline-container-org76ab81e" class="outline-3">
<h3 id="org76ab81e"><span class="section-number-3">2.11</span> State representations</h3>
<div class="outline-text-3" id="text-2-11">
</div>
<div id="outline-container-org2d9803d" class="outline-4">
<h4 id="org2d9803d"><span class="section-number-4">2.11.1</span> Atomic Representation</h4>
<div class="outline-text-4" id="text-2-11-1">
<p>
In an atomic representation each state of the world is indivisible,
and has no internal structure. Search, game-playing, hidden Markov
models and Markov decision processes all work with atomic
representations.
</p>
</div>
</div>
<div id="outline-container-org68d347c" class="outline-4">
<h4 id="org68d347c"><span class="section-number-4">2.11.2</span> Factored Representation</h4>
<div class="outline-text-4" id="text-2-11-2">
<p>
A factored representation splits up each state into a fixed set of
<b>variables</b> or <b>attributes</b>, each of which can have a <b>value</b>.
</p>

<p>
Constraint satisfaction algorithms, propositional logic, planning,
Bayesian networks and machine learning algorithms work with factored
representations.
</p>
</div>
</div>

<div id="outline-container-org1ee377e" class="outline-4">
<h4 id="org1ee377e"><span class="section-number-4">2.11.3</span> Structured Representations</h4>
<div class="outline-text-4" id="text-2-11-3">
<p>
Structured representations underlie relational databases and
first-order logic, first-order probability models, knowledge-based
learning and much of natural language understanding.
</p>
</div>
</div>

<div id="outline-container-orga6daf98" class="outline-4">
<h4 id="orga6daf98"><span class="section-number-4">2.11.4</span> Implications</h4>
<div class="outline-text-4" id="text-2-11-4">
<p>
A more expressive representation can capture, at least as concisely, a
everything a more expressive one can capture, plus more. On the other
hand, reasoning and learning become more complex as the expressive
power of the representation increases.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org9c2be81" class="outline-2">
<h2 id="org9c2be81"><span class="section-number-2">3</span> Problem-Solving</h2>
<div class="outline-text-2" id="text-3">
<p>
Problem-solving agents use <i>atomic</i> representations, as compared to
goal-based agents, which use more advanced factored or structured
representations.
</p>

<p>
The process of looking for a sequence of actions that reaches the goal
is called <i>search</i>. A search algorithm takes a problem as input and
returns a <i>solution</i> in the form of an action sequence.
</p>
</div>
</div>

<div id="outline-container-orgf9c8135" class="outline-2">
<h2 id="orgf9c8135"><span class="section-number-2">4</span> Classical Search</h2>
<div class="outline-text-2" id="text-4">
<p>
This addresses observable, deterministic, and known environments where
the solution is a sequence of actions.
</p>
</div>

<div id="outline-container-orgf79408e" class="outline-3">
<h3 id="orgf79408e"><span class="section-number-3">4.1</span> How Search Algorithms Work</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Search algorithms consider various possible action sequences. The
possible action sequences start at the initial state form a <i>search
tree</i>.
</p>

<p>
Search algorithms require a data structure to keep track of the search
tree that is being constructed.
</p>

<dl class="org-dl">
<dt>state</dt><dd>state in the state space to which the node corresponds</dd>
<dt>parent</dt><dd>the node in the search tree that generated this node</dd>
<dt>action</dt><dd>the action that was applied to the parent to generate this node</dd>
<dt>path-cost</dt><dd>the cost, traditionally denoted by \(g(n)\), of the path
from the initial state to the node, as indicated by the
parent pointers</dd>
</dl>
</div>
</div>

<div id="outline-container-org5669856" class="outline-3">
<h3 id="org5669856"><span class="section-number-3">4.2</span> Measuring Performance</h3>
<div class="outline-text-3" id="text-4-2">
<dl class="org-dl">
<dt>completeness</dt><dd>is the algorithm guaranteed to find a solution if
it exists?</dd>
<dt>optimality</dt><dd>does the strategy find the optimal solution?</dd>
<dt>time complexity</dt><dd>how long does it take to find a solution?</dd>
<dt>space complexity</dt><dd>how much memory is required to do the search?</dd>
</dl>
</div>
</div>

<div id="outline-container-orgef5a053" class="outline-3">
<h3 id="orgef5a053"><span class="section-number-3">4.3</span> Uninformed Search Strategies</h3>
<div class="outline-text-3" id="text-4-3">
</div>
<div id="outline-container-org98adbfe" class="outline-4">
<h4 id="org98adbfe"><span class="section-number-4">4.3.1</span> Breadth-first Search</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
The root node is expanded first, then all the successors of the root
node are expanded next, then their successors, and so on.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">performance</th>
<th scope="col" class="org-left">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">completeness</td>
<td class="org-left">YES</td>
</tr>

<tr>
<td class="org-left">optimal</td>
<td class="org-left">NO</td>
</tr>

<tr>
<td class="org-left">time complexity</td>
<td class="org-left">\(O(b^d)\)</td>
</tr>

<tr>
<td class="org-left">space complexity</td>
<td class="org-left">\(O(b^d)\)</td>
</tr>
</tbody>
</table>

<p>
The shallowest node may not be the most optimal node.
</p>

<p>
The space used in the <i>explored set</i> is \(O(b^{d-1})\) and the space
used in the <i>frontier</i> is \(O(b^d)\).
</p>

<p>
In general, exponential-complexity search problems cannot be solved by
uninformed methods for any but the smallest instances.
</p>
</div>
</div>
<div id="outline-container-org57e65cc" class="outline-4">
<h4 id="org57e65cc"><span class="section-number-4">4.3.2</span> Uniform-cost Search</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
Uniform-cost search expands the node \(n\) with the lowest path
cost \(g(n)\). The goal test is applied to a node when it is selected
for expansion rather than when it is first generated.
</p>

<p>
This is equivalent to BFS if all step costs are qual.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">performance</th>
<th scope="col" class="org-left">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">completeness</td>
<td class="org-left">MAYBE</td>
</tr>

<tr>
<td class="org-left">optimal</td>
<td class="org-left">YES</td>
</tr>

<tr>
<td class="org-left">time</td>
<td class="org-left">\(O(b^{1+\lfloor{\frac{C^*}{\epsilon}}\rfloor})\), where \(C^*\) is the optimal cost.</td>
</tr>

<tr>
<td class="org-left">space</td>
<td class="org-left">\(O(b^{1+\lfloor{\frac{C^*}{\epsilon}}\rfloor})\)</td>
</tr>
</tbody>
</table>

<p>
Completeness is guaranteed only if the cost of every step exceeds some
small positive constant \(\epsilon\). an infinite loop may occur if
there is a path with an infinite sequence of zero-cost actions.
</p>
</div>
</div>
<div id="outline-container-org39629c8" class="outline-4">
<h4 id="org39629c8"><span class="section-number-4">4.3.3</span> Depth-first Search</h4>
<div class="outline-text-4" id="text-4-3-3">
<p>
Always expands the <i>deepest</i> node in the current frontier of the
search tree.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">performance</th>
<th scope="col" class="org-left">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">completeness</td>
<td class="org-left">YES</td>
</tr>

<tr>
<td class="org-left">optimal</td>
<td class="org-left">NO</td>
</tr>

<tr>
<td class="org-left">time complexity</td>
<td class="org-left">\(O(b^m)\)</td>
</tr>

<tr>
<td class="org-left">space complexity</td>
<td class="org-left">\(O(b^m)\), \(O(m)\) if backtrack</td>
</tr>
</tbody>
</table>

<p>
The time complexity of DFS may be worse than BFS: \(O(b^m)\) might be
larger than \(O(b^d)\).
</p>

<p>
DFS only requires storage of \(O(bm)\) nodes, where \(m\) is the maximum
depth of any node. <b>backtracking search</b> only generates one successor
at a time, modifying the current state description rather than copying
it. Memory requirements reduce to one state description and \(O(m)\)
actions.
</p>
</div>
</div>
<div id="outline-container-org8cbb4d2" class="outline-4">
<h4 id="org8cbb4d2"><span class="section-number-4">4.3.4</span> Depth-limited Search</h4>
<div class="outline-text-4" id="text-4-3-4">
<p>
In depth-limited search, nodes at depth of pre-determined limit \(l\)
are treated as if they had no successors. This limit solves the
infinite-path problem.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">performance</th>
<th scope="col" class="org-left">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">completeness</td>
<td class="org-left">YES</td>
</tr>

<tr>
<td class="org-left">optimal</td>
<td class="org-left">NO</td>
</tr>

<tr>
<td class="org-left">time complexity</td>
<td class="org-left">\(O(b^l)\)</td>
</tr>

<tr>
<td class="org-left">space complexity</td>
<td class="org-left">\(O(b^l)\), \(O(l)\) if backtrack</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org2366c86" class="outline-4">
<h4 id="org2366c86"><span class="section-number-4">4.3.5</span> Iterative Deepening Depth-first Search</h4>
<div class="outline-text-4" id="text-4-3-5">
<p>
Key idea is to gradually increase the depth limit: first 0, then
1, then 2&#x2026; until a goal is found.
</p>


<div class="figure">
<p><img src="images/artificial_intelligence/Problem-Solving/screenshot_2018-01-22_15-26-50.png" alt="screenshot_2018-01-22_15-26-50.png" />
</p>
</div>

<p>
\(N(IDS) = (d)b + (d-1)b^2 + \dots + (1)b^d\), which gives a time
complexity of \(O(b^d)\)
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">performance</th>
<th scope="col" class="org-left">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">completeness</td>
<td class="org-left">YES</td>
</tr>

<tr>
<td class="org-left">optimal</td>
<td class="org-left">NO (unless step cost is 1)</td>
</tr>

<tr>
<td class="org-left">time complexity</td>
<td class="org-left">\(O(b^d)\)</td>
</tr>

<tr>
<td class="org-left">space complexity</td>
<td class="org-left">\(O(b^d)\), \(O(m)\) if backtrack</td>
</tr>
</tbody>
</table>


<ol class="org-ol">
<li>BFS and IDS are complete if \(b\) is finite.</li>
<li>UCS is complete if \(b\) is finite and step cost is \(\ge \epsilon\).</li>
<li>BFS and IDS are optimal if all step costs are identical.</li>
</ol>
</div>
</div>
<div id="outline-container-orgfd81acf" class="outline-4">
<h4 id="orgfd81acf"><span class="section-number-4">4.3.6</span> Bidirectional Search</h4>
<div class="outline-text-4" id="text-4-3-6">
<p>
Conduct two simultaneous searches &#x2013; one forward from the initial
state, and the other backward from the goal. This is implemented by
replacing the goal test with a check to see whether the frontiers of
two searches intersect. This reduces the time ad space complexity to \(O(b^{d/2})\).
</p>
</div>
</div>
</div>
<div id="outline-container-org000f0e3" class="outline-3">
<h3 id="org000f0e3"><span class="section-number-3">4.4</span> Informed Search Strategies</h3>
<div class="outline-text-3" id="text-4-4">
</div>
<div id="outline-container-org809b0e3" class="outline-4">
<h4 id="org809b0e3"><span class="section-number-4">4.4.1</span> Greedy best-first search</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
<i>Greedy best-first search</i> tries to expand the node that is closest to
the goal, on the grounds that this is likely to lead to a solution
quickly. It evaluates nodes by using just the heuristic function:
\(f(n) = h(n)\).
</p>

<p>
Greedy best-first tree search is incomplete even in a finite state
space. The graph search version is complete in finite spaces, but not
in infinite ones. The worst case time and space complexity is
\(O(b^m)\). However, with a good heuristic function, the complexity can
be reduced substantially.
</p>
</div>
</div>
<div id="outline-container-org4cad8a8" class="outline-4">
<h4 id="org4cad8a8"><span class="section-number-4">4.4.2</span> A* search</h4>
<div class="outline-text-4" id="text-4-4-2">
<p>
It evaluates nodes by combining \(g(n)\) the cost to reach the node, and
\(h(n)\) the cost to get to the goal: \(f(n) = g(n) + h(n)\). Since \(g(n)\)
gives the path cost from the start node to node \(n\), and \(h(n)\) is the
estimated cost of the cheapest path from \(n\) to the goal,$f(n) = $
estimated cost of the cheapest solution through \(n\).
</p>

<p>
\(h(n)\) is an <i>admissible heuristic</i> iff it never overestimates the
cost to reach the goal. For A*, this means that \(f(n)\) would never
overestimate the cost of a solution along the current path.
</p>

<p>
Admissible heuristics are by nature optimistic because they think the
cost of solving the problem is less than it actually is.
</p>

<p>
A second, slightly stronger condition is called <i>consistency</i>, and is
required only for applications of A* to graph search. A heuristic
\(h(n)\) is <i>consistent</i> iff for every node \(n\) and every successor \(n'\)
of \(n\) generated by any action \(a\), the estimated cost of reaching the
goal from \(n\) is no greater than the step cost of getting to \(n'\) plus
the estimated cost of reaching the goal from \(n'\): \(h(n) \le
c(n,a,n') + h(n')\). This is a form of the general triangle inequality.
</p>

<p>
A* search is complete, optimal and optimally efficient with a
consistent heuristic. The latter means that no other optimal algorithm
is guaranteed to expand fewer nodes than A*.
</p>

<p>
However, for most problems, the number of states within the goal
contour search space is still exponential in the length of the
solution. 
</p>

<p>
The <i>absolute error</i> of a heuristic is defined as \(\Delta = h^*-h\),
and the <i>relative error</i> is defined as \(\epsilon = \frac{h^*-h}{h*}\).
The complexity results depend strongly on the assumptions made about
the state space. For constant step costs, it is \(O(b^{\epsilon d})\),
and the effective branching factor is \(b^\epsilon\).
</p>

<p>
A* keeps all generated nodes in memory, and hence it usually runs out
of space  long before it runs of time. Hence, it is not practical for
large-scale problems.
</p>

<p>
Other memory-bounded heuristic searches include:
</p>
<ul class="org-ul">
<li>iterative-deepening A* (IDA*)</li>
<li>Recursive best-first search (RBFS)</li>
<li>Memory-bounded A* (MA*)</li>
<li>simplified MA* (SMA*)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org3efb2fa" class="outline-3">
<h3 id="org3efb2fa"><span class="section-number-3">4.5</span> Learning to Search Better</h3>
<div class="outline-text-3" id="text-4-5">
<p>
Each state in a <i>metalevel state space</i> captures the internal
computational state of a program that is searching in an <i>object-level
state space</i>. A <i>metalevel learning</i> algorithm can learn from
experiences to avoid exploring unpromising subtrees. The goal of the
learning is to minimise the total cost of problem solving, trading off
computational expense and path cost.
</p>
</div>
</div>
</div>

<div id="outline-container-org3074d03" class="outline-2">
<h2 id="org3074d03"><span class="section-number-2">5</span> Heuristic Functions</h2>
<div class="outline-text-2" id="text-5">
<p>
If for any node n \(h_2(n) \ge h_1(n)\), we say that \(h_2\) <i>dominates</i>
\(h_1\). Domination translates directly into efficiency: A* using \(h_2\)
will never expand more nodes than \(h_1\). Hence it is generally better
to use a heuristic function with higher value, while making sure it is
consistent, and computing the heuristic function is computationally
feasible.
</p>
</div>

<div id="outline-container-orgea81dad" class="outline-3">
<h3 id="orgea81dad"><span class="section-number-3">5.1</span> Generating Admissible Heuristics</h3>
<div class="outline-text-3" id="text-5-1">
</div>
<div id="outline-container-org44d7057" class="outline-4">
<h4 id="org44d7057"><span class="section-number-4">5.1.1</span> From Relaxed Problems</h4>
<div class="outline-text-4" id="text-5-1-1">
<p>
Because the relaxed problem adds edges to the state space, any optimal
solution in the original problem is, by definition, also a solution in
the relaxed problem. Hence, the cost of an optimal solution to a
relaxed problem is an admissible heuristic for the original problem.
Because the derived heuristic is an exact cost for the relaxed
problem, it must obey the triangle inequality and is therefore
consistent.
</p>
</div>
</div>

<div id="outline-container-org36c4337" class="outline-4">
<h4 id="org36c4337"><span class="section-number-4">5.1.2</span> From Subproblems: Pattern Databases</h4>
<div class="outline-text-4" id="text-5-1-2">
<p>
<i>Pattern Databases</i> store exact solution costs for every possible
subproblem instance. In the case of the 8-puzzle, every possible
configuration of the four tiles and the blank. Each pattern database
yields an admissible heuristic, and these heuristics can be combined
by taking the maximum value. Solutions to subproblems can overlap:
<i>disjoint pattern databases</i> account for this. These work by dividing
the problem in a way that each move affects only one subproblem.
</p>
</div>
</div>

<div id="outline-container-orgd53c5a8" class="outline-4">
<h4 id="orgd53c5a8"><span class="section-number-4">5.1.3</span> From Experience</h4>
<div class="outline-text-4" id="text-5-1-3">
<p>
Inductive learning methods work best when supplied with <i>features</i> of
a state that are relevant to predicting the state's value. A common
approach to combining features would be through a linear combination:
\(h(n) = c_1x_1(n) + c_2x_2(n)\).
</p>

<p>
These heuristics satisfy the requirement that \(h(n) = 0\) for goal
states, but are not necessarily admissible or consistent.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orga4fa459" class="outline-2">
<h2 id="orga4fa459"><span class="section-number-2">6</span> Beyond Classical Search</h2>
<div class="outline-text-2" id="text-6">
<p>
Here, we cover algorithms that perform purely <i>local search</i> in the
state space, evaluating and modifying one or more current states
rather than systematically exploring paths from an initial state.
These include methods inspired by statistical physics (simulated
annealing) and evolutionary biology (genetic algorithms).
</p>

<p>
If an agent cannot predict exactly what percept it will receive, then
it will need to consider what to do under each <i>contingency</i> that its
percepts may reveal.
</p>

<p>
If the path to the goal doesn't matter, we giht consider a different
class of algorithms, ones that do not worry about the paths at all.
<i>Local search</i> algorithms operate using a single <i>current node</i> and
generally move only to neighbours of that node. Its advantages
include:
</p>

<ol class="org-ol">
<li>They generally use a <span class="underline">constant amount of memory</span></li>
<li>They can often find <span class="underline">reasonable solutions in large or infinite
state spaces</span> where systematic algorithms are not suitable.</li>
</ol>
</div>

<div id="outline-container-org1a4049c" class="outline-3">
<h3 id="org1a4049c"><span class="section-number-3">6.1</span> Hill-climbing Search</h3>
<div class="outline-text-3" id="text-6-1">
<p>
The hill-climbing search is a loop that continually moves in the
direction of increasing value. 
</p>

<p>
Consider the 8-queens problem.
</p>

<p>
Local search algorithms typically use a <i>complete-state formation</i>. The
successors of a state are all possible states generated by moving a
single queen to another square in the same column.
</p>

<p>
We could use a heuristic cost function \(h\) equal to the number of
queens that are attacking each other, either directly or indirectly.
</p>

<p>
The global minimum of this function is zero, which only occurs for
perfect solutions. Hill-climbing algorithms typically choose randomly
among the set of best successors having the lowest \(h\).
</p>

<p>
Hill-climbing algorithms can get stuck for the following reasons:
</p>

<ul class="org-ul">
<li>local maxima</li>
<li id="ridges">sequence of local maxima</li>
<li id="plateaux">flat local maximum, or <i>shoulder</i>, from which progress
is possible.</li>
</ul>
</div>


<div id="outline-container-org74643a9" class="outline-4">
<h4 id="org74643a9"><span class="section-number-4">6.1.1</span> Variants</h4>
<div class="outline-text-4" id="text-6-1-1">
<dl class="org-dl">
<dt>stochastic hill-climbing</dt><dd>chooses at random from among the uphill
moves; the probability of selection can vary with the steepness
of the uphill move. Usually converges more slowly, but finds
better solutions.</dd>
<dt>first-choice hill-climbing</dt><dd>stochastic hill-climbing with randomly
generated successors until one is generated that is better than
the current state. Good when state has many successors.</dd>
<dt>random-restart hill-climbing</dt><dd>conducts hill-climbing searches from
randomly generated initial states, until a goal is found.
Trivially complete with probability approaching 1.</dd>
</dl>
</div>
</div>
</div>

<div id="outline-container-org5f3ba02" class="outline-3">
<h3 id="org5f3ba02"><span class="section-number-3">6.2</span> Simulated Annealing</h3>
<div class="outline-text-3" id="text-6-2">
<p>
A hill-climbing algorithm that never makes 'downhill' moves towards
states with lower-value is guaranteed to be incomplete, because it can
be stuck on a local maximum.
</p>

<div class="org-src-container">
<pre class="src src-text">function SIMULATED-ANNEALING(problem, schedule)
  inputs: problem, a problem
          schedule, a mapping from time to 'temperature'

  current ← MAKE-NODE(problem, INITIAL-STATE)
  for t = 1 to ∞ do
    T ← schedule(t)
    if T = 0 then return current
    next ← a randomly selected successor of current
    𝞓E ← next.VALUE - current.VALUE
    if 𝞓E &gt; 0 then current ← next
    else current ← next only with probability e^(𝞓E/T)
</pre>
</div>
</div>
</div>

<div id="outline-container-orga595642" class="outline-3">
<h3 id="orga595642"><span class="section-number-3">6.3</span> Local Beam Search</h3>
<div class="outline-text-3" id="text-6-3">
<p>
<a href="https://www.youtube.com/watch?v=RLWuzLLSIgw">https://www.youtube.com/watch?v=RLWuzLLSIgw</a>
</p>

<p>
Local beam search keeps track of \(n\) states rather than just one. It
begins with \(n\) randomly generated states, at each step all the
successors of all states are generated. If any one is a goal, the
algorithm halts. 
</p>

<p>
Local-beam search passes useful information between the parallel
search threads (compared to running random-restart \(n\) times), quickly
abandoning unfruitful searches and moves its resources to where the
most progress is being made.
</p>

<p>
Stochastic local beam search chooses \(n\) successors at random, with
the probability of choosing a given successor being an increasing
function of its value.
</p>
</div>
</div>

<div id="outline-container-orgee1a02f" class="outline-3">
<h3 id="orgee1a02f"><span class="section-number-3">6.4</span> Genetic Algorithms</h3>
<div class="outline-text-3" id="text-6-4">
<p>
A <i>genetic algorithm</i> is a variant of stochastic beam search in which
successor states are generated by combining two parent states rather
than by modifying a single state.
</p>

<p>
GA begins with a set of \(n\) randomly generated states, called the
<i>population</i>. Each state is also called an <i>individual</i>.
</p>

<p>
The production of the next generation of states is rated by the
objective function, or <i>fitness function</i>. A fitness function returns
higher values for better states.
</p>

<p>
Like stochastic beam search, genetic algorithms combine an uphill
tendency with random exploration and exchange of information among
parallel search threads. <i>crossover</i> in genetic algorithms raises the
level of granularity at which the search operates.
</p>

<div class="org-src-container">
<pre class="src src-text">function GENETIC-ALGORITHM(population, FITNESS-FN) returns an individual
  inputs: population, a set of individuals
          FITNESS-FN, a function that measures the fitness of an
  individual

  repeat
    new_population ← empty set
    for i = 1 to SIZE(population) do
      x ← RANDOM-SELECTION(population, FITNESS-FN)
      y ← RANDOM-SELECTION(population, FITNESS-FN)
      child ← REPRODUCE(x,y)
      if (small random probability) then child ← MUTATE(child)
      add child to new_population
    population ← new_population
  until some individual is fit enough, or enough has elapsed
  return the best individual in population, according to FITNESS-FN

function REPRODUCE(x,y) returns an individual
  inputs: x,y, parent individuals

  n ← LENGTH(x); c ← random(1,n)
  return APPEND(SUBSTRING(x,1,c), SUBSTRING(y, c+1, n))
</pre>
</div>
</div>
</div>

<div id="outline-container-orgd42836c" class="outline-3">
<h3 id="orgd42836c"><span class="section-number-3">6.5</span> Local Search in Continuous Spaces</h3>
<div class="outline-text-3" id="text-6-5">
<p>
One way to avoid continuous problems is simply to <i>discretize</i> the
neighbourhood of each state. Many methods attempt to use the
<i>gradient</i> of the landscape to find a maximum: \(x \leftarrow x +
\delta \nabla (x)\), where \(\delta\) is a small constant called the <i>step
size</i>. For many problems, the <i>Newton-Raphson</i> method is effective. It
solves the roots for equations \(g(x) = 0\), by computing a new
estimate: \(x \leftarrow x - g'(x)/g(x)\). To find a maximum or minimum
of \(f\), we need to find \(x\) such that the gradient is zero. In this
case \(g(\mathbf{x})\) in Newton's formula becomes \(\nabla
f(\mathbf{x})\) and the update equation can be written in matrix-vector
form as:
</p>

\begin{align*}
\mathbf{x} \leftarrow \mathbf{x} - H_f^{-1}(\mathbf{x})\nabla f(\mathbf{x})
\end{align*}

<p>
where \(H_f\) is the <i>Hessian</i> matrix of second derivatives. For
high-dimensional problems, computing the \(n^2\) entries of the Hessian
and inverting it may be expensive, and approximate versions have been
developed.
</p>

<p>
Local search methods suffer from local maxima, ridges and plateaux
in continuous spaces just as much as in discrete spaces.
</p>
</div>
</div>

<div id="outline-container-orgc7c1540" class="outline-3">
<h3 id="orgc7c1540"><span class="section-number-3">6.6</span> Searching with Non-deterministic Actions</h3>
<div class="outline-text-3" id="text-6-6">
<p>
When the environment is either partially observable or
non-deterministic, percepts become useful. In a partially observable
environment, every percept helps narrow down the set of possible
states the agent might be in. In a non-deterministic environment,
percepts tell the agent which of the possible outcomes of its actions
has actually occurred. Future percepts cannot be determined in
advance, and the agent's future actions will depend on those future
percepts. The solution to a problem is not a sequence but a
<i>contingency plan</i>
</p>

<p>
The solutions for no-deterministic problems can contain nested
if-then-else statements, meaning they are trees and not sequences.
</p>
</div>

<div id="outline-container-org11dd4f8" class="outline-4">
<h4 id="org11dd4f8"><span class="section-number-4">6.6.1</span> AND-OR search trees</h4>
<div class="outline-text-4" id="text-6-6-1">
<p>
A solution for an AND-OR search problem is a subtree that:
</p>

<ol class="org-ol">
<li>includes every outcome branch leaf</li>
<li>specifies one action at each of its OR nodes</li>
<li>includes every outcome branch at each of its AND nodes</li>
</ol>

<div class="org-src-container">
<pre class="src src-text">function AND-OR-GRAPH-SEARCH(problem) returns a conditional plan, or failure
  OR-SEARCH(problem, INITIAL-STATE, problem, [])

function OR-SEARCH(state,problem,path) returns a conditional plan, or failure
  if problem, GOAL-TEST(state) then return the empty plan
  if state is on path then return failure
  for each action in problem, ACTIONS(state) do
    plan ← AND-SEARCH(RESULTS(state,action), problem, [state | path])
    if plan ≠ failure then return [action | plan]
  return failure

function AND-SEARCH(states,problem,path) returns a conditional plan, or failure
  for each s_i in states do
    plan_i ← OR-SEARCH(s_i,problem,path)
    if plan_i = failure then return failure
  return [if s_1 then plan_1 else if s_2 then plan_2 ...]
</pre>
</div>

<p>
(stop at AIMA 4.4)
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org3602928" class="outline-2">
<h2 id="org3602928"><span class="section-number-2">7</span> Adversarial Search</h2>
<div class="outline-text-2" id="text-7">
<p>
Competitive environments, in which the agent's goals are in conflict,
give rise to <i>adversarial search</i> problems.
</p>

<p>
Game theory views any multi-agent environment as a game, provied that
the impact of each agent on the others is significant.
</p>

<p>
Games often have large branching factors, and require making some
decision even before computing the optimal decision.
</p>

<p>
<i>Pruning</i> allows us to ignore portions of the search tree that make no
difference to the final choice, and heuristic evaluation functions
allow us to approximate the true utility of a state without doing a
complete search.
</p>

<p>
A game can be formally defined as a search problem with the following
elements:
</p>

<dl class="org-dl">
<dt>\(S_0\)</dt><dd>the <i>initial state</i>, which specifies how the game is set up
at the start</dd>
<dt>\(Player(s)\)</dt><dd>Defines which player has the move in a state</dd>
<dt>\(Actions(s)\)</dt><dd>Returns the set of legal moves in a state</dd>
<dt>\(Result(s,a)\)</dt><dd>The transition model, which defines the result of a move</dd>
<dt>\(TerminalTest(s)\)</dt><dd>Terminal test, which is true when the game is
over, and false otherwise.</dd>
<dt>\(Utility(s,p)\)</dt><dd>A utility function defines the numeric value for a
game that ends in terminal state \(s\) for a player
\(p\).</dd>
</dl>


<p>
The initial state, \(Actions\) function and \(Result\) function define the
game tree for the game.
</p>
</div>


<div id="outline-container-org2b86968" class="outline-3">
<h3 id="org2b86968"><span class="section-number-3">7.1</span> Optimal Strategy</h3>
<div class="outline-text-3" id="text-7-1">
<p>
The optimal strategy can be determined from the <i>minimax value</i> of
each node (\(Minimax(n)\)). The minimax value of a node is the utility
of being in the corresponding state, assuming that players play
optimally from there to the nd of the game. The minimax value of a
terminal state is its utility.
</p>

\begin{align}
  Minimax(s) =
  \begin{cases}
    Utility(s), \text{ if } TerminalTest(s) \\
    max_{a \in Actions(s)}MINIMAX(Result(s,a)), \text{if Player(s) =
      Max} \\
    min_{a \in Actions(s)}MINIMAX(Result(s,a)), \text{if Player(s) = Min}
  \end{cases}
\end{align}

<p>
Minimax uses utility function on leaf nodes, backing up through the
tree, setting the node value to be the minimum of the children.
</p>
</div>
</div>

<div id="outline-container-org5fe5fac" class="outline-3">
<h3 id="org5fe5fac"><span class="section-number-3">7.2</span> Alpha-Beta Pruning</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Eliminate parts of the search tree that do not affect decision. 
</p>
</div>
</div>
</div>

<div id="outline-container-org0a029c4" class="outline-2">
<h2 id="org0a029c4"><span class="section-number-2">8</span> Classical Planning</h2>
<div class="outline-text-2" id="text-8">
<p>
Problem-solving agents that deal with atomic representations of states
require good domain-specific heuristics to perform well. The hybrid
propositional logical agent can find plans without domain-specific
heuristics because it uses domain-independent heuristics based on the
logical structure of the problem. However, it relies on ground
propositional inference, and suffers when there are large numbers of
actions and states.
</p>

<p>
<b>Planning Domain Definition Language (PDDL)</b> was created in response to
these deficiencies.
</p>

<p>
Each state is represented as a conjunction of fluents that are ground,
functionless atoms. Database semantics is used, which involves:
</p>

<dl class="org-dl">
<dt>closed-world assumption</dt><dd>all fluents not mentioned are false</dd>
<dt>unique names assumption</dt><dd>fluents with different names are distinct</dd>
</dl>

<p>
State representations are carefully designed so that they can be
manipulated by set operations or logical inference.
</p>

<p>
Actions are defined in terms of the <b>preconditions</b> and <b>effects</b>.
Preconditions and effects are described in terms of a conjunction of
literals.
</p>
</div>

<div id="outline-container-orgc4c1a43" class="outline-3">
<h3 id="orgc4c1a43"><span class="section-number-3">8.1</span> Complexities of classical planning</h3>
<div class="outline-text-3" id="text-8-1">
<p>
<b>PlanSAT</b> is the question of whether there exists any plan that solves a
planning problem. <b>Bounded PlanSAT</b> asks whether there is a solution of
length k or less.
</p>

<p>
While the number of states is finite, adding function symbols make
them infinite, making these problems semi-decidable at best. Certain
restrictions can reduce the questions into a P class problem.
</p>

<p>
However, most agents would not be asked to derive plans for worst-case
problem instances. For many problem domains, bounded PlanSAT is
NP-complete, while PlanSAT is in P.
</p>
</div>
</div>

<div id="outline-container-orga4be898" class="outline-3">
<h3 id="orga4be898"><span class="section-number-3">8.2</span> State-space search for planning</h3>
<div class="outline-text-3" id="text-8-2">
<p>
The first approach is forward (propogation) state-space search, which
searches forward from the initial state. However, it is inefficient,
for the following reasons:
</p>

<ol class="org-ol">
<li>It tends to explore irrelevant actions</li>
<li>Planning problems tend to have large state spaces, and relatively
small problems will be an issue without a good heuristic</li>
</ol>

<p>
It turn out that good domain-independent heuristics can be derived for
forward search, which makes it feasible.
</p>

<p>
Alternatively, we can do a backward (regression) state-space search,
which looks for actions that can lead to the goal. Unlike forward
search, backward search only explores relevant actions, hence has a
low branching factor. However, backward search deals with sets, which
are make it harder to derive good domain-independent heuristics.
</p>
</div>
</div>

<div id="outline-container-org0bd432e" class="outline-3">
<h3 id="org0bd432e"><span class="section-number-3">8.3</span> Heuristics for planning</h3>
<div class="outline-text-3" id="text-8-3">
<p>
Framing the search problem as a graph where the nodes are states and
the edges are actions. We can think of a number of ways to relax the
problem, generating admissible heuristics:
</p>

<ol class="org-ol">
<li>Add more edges to the graph, making it easier to find a path</li>
</ol>

<p>
The <b>ignore preconditions heuristic</b> drops all preconditions from
actions, and every action becomes applicable in every state. We count
the number of actions required such that the union of the action's
effects satisfy the goal. This is called the set-cover problem, which
is unfortunately NP-hard. We can also ignore selected preconditions of
actions.
</p>

<p>
The ignore delete list heuristic, drops all negative literals in goals
and preconditions. This way, an action cannot undo progress towards
the goal, and each action taken would monotonically progress towards
it.
</p>

<ol class="org-ol">
<li>Grouping multiple nodes together, shrinking the size of the graph</li>
</ol>

<p>
We can reduce the number of states by forming a <b>state abstraction</b> &#x2013; a
many-to-one mapping from states in the ground representation of the
problem to the abstract representation. For example, one can ignore
some fluents.
</p>

<p>
The key idea in defining heuristics is <b>decomposition</b>: diving a problem
into parts. <b>Subgoal independence</b> is the assumption that the cost
of solving a conjunction of subgoals is approximated by the sum of the
costs of solving a subgoal independently.
</p>
</div>
</div>

<div id="outline-container-org482dd0a" class="outline-3">
<h3 id="org482dd0a"><span class="section-number-3">8.4</span> Other classical planning approaches</h3>
<div class="outline-text-3" id="text-8-4">
<p>
We can translate a problem description in PDDL to  a form that can be
processed by SATPlan. The steps are below:
</p>

<ol class="org-ol">
<li>Propositionalize the actions: replace each action schema with a set
of ground actions formed by substituting constants for each of the
variables. These ground actions are not part of the translation,
but will be used in subsequent steps.</li>
<li>Define the initial state: assert \(F^0\) for every fluent \(F\) in the
problem's initial state, and \(\neg F^0\) for every fluent not mentioned
in the final state.</li>
<li>Propositionalize the goal: the goal becomes a disjunction over all
of its ground instances, where variables are replaced by constants.</li>
<li>Add successor-state axioms: For each fluent \(F\), add an axiom of
the form:</li>
</ol>

\begin{equation}
  F^{t+1} \iff ActionCausesF^t \vee \left( F^t \wedge \neg ActionCausesNotF^t \right)
\end{equation}

<p>
Where \(ActionCausesF\) is a disjunction of all the ground actions that
have \(F\) in their add list, and \(ActionCausesNotF\) is a disjunction of
all ground actions that have \(F\) in their delete list.
</p>

<ol class="org-ol">
<li>Add precondition axioms: for each ground action \(A\), add the axiom
\(A^t \iff PRE(A)^t\), i.e. if an action is taken at time \(t\), then the
preconditions must be true.</li>
<li>Add action exclusion axioms: say that exactly one ground action can
occur at each step.</li>
</ol>

<div class="org-src-container">
<pre class="src src-text">function SATPLAN(init, transition, goal, Tmax) returns solution or failure
  ""inputs: init, transition, goal constitute the problem
    description
    Tmax: upper limit for plan length""
  for t = 0 to Tmax do
    cnf &lt;- translate-to-sat(init, transition, goal, t)
    model &lt;- SATSolver(cnf)
    if model is not null then
      return extract-solution(model)
  return failure
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org597c608" class="outline-2">
<h2 id="org597c608"><span class="section-number-2">9</span> Decision Theory</h2>
<div class="outline-text-2" id="text-9">
<p>
Reference: AIMA Chapter 16
</p>

<p>
Utility theory combines with probability theory to yield a
decision-making agent, that can make rational decisions based on what
it believes and what it wants. This is opposed to goal-based agents,
that make binary decisions between good and bad states.
</p>
</div>

<div id="outline-container-org670196e" class="outline-3">
<h3 id="org670196e"><span class="section-number-3">9.1</span> Combining beliefs and desires under uncertainty</h3>
<div class="outline-text-3" id="text-9-1">
<p>
Under nondeterministic, partially observable environments, the result
of an action cannot be represented by a single number. \(RESULT(\alpha)\) is
now expressed as a random variable, whose values are the possible
outcome states with corresponding probabilities of occurrence. The
probability of an outcome \(s'\), given evidence observations
\(\mathbb{e}\), is written \(P(RESULT(\alpha) = s' | a, \mathbb{e})\).
</p>

<p>
The agent's preferences are captured by a <b>utility function</b> \(U(s)\),
which expresses the desirability of a state. We can calculate the
expected utility of a function as follows:
</p>

\begin{equation}
  EU(a | \mathbb{e}) = \sum_{s'} P(RESULT(\alpha) = s' | a,  \mathbb{e}) U(s')
\end{equation}

<p>
The principle of maximum expected utility (MEU) says that a rational
agent should choose the action that maximizes the expected utility. If
an agent acts to maximise a utility function that correctly reflects
all possible outcomes, then it will achieve the highest possible
performance score (averaged across all environments).
</p>
</div>
</div>

<div id="outline-container-org1646b13" class="outline-3">
<h3 id="org1646b13"><span class="section-number-3">9.2</span> Axioms of Utility Theory</h3>
<div class="outline-text-3" id="text-9-2">
<p>
To show why MEU is a rational choice, we turn to utility theory.
First, we define notation:
</p>

<ol class="org-ol">
<li>\(A \succ B\): the agent prefers \(A\) over \(B\).</li>
<li>\(A \sim B\): the agent is indifferent between \(A\) and \(B\).</li>
<li>\(A \succeq B\): the agent prefers \(A\) over \(B\), or is indifferent.</li>
</ol>

<p>
The axioms of utility theory are:
</p>

<dl class="org-dl">
<dt>orderability</dt><dd>Exactly one of \(A \succ B\), \(B \succ A\) or \(A \sim B\) holds.</dd>
<dt>transitivity</dt><dd>\((A \succ B) \wedge (B \succ C) \Rightarrow A \succ C\)</dd>
<dt>continuity</dt><dd>\(A \succ B \succ C \Rightarrow \exist p [p, A; 1-p, C] \sim B\)</dd>
<dt>substitutability</dt><dd>\(A \sim B \Rightarrow [p, A; 1-p, C] \sim [p, B; 1-p, C]\)</dd>
<dt>monotonicity</dt><dd>\(A \succ B \Rightarrow (p > q) \iff [p, A; 1-p B] \succ [q, A; 1-q,
                  B]\)</dd>
<dt>Decomposability</dt><dd>\([p, A; 1-p, [q, B; 1-q, C]] \sim[p, A;(1-p)q, B;
     (1-p)q, C]\)</dd>
</dl>

<p>
von Neumann and Morgenstern showed that following these axioms, it can
be shown that.
</p>

<ol class="org-ol">
<li><b>Utility functions exist</b>. \(U(A) > U(B) \iff A \succ B\) and \((U(A) = U(B)
   \iff A \sim B)\).</li>
<li><b>The utility of a lottery</b>, can be represented as the sum of
probability of each outcome, multiplied by the utility of that
outcome: \(U([p_1, S_1; \dots; p_n, S_n]) = \sum_i p_iU(S_i)\).</li>
</ol>

<p>
Utility functions are not unique: in fact, that are not changed with
affine transformations, of the form \(U'(s) = aU(s) + b\). This is
because the agent only needs a preference on the ranking of states,
and the actual value of the utility does not matter.
</p>
</div>
</div>

<div id="outline-container-org420ffdb" class="outline-3">
<h3 id="org420ffdb"><span class="section-number-3">9.3</span> Utility assessment and utility scales</h3>
<div class="outline-text-3" id="text-9-3">
<p>
To build a decision-theoretic system, we must first work out what the
agent's utility function is. This process is called <b>preference
elicitation</b>, which involves presenting choices to the agent and using
the observed preferences to pin down the underlying utility function.
</p>

<p>
We have established above that utility functions are not unique, but
it helps to have a <b>normalized utility</b>. Normalized utilities fix the
utility of a "best possible prize" at \(u_\top = 1\) and the "worst possible
prize" at \(u_\bot = 0\).
</p>

<p>
Given this utility scale, we can assess the utility of a prize \(S\), by
asking the agent to choose between \(S\) and the standard lottery \([p,
u_\top;1-p, u_\bot]\). \(p\) is adjusted until the agent is indifferent
between \(S\) and the lottery. This is done for each prize \(S\).
</p>
</div>
</div>

<div id="outline-container-org6499302" class="outline-3">
<h3 id="org6499302"><span class="section-number-3">9.4</span> Multi-attribute utility functions</h3>
<div class="outline-text-3" id="text-9-4">
<p>
When outcomes are characterized by two or more attributes, we need a
multi-attribute utility function.
</p>
</div>

<div id="outline-container-orgc0e912a" class="outline-4">
<h4 id="orgc0e912a"><span class="section-number-4">9.4.1</span> Dominance</h4>
<div class="outline-text-4" id="text-9-4-1">
<p>
We say that there is strict dominance of \(S_1\) over \(S_2\) if \(S_2\) is
lower on all attributes as compared to \(S_1\). Under uncertainty, we use
a more useful generalization: <b>stochastic dominance</b>.
</p>

<p>
If \(A_1\) stochastically dominates \(A_2\), then for any monotonically
increasing utility function \(U(x)\), the expected utility of \(A_1\) is at
least as high as the expected utility of \(A_2\). We can understand this
via the cumulative distribution. For any action \(A_1\) and \(A_2\), that
lead to probability distributions \(p_1(x)\) and \(p_2(x)\) on the random
variable \(X\):
</p>

\begin{equation}
  \forall x \int_{-\infty}^{x} p_1(x')dx' \le \int_{-\infty}^{x} p_2(x')dx'
\end{equation}
</div>
</div>


<div id="outline-container-org25f2fe5" class="outline-4">
<h4 id="org25f2fe5"><span class="section-number-4">9.4.2</span> Preference structure and multi-attribute utility</h4>
<div class="outline-text-4" id="text-9-4-2">
<p>
Multi-attribute utility theory is based on the supposition that
preferences of typical agents have structure. This alleviates the
difficulty in expressing a utility function with many attributes and
possible distinct values.
</p>

<p>
One simple regularity in preference structures is called preference
independence. Two attributes \(X_1\) and \(X_2\) are preferentially
independent if the preference between outcomes \((x_1, x_2, x_3)\) and
\((x_1', x_2', x_3)\) does not depend on the value of the value \(x_3\) of
attribute \(X_3\).
</p>

<p>
If all attributes are mutually preferentially independent, then an
agent's preference can be described by maximising the function:
</p>

\begin{equation}
  V(x_1, x_2, \dots, x_n) = \sum_i V_i(x_i)
\end{equation}

<p>
An example is:
</p>

\begin{equation}
V(noise, cost, deaths) = - noise \times 10^4 - cost - deaths \times 10^{12}
\end{equation}

<p>
Under uncertainty, the mathematics gets a bit more complicated.
Utility independence extends preference independence to cover
lotteries: a set of attributes \(\mathbb{X}\) is utility independent of
a set of attributes \(\mathbb{Y}\) if preference between lotteries on
the attributes in \(\mathbb{X}\) are independent of the particular
values of the attributes in \(\mathbb{Y}\).
</p>

<p>
Mutual utility independence implies that the agent's behaviour can be
described using a multiplicative utility function. If we denote \(U_i\)
to be \(U_i(x_i)\), then a 3 attribute utility function can be expressed
as:
</p>

\begin{equation}
 U = k_1U_1 + k_2U_2 + k_3U_3 + k_1k_2U_1U_2 + k_1k_3U_1U_3 +
 k_2k_3U_2U_3 + k_1k_2k_3U_1U_2U_3
\end{equation}
</div>
</div>

<div id="outline-container-org13eef06" class="outline-4">
<h4 id="org13eef06"><span class="section-number-4">9.4.3</span> Decision Networks</h4>
<div class="outline-text-4" id="text-9-4-3">
<p>
Decision networks extend Bayesian networks with nodes for actions and
utilities. An example of a decision network is below:
</p>


<div class="figure">
<p><img src="images/artificial_intelligence/Decision Networks/diagnosisDN_2018-09-01_16-50-40.png" alt="diagnosisDN_2018-09-01_16-50-40.png" />
</p>
</div>

<p>
The notation is as such:
</p>

<dl class="org-dl">
<dt>chance nodes (oval)</dt><dd>these represent random variables</dd>
<dt>decision nodes (rectangles)</dt><dd>these represent points where the
decision maker has a choice of actions.</dd>
<dt>utility nodes (diamonds)</dt><dd>represent the agent's utility function</dd>
</dl>

<p>
A simplified form is used in many cases. The notation remains
identical, but the chance nodes describing the outcome state are
omitted. Rather than representing a utility function on outcome states,
the utility node represents the expected utility associated with each
action. This node is associated with an action-utility function (also
known as Q-function). 
</p>
</div>
</div>

<div id="outline-container-org3b48794" class="outline-4">
<h4 id="org3b48794"><span class="section-number-4">9.4.4</span> Information Value Theory</h4>
<div class="outline-text-4" id="text-9-4-4">
<p>
The value of a given piece of information is defined to be the
difference in expected value between the best action before and after
information is obtained.  This is one of the most important parts of
decision-making: knowing what information to obtain.  Information has
value to the extent that it is likely to cause a change of plan and to
the extent that the new plan is significantly better than the old
plan.
</p>

<p>
Mathematically, the value of perfect information (VPI) is defined as:
</p>

\begin{equation}
VPI_e(E_j) = \left( \sum_k P(E_j = e_{jk} | \mathbb{e})
  MEU(\alpha_{e_{jk}} | \mathbb{e}, E_j = e_{jk}) \right) - MEU(\alpha |
\mathbb{e})
\end{equation}

<p>
This is obtained by considering the best action (maximum expected
utility) before and after obtaining the information, and averaging it
across all possible values for the new information, using our current
beliefs of its value.
</p>
</div>

<div id="outline-container-org3db5262" class="outline-5">
<h5 id="org3db5262"><span class="section-number-5">9.4.4.1</span> Properties of the value of information</h5>
<div class="outline-text-5" id="text-9-4-4-1">
<p>
First, the expected value of information is non-negative. 
</p>

\begin{equation}
  \forall \mathbb{e}, E_j VPI_{\mathbb{e}} (E_j) \ge 0
\end{equation}

<p>
The theorem is about the expected value, and not the real value. This
means that information can sometimes lead to a plan that is harmful.
</p>

<p>
It is important to note that VPI is dependent on the current state of
information. VPI is not additive in general:
</p>

\begin{equation}
  VPI_{\mathbb{e}}(E_j, E_k) \ne VPI_{\mathbb{e}}(E_j) + VPI_{\mathbb{e}}(E_k)
\end{equation}

<p>
VPI is order independent. That is:
</p>

\begin{equation}
  VPI_{\mathbb{e}}(E_j, E_k) = VPI_{\mathbb{e}}(E_j) +
  VPI_{\mathbb{e}, e_j}(E_k) = VPI_{\mathbb{e}}(E_k) +
  VPI_{\mathbb{e}, e_k}(E_j)
\end{equation}
</div>
</div>

<div id="outline-container-org78d30da" class="outline-5">
<h5 id="org78d30da"><span class="section-number-5">9.4.4.2</span> Information Gathering Agents</h5>
<div class="outline-text-5" id="text-9-4-4-2">
<p>
we can implement a myopic information-gathering agent, by using the
VPI formula shortsightedly.
</p>

<div class="org-src-container">
<pre class="src src-text">function INFORMATION-GATHERING-AGENT(percept) returns an action
  persistent: D, a decision network

integrate percept into D
j &lt;- the value that maximises VPI(E_j) / Cost(E_j)
if VPI(E_j) &gt; Cost(E_j)
  return REQUEST(E_j)
else return the best action from D
</pre>
</div>

<p>
If we know the associated cost of observing evidence, it simply
retrieves the evidence if the cost of observing it is less than the
value it provides.
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-orge7f7d59" class="outline-2">
<h2 id="orge7f7d59"><span class="section-number-2">10</span> Making Complex Decisions</h2>
<div class="outline-text-2" id="text-10">
<p>
Earlier, we were concerned with environments with one-shot, episodic
decision problems. Sequential decision problems incorporate utilities,
uncertainty and sensing. These include searching and planning problems
as special cases.
</p>
</div>

<div id="outline-container-orga843c28" class="outline-3">
<h3 id="orga843c28"><span class="section-number-3">10.1</span> Markov Decision Process (MDP)</h3>
<div class="outline-text-3" id="text-10-1">
<p>
A sequential decision problem for a fully observable, stochastic
environment with a Markovian transition model and additive rewards is
called a Markov decision process. It consists of a set of states, with
initial state \(s_0\), a set \(ACTIONS(s)\) of actions in each state, a
transition model \(P(s'|s, a)\), and a reward function \(R(s)\).
</p>

<p>
A policy, denoted \(\pi\), specifies what the agent should do in any state
\(s\). This action is denoted by \(\pi(s)\). The optimal policy $&pi;<sup>*</sup>$*yields the
highest expected utility.
</p>

<p>
The careful balancing of risk and reward is a characteristic of MDPs
that does not arise in deterministic search problems.
</p>
</div>
</div>

<div id="outline-container-org14380ef" class="outline-3">
<h3 id="org14380ef"><span class="section-number-3">10.2</span> Utilities over time</h3>
<div class="outline-text-3" id="text-10-2">
<p>
A finite horizon for decision making means that there is a fixed time
\(N\) after which nothing matters. In these scenarios, the optimal
action in a given state could change over time, i.e. the optimal
policy is non-stationary.
</p>

<p>
It turns out that under stationarity, there are only 2 coherent ways
to assign utilities to sequences:
</p>

<dl class="org-dl">
<dt>additive rewards</dt><dd>\(U_h([s_0, s_1, \dots, s_n]) = R(s_0) + R(s_1) + \dots + R(s_n)\)</dd>
<dt>discounted rewards</dt><dd>\(U_h([s_0, s_1, \dots, s_n]) = R(s_0) + \gamma \cdot R(s_1) + \dots +
     \gamma^2 \cdot R(s_n)\)</dd>
</dl>

<p>
This discount factor \(\gamma\) is a number between 0 and 1. Assuming
stationarity has several problems. First, if the environment does not
contain a terminal state, then utilities of undiscounted rewards go to
infinite, and comparing two infinitely state sequences would be
impossible. With discounted rewards, the utility of an infinite
sequence can be made finite.
</p>

<p>
However, if the environment contains a terminal state, and the agent
is guaranteed to reach a terminal state eventually, then this policy
is called a <b>proper policy</b>, and the above issue goes away. Infinite
sequences can be compared in terms of the average reward obtained per
time step. 
</p>
</div>
</div>

<div id="outline-container-orgad660b3" class="outline-3">
<h3 id="orgad660b3"><span class="section-number-3">10.3</span> Optimal policies and the utilities of states</h3>
<div class="outline-text-3" id="text-10-3">
<p>
First, we can derive the expected utility of executing a policy \(\pi\) in
\(s\):
</p>

\begin{equation}
U^\pi (s) = \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^t R(S_t) \right]
\end{equation}

<p>
where the expectation is with respect to the probability distribution
over state sequences. determined by \(s\) and \(\pi\). Then $&pi;<sup>*</sup>(s) =
argmax<sub>&pi;</sub> U^&pi; (s)*.
</p>

<p>
A consequence of using discounted utilities with infinite horizons is
that the optimal policy is independent of the starting state. This
allows us to compute the true utility of the state as \(U^{\pi^*} (s)\).
The utility function allows the agent to select actions by using the
principle of maximum expected utility from the earlier chapter: \(\pi^*(s)
= argmax_{a \in A(s) } \sum_{s^{i}} P(s' |s, a)U(s')\).
</p>
</div>
</div>

<div id="outline-container-org38f8952" class="outline-3">
<h3 id="org38f8952"><span class="section-number-3">10.4</span> Value Iteration</h3>
<div class="outline-text-3" id="text-10-4">
<p>
The bellman equation illustrates that the utility of a state is the
immediate reward for that state plus the expected discounted utility
of the next state:
</p>

\begin{equation}
U(s) = R(s) + \gamma max_{a \in A(s)} \sum_{s'} P(s' | s, a)U(s')
\end{equation}

<p>
If there are \(n\) possible states, there are \(n\) Bellman equations to
solve. However, these equations are non-linear, and cannot be solved
using linear algebra techniques.
</p>

<p>
Value iteration is an algorithm that is guaranteed to converge to an
equilibrium. The basic idea is to start with arbitrary initial values
for the utilities. We compute the right hand side of the equation, and
update the utility on the left hand side of the equation.
</p>

<div class="org-src-container">
<pre class="src src-text">function VALUE-ITERATION(mdp, e) returns a utility function
  inputs: mdp, an MDP with states S, actions A(s), transition model P(s' | s, a), discount \gamma
          e, the maximium error allowed in the utility of any state
  locals: U, U', vectors of utilities for states in S, initially zero
          \delta, the maximum change in utility for any state
  repeat
    U &lt;- U'; \delta &lt;- 0
    for each state s in S do
      U'[s] &lt;- R(s) + \gamma max_{a \in A(s)} \sum_{s'} P(s' | s, a) U[s']
      if |U'[s] - U[s] | &gt; \delta then \delta &lt;- |U'[s] - U[s]|
    until \delta &lt; e(1-\gamma) / \gamma
  return U
</pre>
</div>
</div>
</div>

<div id="outline-container-org269ddae" class="outline-3">
<h3 id="org269ddae"><span class="section-number-3">10.5</span> Policy Iteration</h3>
<div class="outline-text-3" id="text-10-5">
<p>
We have already observed that it is possible to get an optimal policy,
without having accurate utility function estimates.
</p>

<p>
The policy iteration algorithm exploits this. The algorithm alternates
between 2 steps, beginning at some policy \(\pi_0\):
</p>

<ol class="org-ol">
<li><b>Policy evaluation</b>: given a policy \(\pi_i\), calculate \(U_i = U^{\pi_i}\),
the utility of each state if \(\pi_i\) were to be executed</li>
<li><b>Policy improvement</b>: calculate a new MEU policy \(\pi_{i+1}\), using
one-step look ahead based on \(U_i\):</li>
</ol>

\begin{equation}
\pi^* (s) = argmax_{a \in A(s)} P(s' | s, a) U(s')
\end{equation}

<p>
Policy evaluation is simple, because the policy \(\pi_i\) specifies the
action \(\pi_i(s)\) in state \(s\). This means we have a simplified version
of the Bellman equation relating the utility of \(s\) to the utility of
its neighbours:
</p>

\begin{equation}
  U(s) = R(s) + \gamma  \sum_{s'} P(s' | s, a)U(s')
\end{equation}

<p>
These equations are linear and can be quickly solved (in \(O(n^3)\) time)
with linear algebra techniques. We can further speed up this process
by performing an approximate policy evaluation. We do this by
performing some number of value iteration steps to update the
utilities:
</p>

\begin{equation}
U_{i+1}(s) \leftarrow R(s) + \gamma \sum_{s'}P(s'|s, \pi_i(s))U_i(s')
\end{equation}

<p>
The resulting algorithm is called modified policy iteration, and is
often much more efficient.
</p>

<div class="org-src-container">
<pre class="src src-text">function POLICY-ITERATION(mdp) returns a policy
  inputs: mdp, an MDP
  locals: U, vector of utilities for states in S
          \pi, a policy vector indexed by state, initially random
  repeat
    U &lt;- POLICY-EVALUATION(\pi, U, mdp)
    unchanged? &lt;- true
    for each state s in S do
      if max_{a \in A(s)} \sum_{s'} P(s'|s, a) U[s'] &gt; \sum_{s'} P(s'|s, \pi[s])U[s'] then do
        \pi[s] \leftarrow argmax_{a \in A(s)} P(s'|s, a) U[s']
        unchanged &lt;- false
  until
    unchanged?
  return \pi
</pre>
</div>

<p>
<b>Asynchronous policy iteration</b> involves picking a subset of states and
applying either kind of updating (policy improvement or simplified
value iteration) on that subset. Given certain conditions, this is
guaranteed to converge, and the freedom to choose any subset of states
gives us a means to design efficient heuristic algorithms.
</p>
</div>
</div>

<div id="outline-container-org6e4e56c" class="outline-3">
<h3 id="org6e4e56c"><span class="section-number-3">10.6</span> Summary</h3>
<div class="outline-text-3" id="text-10-6">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Problem</th>
<th scope="col" class="org-left">Bellman Equation</th>
<th scope="col" class="org-left">Algorithm</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Prediction</td>
<td class="org-left">Bellman Expectation Equation</td>
<td class="org-left">Iterative Policy Evaluation</td>
</tr>

<tr>
<td class="org-left">Control</td>
<td class="org-left">Bellman Expectation Equation + Greedy Policy Improvement</td>
<td class="org-left">Policy Iteration</td>
</tr>

<tr>
<td class="org-left">Control</td>
<td class="org-left">Bellman Optimality Equation</td>
<td class="org-left">Value Iteration</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org73b1f1b" class="outline-3">
<h3 id="org73b1f1b"><span class="section-number-3">10.7</span> Partially Observable MDPs (POMDPs)</h3>
<div class="outline-text-3" id="text-10-7">
<p>
The assumption of full observability, accompanied with the Markov
assumption for the transition model means that the optimal policy
depends only on the current state. When the environment is partially
observable, the agent does not know which state it is in. The agent
then cannot execute \(\pi(s)\). The utility of a state \(s\) and the optimal
action in \(s\) does not only depend on \(s\), but also how much the agent
knows when it is in \(s\).
</p>

<p>
In addition to the elements of the MDP &#x2013; the transition model
\(P(s'|s, a)\), actions \(A(s)\), and reward function \(R(s)\), it also has
a sensor model \(P(e|s)\). The sensor model specifies the probability of
perceiving evidence \(e\) in state \(s\). For example, a sensor might
measure the number of adjacent walls. A noisy sensor might return the
wrong value with some probability.
</p>

<p>
Belief states are the set of actual states the agent might be in. In
POMDPs, these belief states are probability distributions over all
possible states. The agent can calculate its current belief state as
the conditional probability distribution over the actual states given
the sequences of percepts and actions so far.
</p>

<p>
If the previous belief state is \(b(s)\), and the agent performs some
action \(a\) and perceives evidence \(e\), then the new belief state is
given by:
</p>

\begin{equation}
b'(s') = \alpha P(e | s') \sum_s P(s' | s, a) b(s)
\end{equation}

<p>
where \(\alpha\) is a normalizing constant that makes the belief state sum
to 1.
</p>

<p>
<b>The optimal action in a POMDP depends only on the agent's belief
state</b>. The optimal policy can be described by a mapping \(\pi^* (b)\) from
belief states to actions. 
</p>

<p>
The decision cycle of a POMDP can be broken down into 3 steps:
</p>

<ol class="org-ol">
<li>Given the current belief state \(b\), execute the action \(a = \pi^* (b)\).</li>
<li>Receive percept \(e\).</li>
<li>Update the belief state to \(b'\) and repeat.</li>
</ol>

<p>
If we knew the action and the subsequent percept, then the update to
the belief state would be a deterministic one, following the update
equation. The subsequent percept is not yet known, so the agent will
arrive in one of several possible belief states. The probability of
perceiving \(e\), given that \(a\) is was the action taken from belief
state \(b\), is given by:
</p>

\begin{align}
  P(e | a,b) &= \sum_{s'} P(e | a, s', b) P(s' | a, b) \\
             &= \sum_{s'} P(e|s')P(s'|a, b) \\
             &= \sum_{s'} P(e | s')\sum_{s'}P(s'|s, a)b(s)
\end{align}

\begin{align}
  P(b' | a, b) &= \sum_eP(b' | e, a, b) P(e | a, b) \\
               &= \sum_eP(b' | e, a, b)\sum_{s'} P(e|s')\sum_{s'} P(s'
                 | s, a)b(s)
\end{align}

<p>
Where \(P(b' | e, a, b) = 1\) if \(b' = FORWARD(b,a,e)\) and \(0\)
otherwise.
</p>

<p>
Because POMDPs have continuous state space, new algorithms for
computing or approximating the optimal policies for MDPs do not apply here.
</p>
</div>
</div>

<div id="outline-container-org2033b97" class="outline-3">
<h3 id="org2033b97"><span class="section-number-3">10.8</span> Value iteration for POMDPs</h3>
<div class="outline-text-3" id="text-10-8">
<p>
The value iteration algorithm for the MDP computed one utility value
for each state. With infinitely many belief states, we need to be more
creative.
</p>

<p>
Consider conditional plans, and how the expected utility of executing
a fixed conditional plan varies with the initial belief state.
</p>

<ol class="org-ol">
<li>Let the utility of executing a fixed conditional plan \(p\) starting
in physical state \(s\) be \(\alpha_p(s)\). Then the expected utility of
executing \(p\) in belief state \(b\) is \(\sum_s b(s) \alpha_p (s)\) Hence the
expected utility of a fixed conditional plan varies linearly with
\(b\).</li>
<li>At any given belief state \(b\), the optimal policy will choose to
execute the conditional plan with the highest expected utility, and
the expected utility is just the utility of that conditional plan:</li>
</ol>

\begin{equation}
  U(b) = U^{\pi^*}(b) = max_{p} b \cdot \alpha_p
\end{equation}

<p>
If the optimal policy \(\pi^*\) chooses to execute \(p\) starting at \(b\),
then it is reasonable to expect that it might choose to execute \(p\) in
belief states that are close to \(b\).
</p>

<p>
From these 2 observations, we see that the utility function \(U(b)\) on
belief states, being the maximum of a collection of hyperplanes, will
be piecewise linear and convex.
</p>

<p>
Let \(p\) be a depth-d conditional plan whose initial action is \(a\) and
whose depth-d-1 subplan for percept \(e\) is \(p.e\), then
</p>

\begin{equation}
  \alpha_p{s} = R(s) + \gamma \left( \sum_{s'} P(s' | s,a)\sum_e P(e|s')\alpha_{p.e}(s') \right)
\end{equation}

<p>
This recursion gives rise to a value iteration algorithm:
</p>

<div class="org-src-container">
<pre class="src src-text">function POMDP-VALUE-ITERATION returns a utility function
  inputs: pomdp
          e, the maximum error allowed for utility
  locals: U, U' sets of plans p

  U' &lt;- set containing the empty plan [], with \alpha_[](s) = R(s)
  repeat
    U &lt;- U'
    U' &lt;- set of all plans computed with above equation
    U' &lt;- REMOVE-DOMINATED-PLANS(U')
  until MAX-DIFFERENCE(U, U') &lt; e(1-\gamma) / \gamma
  return U
</pre>
</div>

<p>
The algorithm's complexity is dominated by the number of plans
generated: given \(|A|\) possible actions and \(|E|\) possible
observations, there are \(|A|^{O(|E|^{d-1})}\) distinct depth-d plans,
making the algorithm hopelessly inefficient for larger problems.
</p>
</div>
</div>

<div id="outline-container-orgb713627" class="outline-3">
<h3 id="orgb713627"><span class="section-number-3">10.9</span> <span class="todo TODO">TODO</span> Online agents for POMDPs</h3>
</div>

<div id="outline-container-org598e994" class="outline-3">
<h3 id="org598e994"><span class="section-number-3">10.10</span> Monte Carlo Tree Search</h3>
<div class="outline-text-3" id="text-10-10">
<p>
We are still far from making anything that even resembles a strong AI.
What makes MCTS different from <a href="https://en.wikipedia.org/wiki/Minimax">Minimax</a>?
</p>

<p>
Minimax can take an impractical amount of time to do a full search of
the game tree, especially games with high branching factor. Some games
are highly open-ended, with game trees that are highly complex. This
makes it difficult to write an evaluation function for each state.
MCTS is a technique that will give good results for games, and is
domain-independent.
</p>

<p>
UCB1 constructs statistical confidence intervals:
</p>

\begin{equation}
  \bar{x_i} \pm \sqrt{\frac{2 \ln n}{n_i}}
\end{equation}

<p>
where:
</p>

<ul class="org-ul">
<li>\(\bar{x_i}\) is the mean payout for action \(i\)</li>
<li>\(n_i\) is the number of simulations of action \(i\)</li>
<li>\(n\) is the total number of plays</li>
</ul>

<p>
The strategy is to pick the action with the highest upper bound each time.
</p>

<p>
How could an AI possibly "plan" ahead when there are so many potential
moves and counter moves in Go?
</p>

<p>
MCTS builds a <i>statistics tree</i> (detailing value of nodes) that
partially maps onto the entire tree. Statistics tree guides the AI.
</p>

<p>
MCTS constructs the statistics tree at the starting point.
</p>

<dl class="org-dl">
<dt>Selection</dt><dd>All child nodes have now been visited at least once.
Now AI can select the best child node.
<ul class="org-ul">
<li>based on how good the statistics are</li>
<li>how much the child node has been "ignored"</li>
</ul></dd>
<dt>Expansion</dt><dd>Add a new node that the AI will investigate</dd>
<dt>Simulation</dt><dd>starting from position represented by left child node,
make random moves repeatedly until the game is won or lost</dd>
<dt>Update</dt><dd>Depending on win or loss, update left child node in stats
tree with relevant stats</dd>
</dl>

<p>
The parent nodes inherit statistics from child nodes.
</p>

<p>
The node with the highest number of simulations will be chosen as the
next move.
</p>

<p>
The first phase, selection, lasts until the statistics necessary to
treat each position reached as a multi-armed bandit problem is
collected. 
</p>

<p>
The second phase, expansion, occurs when the algorithm can longer be
applied. An unvisited child is randomly chosen, and a new record node
is added to the tree of statistics.
</p>

<p>
After expansion, the remainder of the playout is in phase 3,
simulation. This is done as a typical monte carlo simulation.
</p>

<p>
When the playout reaches the end, the update phase takes place. All of
the positions visited during this playout have their play count and
their win count incremented.
</p>

<p>
Some great references for productionized implementations of MCTS
include:
</p>

<ul class="org-ul">
<li><a href="https://github.com/tensorflow/minigo/blob/master/strategies.py">minigo/strategies.py</a></li>
<li><a href="https://github.com/tensorflow/minigo/blob/master/mcts.py">minigo/mcts.py</a></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org3e855ab" class="outline-2">
<h2 id="org3e855ab"><span class="section-number-2">11</span> Reinforcement Learning</h2>
<div class="outline-text-2" id="text-11">
</div>
<div id="outline-container-orgb233f3d" class="outline-3">
<h3 id="orgb233f3d"><span class="section-number-3">11.1</span> Passive Reinforcement Learning</h3>
<div class="outline-text-3" id="text-11-1">
<p>
We start with a passive learning agent using a state-based
representation in a fully observable environment.
</p>

<p>
In passive learning, the agent's policy \(\pi\) is fixed: in state \(s\), it
always executes the action \(\pi(s)\). Its goal is simply to learn how
good the policy is &#x2013; the utility function \(U^\pi (s)\).
</p>

<p>
The passive learning task is similar to policy evaluation, but the
agent does not know the transition model \(P(s'|s, a)\) and the reward
function \(R(s)\).
</p>

<p>
The agent executes a number of trials using the policy \(\pi\), and
experiences a sequence of state transitions. At each state its
percepts receives the current state and the reward of the state.
</p>

<p>
We write the utility as:
</p>

\begin{equation}
  U^\pi (s) = E\left[\sum_{t=0}^\infty \gamma^t R(S_t) \right]
\end{equation}
</div>

<div id="outline-container-orgeea71fb" class="outline-4">
<h4 id="orgeea71fb"><span class="section-number-4">11.1.1</span> Direct Utility Estimation (MC Learning)</h4>
<div class="outline-text-4" id="text-11-1-1">
<p>
The main idea of direct utility estimation is that the utility of a
state is the expected total reward from that state onward, and each
trial provides a sample of this quantity for each state visited.
</p>

<p>
Direct utility estimation reduces the reinforcement learning problem
to a supervised inductive learning problem, where each example has the
state as input, and the observed reward-to-go as output.
</p>

<p>
However, it misses an important source of information: that the
<i>utility of states are not independent</i>. This means it misses many
opportunities for learning. For example, if a state has high expected
utility, then neighbouring states should also have high expected
utility.
</p>

<p>
The utility of each state equals its own reward plus the expected
utility of its successor states: i.e. it obeys the <a href="#orgaba2945">Bellman Equation</a>
for a fixed policy.
</p>

<p>
We can view directed utility estimation as searching for \(U\) in a
hypothesis space that is much larger than it needs to be, since it
includes many functions that violate the Bellman equations.
</p>
</div>
</div>

<div id="outline-container-org23590df" class="outline-4">
<h4 id="org23590df"><span class="section-number-4">11.1.2</span> Adaptive Dynamic Programming</h4>
<div class="outline-text-4" id="text-11-1-2">
<p>
An ADP agent takes advantage of the constraints among the utilities of
states by learning the transition model that connects them and solving
the corresponding MDP using a dynamic programming method.
</p>

<p>
For a passive learning agent, the task is as simply as plugging in the
learnt transition model and the rewards into the Bellman equations to
calculate the utility of each state.
</p>

<p>
The task of learning the model is easy, because the environment is
fully observable. This means we have a supervised learning task where
the input is a state-action pair, and the output is the resulting
state. We keep track of how often each action outcome occurs and
estimate the transition probability \(P(s' | s, a)\) from the frequency
with which \(s'\) is reached when executing \(a\) in \(s\).
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 1: </span>A passive RL agent based on ADP.</label><pre class="src src-text">function PASSIVE-ADP_AGENT(percept) returns an action
  inputs: percept, indicating state s' and reward signal r'
  persistent: \pi, a fixed policy
    mdp: MDP with model P, rewards R, and discount \gamma
    U: a table of utilities, initially empty
    N_{sa}: a table of frequencies for each state-action pair
    N_{s'|s,a}: a table of outcome frequencies
    s, a: the previous state and action
  if s' is new then $U[s'] &lt;- r'; R[s'] &lt;- r'
  if s is not null then
    increment N_{sa}[s, a] and N_{s'|s,a}[s', s, a]
    for each t such that N_{s'|s, a}[t,s,a] is nonzero do
      P(t|s, a) &lt;- N_{s'|s, a}[t,s,a] / N_{sa}[s, a]
    U &lt;- POLICY-EVALUATION(\pi, U, mdp)
  if s'.TERMINAL? then s,a &lt;- null else s,a &lt;- s', \pi[s']
  return a
</pre>
</div>

<p>
This approach is computationally intractable for large state spaces.
In addition, it uses the maximum-likelihood estimation for learning
the transition model.
</p>

<p>
A more nuanced approach would be Bayesian reinforcement learning,
which assumes a prior probability \(P(h)\) for each hypothesis \(h\) about
what the true model is. The posterior probability \(P(h|e)\) is obtained
via Bayes' rule. Then \(\pi^* = argmax_\pi \sum_h P(h|e) u_h^\pi\).
</p>

<p>
Another approach, derived from robust control theory, allows for a set
of possible models \(H\) and defines an optimal robust policy as one
that gives the best outcome in the worst case over \(H\): \(\pi^* =
argmax_\pi min_h u_h^\pi\).
</p>
</div>
</div>

<div id="outline-container-orgf46bf0e" class="outline-4">
<h4 id="orgf46bf0e"><span class="section-number-4">11.1.3</span> Temporal-difference Learning</h4>
<div class="outline-text-4" id="text-11-1-3">
<p>
TD learning involves using the observed transitions to adjust the
utilities such that the constraint equations are met.
</p>

<p>
When a transition occurs from state \(s\) to state \(s'\), we apply the
update rule:
</p>

\begin{equation}
  U^\pi(s') \leftarrow U^\pi(s) + \alpha (R(s) + \gamma U^\pi(s') -U^\pi(s))
\end{equation}

<p>
Where \(\alpha\) is the learning rate. The difference in utilities gives rise
to the name temporal-difference.
</p>

<div class="org-src-container">
<pre class="src src-text">function PASSIVE-TD-AGENT(percept) returns an action
  inputs: percept, with current state s' and reward r'
  persistent: \pi, a fixed policy
    U, a table of utilities, initially empty
    N_s, a table of frequencies
    s, a, r, the previous state, action and reward

  if s' is new then U[s'] &lt;- r'
  if s is not null then
    increment N_s[s]
    U[s] &lt;- U[s] + \alpha N_s[s] (r + \gamma U[s'] - U[s])
  if s'.TERMINAL? then s, a r &lt;- null else s,a,r &lt;- s', \pi[s'], r'
  return a
</pre>
</div>

<p>
TD learning learns slower than ADP and shows much higher variability,
but is simpler and requires less computation. TD learning does not
need a transition model to perform updates.
</p>

<p>
ADP and TD are closely related. Both try to make local adjustments to
the utility estimates in order to make each state "agree" with its
successors. However, TD adjusts a state to agree with its observed
successor, while ADP adjusts the state to agree with all of the
successors that might occur, weighted by their probabilities.
</p>

<p>
ADP can be made more efficient by approximating the algorithms for
value or policy iteration. For example, the prioritized sweeping
heuristic prefers adjustments to states that have undergone a large
adjustment in their own utility schemes. This enables them to handle
state spaces that are far too large for a full ADP. An approximation
algorithm can use a minimum adjustment size that decreases as the
environment model becomes more accurate, eliminating very long value
iterations that occur early in learning due to large changes in the
model.
</p>
</div>
</div>
</div>

<div id="outline-container-org784b356" class="outline-3">
<h3 id="org784b356"><span class="section-number-3">11.2</span> Active Reinforcement Learning</h3>
<div class="outline-text-3" id="text-11-2">
<p>
A passive learning agent has a fixed policy that determines its
behaviour. An active agent must learn what actions to take.
</p>

<p>
First, the agent will need to learn a complete model with outcome
probabilities for all actions, rather than the model for the fixed
policy. The learning mechanism for the passive ADP agent will work for
this
</p>

<p>
Next, the agent has a choice of actions. The utilities it learns are
defined by the optimal policy, governed by the <a href="#orgaba2945">Bellman equations</a>.
Having obtained a utility function for the given  model, the agent can
extract an optimal action by one-step look-ahead to maximise the
expected utility.
</p>
</div>

<div id="outline-container-org1c5ec1f" class="outline-4">
<h4 id="org1c5ec1f"><span class="section-number-4">11.2.1</span> Potential Pitfalls</h4>
<div class="outline-text-4" id="text-11-2-1">
<p>
A greedy agent, that picks the best action given the learned model,
very seldom converges to the optimal policy for the environment and
sometimes converges to horrible policies.
</p>

<p>
This is because the learned model is not the same as the true
environment. What is optimal in the learned model might not be optimal
in the true environment.
</p>

<p>
An agent therefore has to make a tradeoff between exploitation to
maximise its reward, and exploration to maximise its long-term
well-being. The question on whether there is an optimal exploration
policy is a subfield of statistical decision theory called the bandit
problem.
</p>


<p>
An agent has to be greedy in the limit of infinite exploration, or
GLIE. This is the scenario where the learned model is the true model.
There are several GLIE schemes, one of the simplest is to have the
agent choose a random action a fraction \(\frac{1}{t}\) of the time and
to follow the greedy policy otherwise. This can be extremely slow to
converge.
</p>

<p>
A more sensible approach is to assign some eight to actions that the
agent has not tried very often,while tending to avoid actions that are
believed to be of low utility. This can be achieved by altering the
constraint equation to assign higher utility estimates to unexplored
state-action pairs.
</p>

\begin{equation}
  U^+(s) \leftarrow R(s) + \gamma max_{a} f\left( \sum_{s'} P(s' |
    s, a) U^+(s), N(s, a) \right)
\end{equation}

<p>
\(f(u, n)\) is called the exploration function. It determines how greed
is traded off against curiosity. The function should be increasing in
\(u\) and decreasing in \(n\).
</p>
</div>
</div>

<div id="outline-container-org52fdacd" class="outline-4">
<h4 id="org52fdacd"><span class="section-number-4">11.2.2</span> Learning an action-utility function</h4>
<div class="outline-text-4" id="text-11-2-2">
<p>
An active TD agent is no longer equipped with a fixed policy, so if it
learns a utility function \(U\), it will need to learn a model in order
to be able choose an action based on \(U\) via one-step look-ahead. The
<a href="#orga53ac4c">update rule for TD</a> remains unchanged. IT can be shown that the TD
algorithm will converge to the same values as ADP as the number of
training sequences tends to infinity.
</p>
</div>
</div>

<div id="outline-container-orgf810f0e" class="outline-4">
<h4 id="orgf810f0e"><span class="section-number-4">11.2.3</span> Q-learning</h4>
<div class="outline-text-4" id="text-11-2-3">
<p>
Q-learning learns an action-utility representation instead of learning
utilities. We will use the notation \(Q(s,a)\) to denote the value of
doing action \(a\) in state \(s\).
</p>

\begin{equation}
  U = max_a Q(s, a)
\end{equation}

<p>
<b>A TD agent that learns a Q-function does not need a model of the form
\(P(s' | s, a)\), either for learning or for action selection.</b>
Q-learning is hence called a model-free method. We can write a
constraint equation as follows:
</p>

\begin{equation}
  Q(s,a) = R(s) + \gamma \sum_{s'} P(s' | s, a) max_{a'} Q(s', a')
\end{equation}

<p>
However, this equation requires a model to be learnt, since it depends
on \(P(s' | s, a)\). The TD approach requires no model of state
transitions.
</p>

<p>
The updated equation for TD Q-learning is:
</p>

\begin{equation}
  Q(s, a) \leftarrow Q(s, a) + \alpha (R(s) + \gamma max_{a'} Q(s',
  a') - Q(s,a))
\end{equation}

<p>
which is calculated whenever action \(a\) is executed in state \(s\)
leading to state \(s'\).
</p>

<p>
Q-learning has a close relative called SARSA
(State-Action-Reward-State-Action). The update rule for SARSA is as
follows:
</p>

\begin{equation}
  Q(s, a) \leftarrow Q(s, a) + \alpha (R(s) + \gamma Q(s', a') - Q(s, a))
\end{equation}

<p>
where \(a'\) is the action actually taken in state \(s'\). The rule is
applied at the end of each \(s, a, r, s', a'\) quintuplet, hence the
name.
</p>

<p>
Whereas Q-learning backs up the best Q-value from the state reached in
the observed transition, SARSA waits until an action is actually taken
and backs up the Q-value for that action. For a greedy agent that
always takes the action with best Q-value, the two algorithms are
identical. When exploration is happening, they differ significanty.
</p>

<p>
Because Q-learning uses the best Q-value, it pays no attention to the
actual policy being followed - it is an off-policy learning algorithm.
However, SARSA is an on-policy algorithm.
</p>

<p>
Q-learning is more flexible in the sense that a Q-learning agent can
learn how to behave well even when guided by a random or adversarial
exploration policy. On the other hand, SARSA is more realistic: for
example if the overall policy is even partly controlled by other
agents, it is better to learn a Q-function for what will actually
happen rather than what the agent would like to happen.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orge07f974" class="outline-2">
<h2 id="orge07f974"><span class="section-number-2">12</span> RANDOM</h2>
<div class="outline-text-2" id="text-12">
</div>
<div id="outline-container-orgfd9eff2" class="outline-3">
<h3 id="orgfd9eff2"><span class="section-number-3">12.1</span> Simon's Ant</h3>
<div class="outline-text-3" id="text-12-1">
<p>
Simon, noble prize in economics
</p>

<blockquote>
<p>
The complexity of the behaviour is the manifestation of the complexity
of the environment and not the complexity of the program.
</p>
</blockquote>
</div>
</div>
</div>

<div id="outline-container-orgcd98514" class="outline-2">
<h2 id="orgcd98514"><span class="section-number-2">13</span> REFILE</h2>
<div class="outline-text-2" id="text-13">

<div class="figure">
<p><img src="REFILE/screenshot_2018-11-09_13-55-33.png" alt="screenshot_2018-11-09_13-55-33.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orged91972" class="outline-2">
<h2 id="orged91972"><span class="section-number-2">14</span> Papers</h2>
<div class="outline-text-2" id="text-14">
</div>
<div id="outline-container-org6b319e1" class="outline-3">
<h3 id="org6b319e1"><span class="section-number-3">14.1</span> Improving Policy Gradient by Exploring Under-Appreciated Rewards</h3>
<div class="outline-text-3" id="text-14-1">
<p>
The REINFORCE algorithm minimizes the KL divergence between \(\pi_\theta\)
and \(\pi^*\), the optimal policy. However, learning an optimal policy by
optimizing this direction of the KL is known to be <i>mode-seeking</i>.
Hence, it the agent is prone to falling into a local optimum, and miss
out some modes of \(\pi^*\).
</p>

<p>
Hence, a entropy regularization loss is added to encourage
exploration.
</p>

<p>
However, this exploration is undirected, and requires a small
regularization coefficient to prevent too much random exploration. 
</p>

<p>
Optimizing in the mean-seeking direction of the KL divergence is to
learn a policy by following:
</p>

\begin{equation}
  O_{RAML}(\theta; \tau) = \mathcal{E}_{h \sim p(h)}\left\{ \tau
    \sum_{a \in A} \pi_\tau^* (a | h) \log \pi_\theta (a | h) \right\}
\end{equation}

<p>
In RL, the reward landscape is unknown, hence sampling from
\(\pi_\tau^*\) is not straightforward. We can approximate the
expectation with respect to \(\pi_\tau^*\) by using self-normalized
importance sampling. For importance sampling, one draws \(K\) i.i.d
samples \(\{ a^{(k)}\}^{K}_{k=1}\) from \(\pi_\theta\) and computes a set
of normalized weights to approximate \(O_{RAML}(\theta; \tau | h)\).
</p>

\begin{equation}
O_{RAML} (\theta; \tau | h) \approx \tau
\sum_{k=1}^{K}\frac{w_\tau(a^{(k)} | h)}{\sum_{m=1}^{K}w_\tau(a^{(m)}
  | h)} \log \pi_\theta(a^{(k)} | h)
\end{equation}

<p>
where \(w_\tau(a^{(k)} | h) \propto \pi_\tau^* / \pi_\theta\) denotes an
importance weight defined by:
</p>

\begin{equation}
  w_\tau (a^{(k)} | h) = exp \left\{ \frac{1}{\tau}r (a^{(k)} | h) -
    \log \pi_\theta (a^{(k)} | h) \right\}
\end{equation}

<p>
One can view the weights as the discrepancy between scaled rewards \(r /
\tau\) and the policy's log-probabilities \(\log \pi_\theta\).
</p>

<p>
In UREX, both the RAML objective and the expected reward objective is
combined and jointly maximized.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-11-26 Mon 09:54</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
